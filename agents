# AI app generation
Source: https://docs.daydream.live/ai-oneshot

Learn how to use AI tools to one-shot a StreamDiffusion-powered app

## Overview

This guide shows how to “one shot” an app using an AI coding assistant. You’ll copy a single set of instructions, open your preferred AI tool, and let it scaffold and wire everything up.

<Info>
  You’ll paste a single, comprehensive prompt. We’ve included quick‑launch links that prefill the prompt for tools that support it.
</Info>

## Prerequisites

* **AI tool account**: Claude, ChatGPT, Gemini, or another assistant.
* **Local dev environment**: Git, Node.js, and your editor of choice.
* **Daydream API key**: See [Authentication](/authentication) if your app will call the Daydream API.

## Steps

<Steps>
  <Step title="Select from the one of the examples below">
    Select from an example to get started.
  </Step>

  <Step title="Copy the one‑shot instructions">
    Copy the prompt below. Replace the placeholder for the API key.
  </Step>

  <Step title="Open your AI coding tool">
    Use the quick‑launch buttons below to open a new chat with the prompt prefilled (where supported) or paste it manually.
  </Step>

  <Step title="Run and iterate">
    Ask the assistant to execute the steps, generate files, run installs, and fix any issues. Approve tool actions if your assistant supports them.
  </Step>
</Steps>

## Select From the Examples Below

<Columns>
  <Card title="Webcam StreamDiffusion" icon="link" href="/oneshot-examples/webcam-streamdiffusion">
    Webcam Livestream with StreamDiffusion
  </Card>

  <Card title="DrawCanvas" icon="link" href="/oneshot-examples/drawcanvas">
    DrawCanvas
  </Card>

  <Card title="FluidCanvas" icon="link" href="/oneshot-examples/fluidcanvas">
    FluidCanvas
  </Card>

  <Card title="Audio-Reactive" icon="link" href="/oneshot-examples/audio-reactive">
    Audio-Reactive
  </Card>
</Columns>


# Create a New Stream
Source: https://docs.daydream.live/api-reference/create-stream

POST /v1/streams
Creates a new video processing stream with the specified configuration

This endpoint creates a new video processing stream using the Daydream StreamDiffusion pipeline. You'll specify the pipeline type and model configuration in the request body.

## Request Body Structure

```json theme={null}
{
  "pipeline": "streamdiffusion",
  "params": {
    "model_id": "stabilityai/sdxl-turbo",
    // ... additional parameters
  }
}
```

* **`pipeline`**: The processing pipeline to use. Currently `"streamdiffusion"` is the only public option.
* **`params`**: Configuration object containing the model and generation parameters.
* **`params.model_id`**: The specific model to use (see table below).

## Available Models

| model\_id                   | Family | ControlNets | IP Adapter | Cached Attention |
| --------------------------- | ------ | ----------- | ---------- | ---------------- |
| `stabilityai/sd-turbo`      | SD2.1  | 6 types     | No         | No               |
| `stabilityai/sdxl-turbo`    | SDXL   | 3 types     | Yes        | Yes              |
| `Lykon/dreamshaper-8`       | SD1.5  | 4 types     | Yes        | Yes              |
| `prompthero/openjourney-v4` | SD1.5  | 4 types     | Yes        | Yes              |

<Tip>
  **Which model should I use?**

  * **SD2.1 (`sd-turbo`)**: Fastest, good for real-time effects
  * **SDXL (`sdxl-turbo`)**: Highest quality, supports IP Adapter for style transfer
  * **SD1.5 (`dreamshaper-8`, `openjourney-v4`)**: Great for stylized/cartoon effects, supports IP Adapter and Cached Attention
</Tip>

## Example: Create an SDXL Turbo Stream

```bash theme={null}
curl -X POST \
  "https://api.daydream.live/v1/streams" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer ${DAYDREAM_API_KEY}" \
  -d '{
    "pipeline": "streamdiffusion",
    "params": {
      "model_id": "stabilityai/sdxl-turbo",
      "prompt": "anime character"
    }
  }'
```

## Example: Create a Stream with ControlNet

```bash theme={null}
curl -X POST \
  "https://api.daydream.live/v1/streams" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer ${DAYDREAM_API_KEY}" \
  -d '{
    "pipeline": "streamdiffusion",
    "params": {
      "model_id": "stabilityai/sdxl-turbo",
      "prompt": "oil painting portrait",
      "controlnets": [
        {
          "model_id": "xinsir/controlnet-depth-sdxl-1.0",
          "preprocessor": "depth_tensorrt",
          "conditioning_scale": 0.5,
          "enabled": true
        }
      ]
    }
  }'
```

<Note>
  For full parameter documentation, see the [Parameters](/parameters/SD15) section.
</Note>

## Legacy Support

<Warning>
  The `pipeline_id` field (e.g., `"pip_SD15"`) is deprecated but still supported for backward compatibility. New integrations should use `pipeline` + `params.model_id` instead.
</Warning>


# Stream Status
Source: https://docs.daydream.live/api-reference/stream-status

GET /v1/streams/{id}/status
Check the status of your stream

The stream status uses the following url, make sure to include your stream Id.


# Update a Stream
Source: https://docs.daydream.live/api-reference/update-stream

PATCH /v1/streams/{id}
Updates pipeline parameters for an existing video processing stream

<Note>
  Rate limit: 300 requests per minute.
</Note>

This endpoint allows you to update pipeline parameters for an existing stream. You can modify prompts, ControlNet settings, IP Adapter configuration, and other generation parameters.

## Request Body Structure

```json theme={null}
{
  "pipeline": "streamdiffusion",
  "params": {
    "model_id": "Lykon/dreamshaper-8",
    "prompt": "new prompt",
    // ... only include parameters you want to change
  }
}
```

<Tip>
  The `pipeline` field defaults to `"streamdiffusion"` if omitted, so you can just send `params` for most updates.
</Tip>

## Hot-Swap vs Reload Parameters

<Warning>
  The following parameters can be updated dynamically (no reload required):

  * `prompt`, `negative_prompt`
  * `guidance_scale`, `delta`
  * `num_inference_steps`, `t_index_list`
  * `seed`
  * `controlnets.conditioning_scale`
  * `ip_adapter` settings
  * `ip_adapter_style_image_url`

  All other parameters trigger a full pipeline reload (\~30s).
</Warning>

## Example: Update Prompt

```bash theme={null}
curl -X PATCH \
  "https://api.daydream.live/v1/streams/${STREAM_ID}" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer ${DAYDREAM_API_KEY}" \
  -d '{
    "params": {
      "model_id": "Lykon/dreamshaper-8",
      "prompt": "cyberpunk cityscape, neon lights"
    }
  }'
```

## Example: Adjust ControlNet Strength

```bash theme={null}
curl -X PATCH \
  "https://api.daydream.live/v1/streams/${STREAM_ID}" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer ${DAYDREAM_API_KEY}" \
  -d '{
    "params": {
      "model_id": "Lykon/dreamshaper-8",
      "controlnets": [
        {
          "model_id": "lllyasviel/control_v11f1p_sd15_depth",
          "preprocessor": "depth_tensorrt",
          "conditioning_scale": 0.7,
          "enabled": true
        }
      ]
    }
  }'
```

<Note>
  For full parameter documentation, see the [Parameters](/parameters/SD15) section.
</Note>


# API Keys
Source: https://docs.daydream.live/api_keys

Guide on generating API

To use the Daydream API, you need to provide an API key. Follow the steps below on how to generate one.

## Steps

<Steps>
  <Step title="Log into Daydream Playground">
    Go to <a href="https://app.daydream.live">Daydream Playground</a> and log in with your credentials. <Note>If you do not have an account, sign up for one and sign in.</Note>
  </Step>

  <Step title="Access the API Dashboard">
    Click on the button on the top left corner that say `</> API Dashboard`.

    <img alt="API Dashboard Button" />
  </Step>

  <Step title="Create API Key">
    Click on the `+ New API Key` button on the top right corner of the page

    <img alt="Create API Button" />
  </Step>

  <Step title="Name the API Key">
    When the `+ New API Key` button is select, you will be prompted to name the API key. Provide the name for the api key and select the `Generate Key` button.

    <img alt="Name API Key Window" />
  </Step>

  <Step title="Generate the API Key">
    Copy the api key and select the `Done` button

    <Warning>
      Copy this key now. You will NOT be able to see this API key again.
    </Warning>

    <img alt="Name API Key Window" />
  </Step>

  <Step title="Verify API Key Creation">
    Once you created the API key, you should see it added to the API Key's dashboard.

    <img alt="API Key Dashboard" />
  </Step>
</Steps>


# Authentication
Source: https://docs.daydream.live/authentication

Authenticating with the Daydream API

The API uses Bearer token authentication. Include your API key in the Authorization header:

```
Authorization: Bearer YOUR_API_KEY
```


# Code Examples
Source: https://docs.daydream.live/code-examples

Ideas for using the Daydream API and SDKs

Here are some examples to inspire building your own applications.

## SDKs

Get started quickly with our official SDKs:

<CardGroup>
  <Card title="TypeScript SDK" icon="node-js" href="/sdks/typescript/installation">
    Server-side stream management with `@daydreamlive/sdk`
  </Card>

  <Card title="Browser SDK" icon="browser" href="/sdks/browser/installation">
    WebRTC broadcasting and playback with `@daydreamlive/browser`
  </Card>

  <Card title="TouchDesigner" icon="cube" href="/sdks/touchdesigner/installation">
    Visual programming and VJ applications
  </Card>

  <Card title="OBS Plugin" icon="video" href="/sdks/obs/installation">
    Add AI effects to your OBS streams
  </Card>
</CardGroup>

## Browser SDK Examples

The Browser SDK includes working examples you can try immediately:

| Example                                                                                                    | Description        | Demo                                                                 |
| ---------------------------------------------------------------------------------------------------------- | ------------------ | -------------------------------------------------------------------- |
| [with-react](https://github.com/daydreamlive/daydream-browser/tree/main/examples/with-react)               | Basic React hooks  | [Live Demo](https://daydream-browser-kohl.preview.livepeer.monster)  |
| [with-compositor](https://github.com/daydreamlive/daydream-browser/tree/main/examples/with-compositor)     | Canvas composition | [Live Demo](https://daydream-browser-zeta.preview.livepeer.monster)  |
| [with-screen-share](https://github.com/daydreamlive/daydream-browser/tree/main/examples/with-screen-share) | Screen capture     | [Live Demo](https://with-screen-share-lime.preview.livepeer.monster) |
| [with-three](https://github.com/daydreamlive/daydream-browser/tree/main/examples/with-three)               | Three.js streaming | [Live Demo](https://with-three-git-main.preview.livepeer.monster)    |
| [with-vanilla](https://github.com/daydreamlive/daydream-browser/tree/main/examples/with-vanilla)           | Pure JS (no React) | —                                                                    |

## Community Examples

<Columns>
  <Card title="FluidCanvas Example" icon="file-code" href="https://github.com/daydreamlive/daydream-examples/tree/main/with-fluid-canvas">
    <iframe />

    An interactive WebGL-based fluid simulation built with React, TypeScript, and Vite. This demo showcases real-time fluid dynamics with customizable parameters and visual effects.
  </Card>

  <Card title="DrawingCanvas Example" icon="file-code" href="https://github.com/daydreamlive/daydream-examples/tree/main/with-drawing-canvas">
    <iframe />

    A React drawing canvas example with WebRTC streaming capabilities. This
    example demonstrates a feature-rich drawing component with multiple tools,
    recording capabilities, and real-time streaming support.
  </Card>

  <Card title="Audio-Reactive Example" icon="file-code" href="https://github.com/daydreamlive/daydream-examples/tree/main/with-audio-input">
    <iframe />

    An AudioInput component system that captures microphone or demo audio, analyzes frequency levels in real-time, and renders audio-reactive visualizations to canvas for streaming applications.
  </Card>
</Columns>

## GitHub Repositories

* [daydream-typescript](https://github.com/daydreamlive/daydream-typescript) - TypeScript SDK
* [daydream-browser](https://github.com/daydreamlive/daydream-browser) - Browser SDK with React hooks
* [daydream-touchdesigner](https://github.com/daydreamlive/daydream-touchdesigner) - TouchDesigner plugin
* [daydream-obs](https://github.com/daydreamlive/daydream-obs) - OBS plugin
* [daydream-examples](https://github.com/daydreamlive/daydream-examples) - Community examples


# Daydream Knowledge Hub
Source: https://docs.daydream.live/index

Your gateway to real-time video AI — from creating live visuals and integrating with APIs to running local models and exploring cutting-edge research.

<div>
  <div />

  <div>
    <h1>
      <span>Daydream </span>

      <span>
        Knowledge Hub
      </span>
    </h1>

    <p>
      Learn to create real-time AI visuals in TouchDesigner, run cutting-edge models locally with Scope, and ship creative apps using the Daydream API. Stay current on the latest world model and real-time video generation research.
    </p>
  </div>

  <div>
    <div>
      <div>
        <div>
          <Icon icon="palette" />
        </div>

        <h3>Create Live Visuals</h3>

        <p>
          Real-time AI visuals powered by StreamDiffusion, SDXL, and IPAdapter—using TouchDesigner or the browser-based Playground
        </p>

        <div>
          <a href="https://playground.daydream.live">
            → Start with Daydream Playground
          </a>
        </div>
      </div>

      <a href="/scope/getting-started/quickstart">
        <div>
          <Icon icon="brain" />
        </div>

        <h3>Run Models Locally</h3>

        <p>
          Run Krea Realtime, LongLive, and StreamDiffusionV2 on your own GPU with full control over pipelines. Cloud GPUs supported.
        </p>

        <div>
          → Start with Scope (Local or Cloud)
        </div>
      </a>

      <a href="/introduction">
        <div>
          <Icon icon="code" />
        </div>

        <h3>Build with APIs</h3>

        <p>
          Build StreamDiffusion-powered web and mobile apps with our hosted infrastructure and performant API—no GPUs required
        </p>

        <div>
          → Start with Daydream API
        </div>
      </a>

      <a href="/knowledge-hub/research-references/index">
        <div>
          <Icon icon="flask" />
        </div>

        <h3>Explore World Models Research</h3>

        <p>
          Stay current on world models and real-time video generation research with curated papers, implementations, and benchmarks
        </p>

        <div>
          → Explore Research
        </div>
      </a>
    </div>
  </div>
</div>


# Introduction
Source: https://docs.daydream.live/introduction

Comprehensive API documentation for Daydream

# Daydream API

<Tip>
  Get a free API key from the [Daydream Builder Dashboard](https://app.daydream.live/dashboard/api-keys?utm_source=docs\&utm_medium=tip\&utm_campaign=api_key_signup\&utm_content=introduction_tip)
</Tip>

<CardGroup>
  <Card title="Developer Quickstart" icon="person-running-fast" href="/quickstart">
    Get an AI video stream up and running
  </Card>

  <Card title="Vibecoding" icon="robot" href="/ai-oneshot">
    Idea to app in 10 minutes
  </Card>
</CardGroup>

Welcome to the Daydream API documentation. This API enables seamless interaction, allowing you to create and manipulate live video streams, generate interactive audio experiences, and produce real-time AI-driven visuals. It is designed to give developers powerful tools for building immersive, dynamic, and engaging multimedia applications.

## Base URL

```
https://api.daydream.live
```

## Getting Started

The API is organized around REST principles and returns JSON responses. All endpoints are automatically documented below with interactive examples.

## Support

If you need help or have questions about the API, please:

* Check the [Quickstart](/quickstart) or the [API Reference](/api-reference/create-stream) section
* Explore our [SDKs](/sdks/overview) for TypeScript, Browser, TouchDesigner, and OBS
* Join our [Discord Community](https://discord.com/invite/5sZu8xmn6U)


# FreeTimeGS (2025)
Source: https://docs.daydream.live/knowledge-hub/research-references/FreeTimeGS

Free Gaussian Primitives at Anytime Anywhere for Dynamic Scene Reconstruction

## Overview

FreeTimeGS introduces a novel 4D representation allowing Gaussian primitives to appear at arbitrary time and locations for dynamic scene reconstruction. Each Gaussian has motion and temporal opacity functions, providing flexibility to model complex motions without canonical space deformation fields.

## Why it matters

Overcomes limitations of canonical-space methods that struggle with complex motions by eliminating the need for difficult deformation field optimization. Achieves real-time dynamic view synthesis with superior quality on scenes with fast, complex movements.

## Key trade-offs / limitations

* Requires lengthy reconstruction process for each dynamic scene
* Currently supports only novel view synthesis (no relighting capabilities)
* May need generative priors for optimization-free reconstruction in future
* High temporal redundancy without motion function constraints

**Link**
[arXiv:2506.05348](https://arxiv.org/abs/2506.05348)


# Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling (2025)
Source: https://docs.daydream.live/knowledge-hub/research-references/GeometryForcing

Enhances video diffusion models with 3D awareness through geometric foundation model alignment for spatially and temporally consistent generation.

## Overview

Geometry Forcing addresses a key limitation in video diffusion models: their failure to capture meaningful geometric-aware structure when trained solely on raw video data. The method enhances video diffusion models with 3D awareness by aligning intermediate representations with features from pretrained geometric foundation models. It introduces Angular Alignment (enforcing directional consistency via cosine similarity) and Scale Alignment (preserving scale information through feature regression) to internalize latent 3D representations. This approach demonstrates substantial improvements in visual quality and 3D consistency for both camera view-conditioned and action-conditioned video generation tasks.

## Why it matters

Video generation models often produce inconsistent scenes due to lack of geometric understanding. By bridging the gap between 2D video data and the underlying 3D nature of the physical world, Geometry Forcing enables more realistic and coherent video synthesis. This geometric awareness is crucial for applications in robotics, autonomous systems, and immersive media where spatial consistency is paramount. The method's ability to generate consistent full-circle rotations and maintain geometric structure during navigation represents a significant step toward world models that understand 3D space.

## Key trade-offs / limitations

* Requires additional pretrained geometric foundation models, increasing computational overhead and complexity.
* Performance heavily depends on the quality and alignment of the geometric foundation model features.
* May not generalize well to scenes or camera motions that significantly differ from the geometric model's training data.
* The alignment objectives add hyperparameters that require careful tuning for optimal performance.

## Link

[arXiv:2507.07982](https://arxiv.org/abs/2507.07982)


# SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching (2025)
Source: https://docs.daydream.live/knowledge-hub/research-references/SpeCa

Novel 'forecast-then-verify' framework enabling 6.34× speedup for diffusion models through speculative sampling with minimal quality loss.

## Overview

SpeCa introduces speculative sampling to diffusion models, drawing inspiration from speculative decoding in large language models. The framework predicts intermediate features for subsequent timesteps based on fully computed reference timesteps, then uses a parameter-free verification mechanism to evaluate prediction reliability. This "forecast-then-verify" approach enables real-time decisions to accept or reject predictions with negligible computational overhead (1.67%-3.5% of full inference costs). SpeCa also implements sample-adaptive computation allocation, dynamically modulating resources based on generation complexity to optimize efficiency across varying sample difficulty levels.

## Why it matters

Diffusion models face fundamental computational bottlenecks: strict temporal dependencies preventing parallelization and intensive forward passes at each denoising step. Modern video generation models like HunyuanVideo require 595.46 TFLOPs per forward pass, making real-time generation prohibitive. SpeCa's 6.34× acceleration while maintaining generation quality represents a breakthrough for practical deployment of diffusion models in real-time applications, from interactive content creation to live video synthesis.

## Key trade-offs / limitations

* Acceleration benefits may vary significantly across different model architectures and generation tasks.
* Verification mechanisms, while lightweight, still add computational overhead that could compound in some scenarios.
* Sample-adaptive computation may introduce inconsistent latency patterns in production systems requiring predictable timing.
* The forecasting accuracy depends on the predictability of feature evolution, potentially limiting gains for highly complex or chaotic generation patterns.

## Link

[arXiv:2509.11628](https://www.arxiv.org/abs/2509.11628)


# Video World Models with Long-term Spatial Memory (2025)
Source: https://docs.daydream.live/knowledge-hub/research-references/VWMSpacialMemory

Incorporating geometry-grounded long-term spatial memory into video world models for improved consistency over revisits.

# Video World Models with Long-term Spatial Memory (2025)

## Overview

Wu et al. propose a framework to bolster video world models with a geometry-grounded long-term spatial memory that can store and retrieve information across long horizons. This is meant to mitigate issues like forgetting or inconsistencies when a generated scene revisits parts of an environment. They also curate datasets specifically designed for evaluating these long-term memory mechanisms. :contentReference\[oaicite:0]

## Why it matters

Maintaining spatial consistency over time is crucial for interactive agents, simulation, AR/VR, robotics, etc. This helps avoid drift and visual or geometric mismatches which degrade user experience or reliability of world models. The idea of geometry-grounded memory is promising for scaling up scenes.

## Key trade-offs / limitations

* Adding memory structures increases model complexity and computation/memory cost.
* Dataset curated for revisits may not cover all types of environments (diversity of geometry, lighting, etc.).
* Retrieval / memory mechanisms can introduce latency, or errors if memory is noisy.

## Link

[arXiv:2506.05284](https://arxiv.org/abs/2506.05284)


# About Real-Time Video Models and World Models
Source: https://docs.daydream.live/knowledge-hub/research-references/about-video-and-world-models

Breaking down the state of the art and opportunities for development

## Introduction

Video generation and **world models** are rapidly converging. What started as separate pursuits — one focused on producing *realistic videos* and the other on *predicting and simulating environments* — is moving toward a common goal: **real-time, interactive, persistent simulations of the world.**

This convergence is reshaping how we think about **AI for gaming, robotics, AR/VR, sports, and interactive media**.

***

## The Shared Foundation

Both video and world models build on the same hierarchy of capabilities:

* **Causal** → predictions must flow forward in time.
* **Interactive** → must accept and respond to user/agent actions.
* **Persistent** → maintain consistency across long horizons.

On top of this shared base, the goals diverge:

* **Physically Accurate** (for robotic learning): fidelity to real-world physics and generalization across conditions.
* **Real-Time** (for human entertainment): ultra-low latency and high frame-rate responsiveness.

***

## What Are World Models?

A **world model** is any system that predicts how the world evolves over time, often under the influence of actions. Two traditions exist:

* **Abstract / semantic world models**\
  Capture internal representations useful for reasoning, prediction, and planning.\
  *Example: An agent predicting “if I push the cup, it will fall,” without needing to render the cup in pixels.*

* **Full-fidelity simulators**\
  Generate high-detail, physically accurate environments, often pixel-by-pixel.\
  *Example: A physics-based engine rendering a falling cup in real time.*

The ultimate goal is **convergence**: models that *understand* the world semantically *and* can **simulate it visually and physically**.

***

## Video Models: Stepping Toward World Models

**Video models** — especially those trained on large video datasets — generate temporally coherent frames. While impressive in producing realistic clips, they’ve historically lacked:

* **Causality**
* **Interactivity**
* **Persistence**
* **Real-time responsiveness**
* **Physical accuracy**

These limitations highlight why today’s video models aren’t yet *true world models*.

***

## Why Real-Time Matters

For many applications — gaming, AR/VR, telepresence, live sports analysis, robotics — **latency and responsiveness are non-negotiable**:

* **Throughput**: generate frames fast enough for live playback.
* **Latency**: reflect actions with near-instant updates.
* **Consistency**: objects and environments should remain stable across thousands of frames.

Without these properties, video models remain beautiful demos rather than interactive worlds.

***

## Pathways Toward Convergence

Researchers are pushing several directions to unify video and world models:

| Approach                                              | Improves                 | Real-Time Implications                         |
| ----------------------------------------------------- | ------------------------ | ---------------------------------------------- |
| **Autoregressive & causal modeling**                  | Causality, interactivity | Enables frame-by-frame responsiveness.         |
| **Action-conditioned video generation**               | Interactivity            | Bridges agent control with video outputs.      |
| **Memory & state-space models**                       | Persistence              | Maintain object stability over long sequences. |
| **Optimized architectures (e.g. few-step diffusion)** | Real-time performance    | Push toward VR/AR frame-rate targets.          |
| **Physics-aware training & loss functions**           | Physical accuracy        | Ensure believable motion & generalization.     |

***

## The Convergence Point

We are seeing **video models evolving into world models**, and **world models adopting video-first realism**:

* **From Video → World Models**:\
  Video generation learns to accept actions, maintain causality, and simulate physics.

* **From World Models → Video**:\
  Abstract predictors are extended to produce **visually rich renderings** that humans and machines can both use.

The result? **Interactive, real-time environments** that serve as both *simulations for agents* and *immersive experiences for humans*.

***

## What’s Next

Key challenges on the path to convergence:

1. **Datasets pairing video with actions** for training interactivity.
2. **Benchmarks** that measure not just pixel quality, but latency, persistence, and physical realism.
3. **Hybrid systems** combining video generation, 3D/4D representations, and explicit physics.
4. **Scaling to real-time** while maintaining fidelity.

***

## Summary

* **Video models** generate frames, but often lack causality, interactivity, persistence, real-time speed, and physical accuracy.
* **World models** aim to predict the unfolding of reality, either abstractly or with simulation fidelity.
* **Convergence** is underway: video models are gaining world-model properties, while world models are adopting visual realism.
* The outcome: **real-time, interactive video world models** that can power games, robotics, AR/VR, and beyond.

***


# Aether: Geometric-Aware Unified World Modeling (2025)
Source: https://docs.daydream.live/knowledge-hub/research-references/aether

Joint 4D dynamics, action-conditioned video prediction & goal-conditioned planning in unified framework.

## Overview

Aether introduces a unified world modeling framework combining 4D dynamic reconstruction, action-conditioned video prediction, and goal-conditioned planning. It emphasizes geometry-aware features and shows generalization without using real-world data, as well as zero-shot performance in action following and reconstruction. :contentReference\[oaicite:5]

## Why it matters

Merging reconstruction, prediction, planning in one model simplifies pipelines for embodied AI tasks. Geometry awareness helps bridge simulation to reality and improves perceptual alignment and control. Zero-shot generalization is especially powerful for real-world deployment.

## Key trade-offs / limitations

* Training multiple objectives jointly can lead to complex tuning.
* Synthetic-to-real generalization, though promising, may still falter in more complex real scenes.
* The approach may not yet scale to very high resolution or large outdoor scenes.

## Link

[arXiv:2503.18945](https://arxiv.org/abs/2503.18945)


# BEVControl (2023)
Source: https://docs.daydream.live/knowledge-hub/research-references/bevcontrol

Two-stage generative model controlling street-view elements via BEV sketch layouts.

## Overview

BEVControl introduces a two-stage generative method that separates geometric control from appearance by using Bird-Eye View (BEV) sketch layouts. It aims to produce realistic street-view images consistent with both foreground and background, and supports human-editable sketch input. The method also proposes a multi-level evaluation protocol to fairly assess scene, object, and background geometry fidelity. :contentReference\[oaicite:0]

## Why it matters

This work offers fine control in synthesis for autonomous driving and scene understanding, leveraging BEV layouts so that downstream perception models (e.g. for detection or segmentation) can be trained on data that is both controllable and consistent. It shows improved performance over BEVGen especially in terms of foreground object consistency.

## Key trade-offs / limitations

* Mostly suited to street / road scenarios; not tested on indoors or unstructured environments.
* High fidelity detail degrades for more complex or far-field parts of scenes.
* Sketch style input is flexible but may still require nontrivial editing by human users.

## Link

[arXiv:2308.01661](https://arxiv.org/abs/2308.01661)


# BEVGen (2023)
Source: https://docs.daydream.live/knowledge-hub/research-references/bevgen

Generative model producing street-view images from BEV layouts with cross-view spatial consistency.

## Overview

BEVGen is a conditional generative model that synthesizes a set of street-view images given a BEV (Bird’s-Eye View) segmentation layout. It uses a cross-view transformation and spatial attention to ensure consistency across views (map layout + street view). Evaluated on datasets like NuScenes and Argoverse 2, it generates varied scenes under different weather and lighting, maintaining road/lanes consistency. :contentReference\[oaicite:1]

## Why it matters

Helps simulate realistic driving environments for perception tasks, linking layout maps to photorealistic street-view images. Useful for data augmentation, visualization, simulation in autonomous driving. It provides a baseline for controllable layout-to-image generation.

## Key trade-offs / limitations

* Limited to static scenes; dynamic motion or temporal consistency are not the focus.
* Weather/time variation works, but more extreme out-of-distribution scenarios may degrade quality.
* Less control at object‐specific or fine detail level compared to methods that use more geometric input.

## Link

[arXiv:2301.04634](https://arxiv.org/abs/2301.04634)


# DIAMOND (2024)
Source: https://docs.daydream.live/knowledge-hub/research-references/diamond

Diffusion-based world models showing visual-detail gains on Atari-style RL.

## Overview

DIAMOND is a diffusion-based world model trained on environment frames that demonstrates improved visual fidelity and downstream RL performance (e.g., on Atari). The paper argues that preserving visual detail via diffusion improves performance for pixel-based RL agents compared to coarse latent transitions.

## Why it matters

Shows diffusion models can be effective representations for world models where detailed pixel fidelity matters for agent decision-making.

## Key trade-offs / limitations

* Diffusion models are typically more compute-intensive at training and inference.
* Results demonstrated in constrained domains (Atari); transfer to large real-world scenes requires further study.

## Link

[NeurIPS 2024 poster / paper page](https://neurips.cc/virtual/2024/poster/95428)


# Genie 3 (2025)
Source: https://docs.daydream.live/knowledge-hub/research-references/genie3

Interactive world model capable of generating navigable 3D environments in real time.

## Overview

Genie 3 is DeepMind’s interactive world model that generates real-time, navigable 3D environments from text or image prompts. It runs at \~24 fps, 720p resolution, maintaining visual and physical consistency over several minutes.

## Why it matters

Pushes beyond passive video generation into real-time interactive environments, useful for robotics, simulation, and gaming. Marks progress toward general world simulators.

## Key trade-offs / limitations

* Public details on architecture are limited.
* Resolution capped at 720p; artifacts still appear.
* Likely high compute cost for real-time inference.

## Link

[DeepMind Blog: Genie 3](https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/)


# HunyuanWorld-Voyager (2025)
Source: https://docs.daydream.live/knowledge-hub/research-references/huanyuanworld-voyager

Video diffusion model for world-consistent, explorable 3D scene generation.

## Overview

HunyuanWorld-Voyager generates consistent 3D point-cloud video sequences from a single image and camera path. It outputs RGB + depth video, supports long-range world exploration, and includes auto-regressive inference for scene extension.

## Why it matters

Enables controllable, traversable 3D scene generation from minimal input. Supports AR/VR, robotics, and content creation.

## Key trade-offs / limitations

* High compute/memory requirements for large scenes.
* Some artifacts in depth alignment and geometry remain.
* Not fully real-time interactive.

## Link

[arXiv:2506.04225](https://arxiv.org/abs/2506.04225)


# ImageGS (2025)
Source: https://docs.daydream.live/knowledge-hub/research-references/imageGS

Content-Adaptive Image Representation via 2D Gaussians

## Overview

Image-GS represents images using content-adaptive 2D Gaussians that are progressively optimized through a custom differentiable renderer. It adaptively allocates anisotropic, colored Gaussians based on local image complexity, achieving efficient compression with hardware-friendly random access.

## Why it matters

Enables flexible, real-time image representation for graphics applications with only 0.3K MACs per pixel decode. Supports texture compression, semantics-aware compression, and joint compression-restoration while maintaining visual quality in low-bitrate scenarios.

## Key trade-offs / limitations

* Linear scaling with number of Gaussians affects memory for complex images
* Progressive optimization requires multiple training iterations
* Performance depends on effective gradient-based initialization
* Limited to 2D image representation (no temporal dynamics yet)

**Link**
[arXiv:2407.01866](https://arxiv.org/abs/2407.01866)


# Research and Foundations
Source: https://docs.daydream.live/knowledge-hub/research-references/index

Explore cutting-edge research in real-time video AI, world models, and generative video technologies

# Research and Foundations

Daydream Research Hub is your gateway to understanding the cutting-edge science behind real-time video AI. Here you'll find curated research papers, model implementations, and methodologies that are shaping the future of interactive AI video generation.

***

## About Video and World Models

Video generation and **world models** are rapidly converging toward a common goal: **real-time, interactive, persistent simulations of the world.** This convergence is reshaping AI for gaming, robotics, AR/VR, sports, and interactive media.

<Card title="Introduction to Video & World Models" icon="book-open" href="/knowledge-hub/research-references/about-video-and-world-models">
  Learn about the foundational concepts, how video models are evolving into world models, and why real-time performance matters for the future of AI video generation.
</Card>

***

## Browse Research by Category

Explore our curated collection of research papers and implementations organized by topic:

<CardGroup>
  <Card title="Models" icon="brain" href="#all-models">
    **20+ cutting-edge models** including Genie3, Aether, Diamond, TesserAct, and more. Discover the latest architectures and approaches for video generation and world modeling.
  </Card>

  <Card title="Methodologies and Benchmarking" icon="chart-line" href="#all-methodologies-and-benchmarking">
    **10+ methodologies and benchmarks** including WorldModelBench, WorldSimBench, MotionBench, and evaluation frameworks for assessing video and world model performance.
  </Card>
</CardGroup>

***

## All Models

<CardGroup>
  <Card title="Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling" href="/knowledge-hub/research-references/GeometryForcing">
    2025
  </Card>

  <Card title="Vid2Sim" href="/knowledge-hub/research-references/vid2sim">
    2025
  </Card>

  <Card title="Genie 3" href="/knowledge-hub/research-references/genie3">
    2025
  </Card>

  <Card title="HunyuanWorld-Voyager" href="/knowledge-hub/research-references/huanyuanworld-voyager">
    2025
  </Card>

  <Card title="Physically Plausible Video Generation via VLM Planning" href="/knowledge-hub/research-references/physically-plausible">
    2025
  </Card>

  <Card title="Pre-Trained Video Generative Models as World Simulators" href="/knowledge-hub/research-references/pretrained-world-dws">
    2025
  </Card>

  <Card title="ProphetDWM" href="/knowledge-hub/research-references/prophetdwm">
    2025
  </Card>

  <Card title="TesserAct: Learning 4D Embodied World Models" href="/knowledge-hub/research-references/tesserAct">
    2025
  </Card>

  <Card title="Vid2World" href="/knowledge-hub/research-references/vid2world" />

  <Card title="Aether: Geometric-Aware Unified World Modeling" href="/knowledge-hub/research-references/aether">
    2025
  </Card>

  <Card title="DIAMOND" href="/knowledge-hub/research-references/diamond">
    2024
  </Card>
</CardGroup>

***

## All Methodologies and Benchmarking

<CardGroup>
  <Card title="SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching" href="/knowledge-hub/research-references/SpeCa">
    2025
  </Card>

  <Card title="ImageGS" href="/knowledge-hub/research-references/imageGS">
    2025
  </Card>

  <Card title="FreeTimeGS" href="/knowledge-hub/research-references/FreeTimeGS">
    2025
  </Card>

  <Card title="WorldModelBench" href="/knowledge-hub/research-references/worldmodelbench" />

  <Card title="WorldSimBench" href="/knowledge-hub/research-references/worldsimbench" />

  <Card title="How Far is Video Generation from World Model?" href="/knowledge-hub/research-references/video-vs-world-how-far" />

  <Card title="Morpheus" href="/knowledge-hub/research-references/morpheus">
    2025
  </Card>

  <Card title="MotionBench" href="/knowledge-hub/research-references/motionbench">
    2025
  </Card>

  <Card title="Learning World Models for Interactive Video Generation" href="/knowledge-hub/research-references/learningworldmodels">
    2025
  </Card>

  <Card title="Long-Context State-Space Video World Models" href="/knowledge-hub/research-references/longcontext">
    2025
  </Card>

  <Card title="Video World Models with Long-term Spatial Memory" href="/knowledge-hub/research-references/VWMSpacialMemory">
    2025
  </Card>

  <Card title="Panacea" href="/knowledge-hub/research-references/panacea">
    2024
  </Card>

  <Card title="BEVControl" href="/knowledge-hub/research-references/bevcontrol">
    2023
  </Card>

  <Card title="BEVGen" href="/knowledge-hub/research-references/bevgen">
    2023
  </Card>
</CardGroup>

***

## Join Our Communities

<CardGroup>
  <Card title="Join Discord" icon="discord" href="https://discord.com/invite/5sZu8xmn6U">
    Discuss research, share insights, and connect with researchers in our community
  </Card>

  <Card title="Contribute on GitHub" icon="github" href="https://github.com/daydreamlive/scope">
    Contribute to Scope, our open-source tool for running real-time AI video pipelines
  </Card>
</CardGroup>


# Learning World Models for Interactive Video Generation (2025)
Source: https://docs.daydream.live/knowledge-hub/research-references/learningworldmodels

Benchmark & method for video generation models with action conditioning and memory to reduce compounding error.

## Overview

Chen et al. introduce VRAG (Video Retrieval Augmented Generation), which augments image-to-video models with action conditioning and explicit global state conditioning to address compounding errors and memory issues in long video generation. Their benchmark highlights where current video world models fail regarding consistency and responsiveness to actions. :contentReference\[oaicite:1]

## Why it matters

Interactive video generation is essential for applications like simulation and games, where user or agent actions affect what happens next. Handling compounding error and ensuring the model remains consistent over longer sequences is necessary for credible-world behavior.

## Key trade-offs / limitations

* More components (memory, retrieval) means more compute and possibly slower inference.
* The approach may be sensitive to how well action conditioning aligns with actual dynamics in the domain.
* Handling diverse environments (lighting, texture, motion complexity) may still be challenging.

## Link

[arXiv:2505.21996](https://arxiv.org/abs/2505.21996)


# Long-Context State-Space Video World Models (2025)
Source: https://docs.daydream.live/knowledge-hub/research-references/longcontext

Combining state-space models and dense local attention to scale video world models with long context.

## Overview

This paper proposes using state-space models (SSMs) in conjunction with dense local attention to maintain spatial consistency while extending the temporal context length of video world models. It addresses the difficulty of scaling to long horizons due to the usual cost of attention. :contentReference\[oaicite:3]

## Why it matters

Long horizon consistency is a big challenge in world modeling: errors compound over time, memory is limited, and many methods break when asked to maintain scenes over extended durations. Solutions that scale temporal memory without huge inefficiency are key.

## Key trade-offs / limitations

* Might sacrifice some fine detail spatial resolution for longer temporal coverage.
* SSMs introduce their own complexity and sometimes harder optimization.
* Benchmarks may not fully reflect real-world varying conditions (lighting, viewpoint changes) that stress the model.

## Link

[arXiv:2505.20171](https://arxiv.org/abs/2505.20171)


# Morpheus (2025)
Source: https://docs.daydream.live/knowledge-hub/research-references/morpheus

Benchmark for testing video generative models against real-world physical conservation laws.

## Overview

Morpheus is a benchmark with real-world videos of physical phenomena that obey conservation laws (mass, energy, momentum). It uses physics-informed metrics, neural networks, and VLMs to evaluate whether generative models respect these laws.

## Why it matters

Unlike synthetic benchmarks, Morpheus grounds evaluation in real physics. It highlights where models violate physical consistency, essential for robotics, simulation, and engineering uses.

## Key trade-offs / limitations

* Benchmark scope limited to curated physical setups.
* Real data is harder to scale than synthetic benchmarks.

## Link

[arXiv:2504.02918](https://arxiv.org/abs/2504.02918)


# MotionBench (2025)
Source: https://docs.daydream.live/knowledge-hub/research-references/motionbench

Benchmark for evaluating fine-grained motion understanding in vision-language models.

## Overview

MotionBench introduces a benchmark focused on fine-grained motion comprehension for vision-language models (VLMs). It includes \~8,000 video/question pairs across tasks like motion recognition, motion location, action order, and repetition counting, drawn from both real and synthetic video sources. The authors also propose “Through-Encoder Fusion” to better preserve motion information. Results show current VLMs struggle, often scoring below 60%.

## Why it matters

This fills a gap in evaluating low-level temporal and motion perception, critical for robotics, surveillance, and medical video analysis. It highlights that architectural and input strategies are key levers for improving motion understanding.

## Key trade-offs / limitations

* Current models perform poorly on repeated or subtle motions.
* Dataset focuses on specific motion types; broader dynamics remain untested.

## Link

[arXiv:2501.02955](https://arxiv.org/abs/2501.02955)


# Panacea (2024)
Source: https://docs.daydream.live/knowledge-hub/research-references/panacea

Panoramic and controllable video generation framework for autonomous driving.

## Overview

Panacea proposes methods to generate panoramic, controllable video sequences tailored for driving settings, focusing on multi-view consistency and scene control (e.g., trajectory and weather). It’s intended as a data-engine for driving world models and perception training.

## Why it matters

Panoramic, controllable video generation supports better simulation and data augmentation for driving systems, enabling evaluation and training with diverse conditions.

## Key trade-offs / limitations

* Emphasis on driving panoramas may limit direct applicability to non-driving domains.
* Realistic long-horizon consistency remains a difficult challenge.


# Physically Plausible Video Generation via VLM Planning (2025)
Source: https://docs.daydream.live/knowledge-hub/research-references/physically-plausible

Two-stage framework for improving physical plausibility in video generation.

## Overview

This paper proposes a two-stage framework where a Vision Language Model performs coarse motion planning before a diffusion model generates final videos. This ensures motion plausibility and consistency across frames.

## Why it matters

Demonstrates a promising way to integrate reasoning into generation, producing videos with both realism and physical correctness. Useful for domains requiring reliable dynamics.

## Key trade-offs / limitations

* Dependent on the quality of the VLM’s planning.
* May increase inference time due to two-stage process.

## Link

[arXiv:2503.23368](https://arxiv.org/abs/2503.23368)


# Pre-Trained Video Generative Models as World Simulators (2025)
Source: https://docs.daydream.live/knowledge-hub/research-references/pretrained-world-dws

Transforming pre-trained video generators into controllable world simulators via action module + motion-reinforced loss

# Pre-Trained Video Generative Models as World Simulators (DWS) (2025)

## Overview

He et al. propose DWS, a method to convert pre-trained video generative models into action-conditioned world simulators. It adds a light action-conditioned module and introduces motion-reinforced loss for better dynamic consistency. Applications demonstrated across games and robotics, with improvements in action controllability. :contentReference\[oaicite:6]

## Why it matters

Repurposing existing generative models reduces the need to train from scratch, leverages massive internet-scale pretraining, while adding controllability important for real-world tasks (e.g. robotics, planning, simulation).

## Key trade-offs / limitations

* Pre-trained models may still have limitations in fine detail or domain mismatch.
* The added action module may have limited influence on complex dynamics.
* Trade-off between visual quality and controllability / dynamic correctness.

## Link

[arXiv:2502.07825](https://arxiv.org/abs/2502.07825)


# ProphetDWM (2025)
Source: https://docs.daydream.live/knowledge-hub/research-references/prophetdwm

End-to-end driving world model that jointly predicts video and control actions.

# ProphetDWM: An End-to-End Driving World Model for Joint Video and Action Prediction (2025)

## Overview

ProphetDWM is an autonomous driving world model that jointly predicts future video frames and driving actions. It features a diffusion-based transition module and an action learning module, trained jointly for alignment.

## Why it matters

Brings world models closer to real-world use cases by combining video imagination with action prediction, useful for self-driving and planning systems.

## Key trade-offs / limitations

* Specialized for driving; generalization to other domains untested.
* Requires large datasets like NuScenes for training.

## Link

[arXiv:2505.18650](https://arxiv.org/abs/2505.18650)


# TesserAct: Learning 4D Embodied World Models (2025)
Source: https://docs.daydream.live/knowledge-hub/research-references/tesserAct

Predicting RGB-D-Normal videos → constructing coherent 4D scene evolutions with embodied agents.

# TesserAct: Learning 4D Embodied World Models (2025)

## Overview

Zhen et al. propose TesserAct, which learns world models from RGB-D-N (RGB, Depth, Normals) video data to produce temporally and spatially coherent predictions of 3D scenes as an agent acts. They also show improvements for novel view synthesis and policy learning over prior video-only world models. :contentReference\[oaicite:4]

## Why it matters

Embedding 3D geometry + normals adds richer structural information, improving fidelity in scene reconstruction and enabling more robust agents that can plan in the 3D world. Bridges gap between purely 2D video generation and embodied 3D world modeling.

## Key trade-offs / limitations

* Requires datasets with depth & normal annotations, which are harder to get and large.
* More computational load due to richer modalities.
* View synthesis and geometry fidelity may still degrade at edges/out-of-distribution views.

## Link

[arXiv:2504.20995](https://arxiv.org/abs/2504.20995)


# Vid2Sim (2025)
Source: https://docs.daydream.live/knowledge-hub/research-references/vid2sim

Realistic and Interactive Simulation from Video for Urban Navigation

## Overview

Vid2Sim bridges the sim-to-real gap by converting monocular videos into photorealistic, physically interactable 3D simulation environments. It enables RL training of visual navigation agents in complex urban environments using neural 3D scene reconstruction and simulation.

## Why it matters

Addresses the major challenge of sim-to-real transfer for robot learning by creating realistic digital twins from minimal video input. Enables scalable, cost-efficient training for urban navigation applications like food delivery bots and assistive vehicles.

## Key trade-offs / limitations

* Time-consuming scene building process requiring GLOMAP initialization
* Limited to 30 environments in current dataset (needs expansion for better performance)
* Requires extensive geometric processing for scene reconstruction
* Weather simulation capabilities still under development

## Link

[arxiv:2501.06693](https://arxiv.org/abs/2501.06693)


# Vid2World
Source: https://docs.daydream.live/knowledge-hub/research-references/vid2world

Adapting pre-trained video diffusion models to support interaction and action controllability

# Vid2World: Crafting Video Diffusion Models to Interactive World Models (2025)

## Overview

Huang et al. present Vid2World, a method that converts pre-trained video diffusion models into interactive world models by adding architectural adjustments and a training objective that enables autoregressive generation plus action controllability. They test in robotic manipulation and game simulations. :contentReference\[oaicite:2]

## Why it matters

Pretrained video diffusion models are powerful in generating realistic dynamics, but often lack control. This work provides a pathway to reuse those models in interactive settings, which expands their applicability in robotics, simulation, etc.

## Key trade-offs / limitations

* Action controllability may still be coarse depending on the domain.
* Diffusion models are heavy; adapting them can increase inference cost.
* Domains with complex geometry or occlusion may reduce quality.

## Link

[arXiv:2505.14357](https://arxiv.org/abs/2505.14357)


# How Far is Video Generation from World Model?
Source: https://docs.daydream.live/knowledge-hub/research-references/video-vs-world-how-far

Evaluates whether video generation models discover and obey classical mechanics laws.

# How Far is Video Generation from World Model: A Physical Law Perspective (2024)

## Overview

Kang et al. assess video generative models on principles of classical mechanics (collision, motion laws etc.) via synthetic 2D testbeds. They evaluate generalization in-distribution, out-of-distribution, combinatorial setups, and find models often fail to truly abstract physical laws.

## Why it matters

Tests if world models do more than memorize or overfit; whether they generalize fundamental dynamics. This is essential for physical plausibility in robotics, simulation, or any safety-critical system.

## Key trade-offs / limitations

* Synthetic environments may not capture the complexity of natural physics (texture, lighting, unmodeled forces etc.).
* Results might differ in noisy real video conditions.
* The testbeds are simplified, limiting the number of physics phenomena tested.

## Link

[arXiv:2411.02385](https://arxiv.org/abs/2411.02385)


# WorldSimBench (2024)
Source: https://docs.daydream.live/knowledge-hub/research-references/worldmodelbench

Dual evaluation framework for assessing video generation models as world simulators.

# WorldSimBench: Towards Video Generation Models as World Simulators (2024)

## Overview

WorldSimBench proposes a dual evaluation framework: (1) Explicit Perceptual Evaluation (visual quality, alignment with prompts) and (2) Implicit Manipulative Evaluation (whether generated videos can guide downstream embodied tasks). It introduces the HF-Embodied dataset and tests models on embodied control scenarios.

## Why it matters

Moves evaluation beyond aesthetics to actual usefulness. It bridges video generation with embodied AI and simulation, making it possible to assess whether generative models can serve as true world simulators.

## Key trade-offs / limitations

* Benchmarks highlight gaps but don’t prescribe specific solutions.
* Embodied tasks are limited to initial domains (robotics, driving).

## Link

[arXiv:2410.18072](https://arxiv.org/abs/2410.18072)


# WorldSimBench (2024)
Source: https://docs.daydream.live/knowledge-hub/research-references/worldsimbench

Dual evaluation framework for assessing video generation models as world simulators.

# WorldSimBench: Towards Video Generation Models as World Simulators (2024)

## Overview

WorldSimBench proposes a dual evaluation framework: (1) Explicit Perceptual Evaluation (visual quality, alignment with prompts) and (2) Implicit Manipulative Evaluation (whether generated videos can guide downstream embodied tasks). It introduces the HF-Embodied dataset and tests models on embodied control scenarios.

## Why it matters

Moves evaluation beyond aesthetics to actual usefulness. It bridges video generation with embodied AI and simulation, making it possible to assess whether generative models can serve as true world simulators.

## Key trade-offs / limitations

* Benchmarks highlight gaps but don’t prescribe specific solutions.
* Embodied tasks are limited to initial domains (robotics, driving).

## Link

[arXiv:2410.18072](https://arxiv.org/abs/2410.18072)


# SD1.5
Source: https://docs.daydream.live/parameters/SD15

Information of SD15 parameter configuration

These are the parameters available when using the SD1.5 model.

## Available Controlet for only SD1.5

<Callout icon="sliders">
  **SD1.5 Models** (`prompthero/openjourney-v4`):

  * `lllyasviel/control_v11f1p_sd15_depth`: Depth-based guidance for spatial structure preservation
  * `lllyasviel/control_v11f1e_sd15_tile`: Tile-based pattern control for texture preservation
  * `lllyasviel/control_v11p_sd15_canny`: Canny edge detection for detailed outline preservation
</Callout>

## Available Parameters for only SD1.5

<Note>
  Parameters are <u>**also**</u> loaded with these values as default when creating a new stream on <u>**this**</u> specific pipeline.
</Note>

```json theme={null}
{
  "params": {
    "seed": 789,
    "delta": 0.7,
    "width": 512,
    "height": 512,
    "prompt": "blooming flower",
    "model_id": "Lykon/dreamshaper-8",
    "lora_dict": null,
    "ip_adapter": {
      "scale": 0.5,
      "enabled": true
    },
    "controlnets": [
      {
        "enabled": true,
        "model_id": "lllyasviel/control_v11f1p_sd15_depth",
        "preprocessor": "depth_tensorrt",
        "conditioning_scale": 0,
        "preprocessor_params": {}
      },
      {
        "enabled": true,
        "model_id": "lllyasviel/control_v11f1e_sd15_tile",
        "preprocessor": "feedback",
        "conditioning_scale": 0.1,
        "preprocessor_params": {},
        "control_guidance_end": 1,
        "control_guidance_start": 0
      }
    ],
    "lcm_lora_id": "latent-consistency/lcm-lora-sdv1-5",
    "acceleration": "tensorrt",
    "do_add_noise": true,
    "t_index_list": [5, 15, 32],
    "use_lcm_lora": true,
    "guidance_scale": 1,
    "negative_prompt": "blurry, low quality, flat, 2d",
    "num_inference_steps": 50,
    "use_denoising_batch": true,
    "normalize_seed_weights": true,
    "normalize_prompt_weights": true,
    "seed_interpolation_method": "linear",
    "ip_adapter_style_image_url": "https://upload.wikimedia.org/wikipedia/commons/f/f0/Fractal_Xaos_psychedelic.png",
    "enable_similar_image_filter": false,
    "prompt_interpolation_method": "slerp",
    "similar_image_filter_threshold": 0.98,
    "similar_image_filter_max_skip_frame": 10
  }
}
```


# SDTurbo
Source: https://docs.daydream.live/parameters/SDTurbo

Information of SDTurbo parameter configuration

These are the parameters available when using the SDTurbo model.

## Available Controlet for only SDTurbo

<Callout icon="sliders">
  **SD2.1 Models** (`stabilityai/sd-turbo`):

  * `thibaud/controlnet-sd21-openpose-diffusers`: Body and hand pose tracking to maintain human poses in the output
  * `thibaud/controlnet-sd21-hed-diffusers`: Soft edge detection preserving smooth edges and contours
  * `thibaud/controlnet-sd21-canny-diffusers`: Sharp edge preservation with crisp outlines and details
  * `thibaud/controlnet-sd21-depth-diffusers`: Preserves spatial depth and 3D structure of objects and faces
  * `thibaud/controlnet-sd21-color-diffusers`: Color composition passthrough to maintain palette and composition
</Callout>

## Available Parameters for only SDTurbo

<Note>
  Parameters are <u>**also**</u> loaded with these values as default when creating a new stream on <u>**this**</u> specific pipeline.
</Note>

```json theme={null}
{
  "params": {
    "seed": 789,
    "delta": 0.7,
    "width": 512,
    "height": 512,
    "prompt": "Superman",
    "model_id": "stabilityai/sd-turbo",
    "lora_dict": null,
    "controlnets": [
      {
        "enabled": true,
        "model_id": "thibaud/controlnet-sd21-openpose-diffusers",
        "preprocessor": "pose_tensorrt",
        "conditioning_scale": 0.711,
        "preprocessor_params": {},
        "control_guidance_end": 1,
        "control_guidance_start": 0
      },
      {
        "enabled": true,
        "model_id": "thibaud/controlnet-sd21-hed-diffusers",
        "preprocessor": "soft_edge",
        "conditioning_scale": 0.2,
        "preprocessor_params": {},
        "control_guidance_end": 1,
        "control_guidance_start": 0
      },
      {
        "enabled": true,
        "model_id": "thibaud/controlnet-sd21-canny-diffusers",
        "preprocessor": "canny",
        "conditioning_scale": 0.2,
        "preprocessor_params": {
          "low_threshold": 100,
          "high_threshold": 200
        },
        "control_guidance_end": 1,
        "control_guidance_start": 0
      },
      {
        "enabled": true,
        "model_id": "thibaud/controlnet-sd21-depth-diffusers",
        "preprocessor": "depth_tensorrt",
        "conditioning_scale": 0.5,
        "preprocessor_params": {},
        "control_guidance_end": 1,
        "control_guidance_start": 0
      },
      {
        "enabled": true,
        "model_id": "thibaud/controlnet-sd21-color-diffusers",
        "preprocessor": "passthrough",
        "conditioning_scale": 0.2,
        "preprocessor_params": {},
        "control_guidance_end": 1,
        "control_guidance_start": 0
      }
    ],
    "lcm_lora_id": "latent-consistency/lcm-lora-sdv1-5",
    "acceleration": "tensorrt",
    "do_add_noise": true,
    "t_index_list": [12, 20, 24],
    "use_lcm_lora": true,
    "guidance_scale": 1,
    "negative_prompt": "blurry, low quality, flat, 2d",
    "num_inference_steps": 25,
    "use_denoising_batch": true,
    "normalize_seed_weights": true,
    "normalize_prompt_weights": true,
    "seed_interpolation_method": "linear",
    "enable_similar_image_filter": false,
    "prompt_interpolation_method": "slerp",
    "similar_image_filter_threshold": 0.98,
    "similar_image_filter_max_skip_frame": 10
  }
}
```


# SDXL
Source: https://docs.daydream.live/parameters/SDXL

Information of SDXL parameter configuration

These are the parameters available when using the SDXL model.

## Available Controlet for only SDXL

<Callout icon="sliders">
  **SDXL Models** (`stabilityai/sdxl-turbo`):

  * `xinsir/controlnet-depth-sdxl-1.0`: High-resolution depth guidance for SDXL models
  * `xinsir/controlnet-canny-sdxl-1.0`: SDXL-optimized canny edge detection
  * `xinsir/controlnet-tile-sdxl-1.0`: Tile-based control for SDXL texture generation
</Callout>

## Available Parameters for only SDXL

<Note>
  Parameters are <u>**also**</u> loaded with these values as default when creating a new stream on <u>**this**</u> specific pipeline.
</Note>

```json theme={null}
{
  "params": {
    "seed": 789,
    "delta": 0.7,
    "width": 512,
    "height": 512,
    "prompt": "blooming flower",
    "model_id": "stabilityai/sdxl-turbo",
    "lora_dict": null,
    "ip_adapter": {
      "scale": 0.5,
      "enabled": true
    },
    "controlnets": [
      {
        "enabled": true,
        "model_id": "xinsir/controlnet-depth-sdxl-1.0",
        "preprocessor": "depth_tensorrt",
        "conditioning_scale": 0.4,
        "preprocessor_params": {},
        "control_guidance_end": 1,
        "control_guidance_start": 0
      },
      {
        "enabled": true,
        "model_id": "xinsir/controlnet-canny-sdxl-1.0",
        "preprocessor": "canny",
        "conditioning_scale": 0.1,
        "preprocessor_params": {},
        "control_guidance_end": 1,
        "control_guidance_start": 0
      },
      {
        "enabled": true,
        "model_id": "xinsir/controlnet-tile-sdxl-1.0",
        "preprocessor": "feedback",
        "conditioning_scale": 0.1,
        "preprocessor_params": {},
        "control_guidance_end": 1,
        "control_guidance_start": 0
      }
    ],
    "acceleration": "tensorrt",
    "do_add_noise": true,
    "t_index_list": [5, 15, 32],
    "use_lcm_lora": true,
    "guidance_scale": 1,
    "negative_prompt": "blurry, low quality, flat, 2d",
    "num_inference_steps": 50,
    "use_denoising_batch": true,
    "normalize_seed_weights": true,
    "normalize_prompt_weights": true,
    "seed_interpolation_method": "linear",
    "ip_adapter_style_image_url": "https://upload.wikimedia.org/wikipedia/commons/f/f0/Fractal_Xaos_psychedelic.png",
    "enable_similar_image_filter": false,
    "prompt_interpolation_method": "slerp",
    "similar_image_filter_threshold": 0.98,
    "similar_image_filter_max_skip_frame": 10
  }
}
```


# SDXL FaceID
Source: https://docs.daydream.live/parameters/SDXL-turbo-faceid

Information of SDXL FaceID parameter configuration

These are the parameters available when using the SDXL Turbo FaceID model.

## Available Controlet for only SDXL Turbo FaceID

<Callout icon="sliders">
  **SDXL Models** (`stabilityai/sdxl-turbo`):

  * `xinsir/controlnet-depth-sdxl-1.0`: High-resolution depth guidance for SDXL models
  * `xinsir/controlnet-canny-sdxl-1.0`: SDXL-optimized canny edge detection
  * `xinsir/controlnet-tile-sdxl-1.0`: Tile-based control for SDXL texture generation
</Callout>

## Available Parameters for only SDXL Turbo FaceID

<Note>
  Parameters are <u>**also**</u> loaded with these values as default when creating a new stream on <u>**this**</u> specific pipeline.
</Note>

```json theme={null}
{
  "params": {
    "seed": 789,
    "delta": 0.7,
    "width": 512,
    "height": 512,
    "prompt": "mona lisa, face-focused webcam portrait blooming into stained-glass prisms and aurora ribbons, retro studio glow, friendly cardigan vibes, dreamlike, cinematic, ultra-detailed",
    "model_id": "stabilityai/sdxl-turbo",
    "lora_dict": null,
    "ip_adapter": {
      "type": "faceid",
      "scale": 1,
      "enabled": true
    },
    "controlnets": [
      {
        "enabled": true,
        "model_id": "xinsir/controlnet-depth-sdxl-1.0",
        "preprocessor": "depth_tensorrt",
        "conditioning_scale": 0.4,
        "preprocessor_params": {},
        "control_guidance_end": 1,
        "control_guidance_start": 0
      },
      {
        "enabled": true,
        "model_id": "xinsir/controlnet-canny-sdxl-1.0",
        "preprocessor": "canny",
        "conditioning_scale": 0.1,
        "preprocessor_params": {},
        "control_guidance_end": 1,
        "control_guidance_start": 0
      },
      {
        "enabled": true,
        "model_id": "xinsir/controlnet-tile-sdxl-1.0",
        "preprocessor": "feedback",
        "conditioning_scale": 0.1,
        "preprocessor_params": {},
        "control_guidance_end": 1,
        "control_guidance_start": 0
      }
    ],
    "acceleration": "tensorrt",
    "do_add_noise": true,
    "t_index_list": [36],
    "use_lcm_lora": true,
    "guidance_scale": 1,
    "negative_prompt": "blurry, low quality, flat, 2d",
    "num_inference_steps": 50,
    "use_denoising_batch": true,
    "normalize_seed_weights": true,
    "normalize_prompt_weights": true,
    "seed_interpolation_method": "linear",
    "ip_adapter_style_image_url": "https://ipfs.livepeer.com/ipfs/bafkreifyvsoa6pbt5r5ahedl6ann4nsp7m2vezkjuwn7uvxuqbie2un4wi",
    "enable_similar_image_filter": false,
    "prompt_interpolation_method": "slerp",
    "similar_image_filter_threshold": 0.98,
    "similar_image_filter_max_skip_frame": 10
  }
}
```


# Quickstart
Source: https://docs.daydream.live/quickstart

Learn how to get a video stream up and running and apply some AI on it. New to live video or streaming concepts? Check out our [Streaming Basics](/streaming-basics/live-video) guide to learn how ingest, delivery, and playback work behind the scenes.

## Overview

This guide will take you through the process of sending video input to our StreamDiffusion pipeline. You will learn how to adjust parameters to create a variety of visual effects, utilize live streaming and audio interactivity features, generate real-time visuals, and view the resulting output video.

Our goal by the end is to have an effect that will transform a user into an [anime character](https://app.daydream.live/creators/ericxtang/anime-character-transformation) via their webcam.

<video />

## API Auth

<Info>
  The use of the API key is currently subsidized for a limited time, and we will provide an update on pricing in the future.
</Info>

Get your API key from the [Daydream Dashboard](https://app.daydream.live/dashboard/api-keys). Keep it secure and never commit it to source control.

## Creating Your First App

Building on top of our StreamDiffusion pipeline consists of three parts:

1. Creating a `Stream` object (backend)
2. Sending in video and playing the output (frontend)
3. Setting StreamDiffusion parameters

***

## Using the SDKs

The easiest way to integrate Daydream is with our SDKs. Here's a full-stack example using the TypeScript SDK (backend) and Browser SDK (frontend).

### Backend: Create a Stream

Install the TypeScript SDK:

```bash theme={null}
npm install @daydreamlive/sdk
```

Create a stream with your API key:

```typescript theme={null}
// server.ts or Next.js Server Action
import { Daydream } from "@daydreamlive/sdk";

const daydream = new Daydream({
  bearer: process.env.DAYDREAM_API_KEY,
});

export async function createStream() {
  const stream = await daydream.streams.create({
    pipeline: "streamdiffusion",
    params: {
      modelId: "stabilityai/sdxl-turbo",
      prompt: "anime character",
    },
  });

  return {
    id: stream.id,
    whipUrl: stream.whipUrl,
    playbackId: stream.outputPlaybackId,
  };
}
```

### Frontend: Broadcast & Play

Install the Browser SDK:

```bash theme={null}
npm install @daydreamlive/browser
```

Broadcast your webcam and play the AI output:

```typescript theme={null}
import { createBroadcast, createPlayer } from "@daydreamlive/browser";

// Get whipUrl from your backend
const { whipUrl } = await createStream();

// Get user's webcam
const stream = await navigator.mediaDevices.getUserMedia({
  video: { width: 512, height: 512 },
});

// Start broadcasting
const broadcast = createBroadcast({ whipUrl, stream });
await broadcast.connect();

// The WHEP URL is available from the WHIP response header 'livepeer-playback-url'
// or access it from broadcast.whepUrl after connecting
const player = createPlayer(broadcast.whepUrl);
await player.connect();
player.attachTo(document.querySelector("video#output"));
```

<Tip>
  The WHEP playback URL is returned in the WHIP response header `livepeer-playback-url`. The Browser SDK automatically captures this for you in `broadcast.whepUrl`.
</Tip>

For React apps, see the [React Hooks guide](/sdks/browser/react-hooks) for a complete example with `useBroadcast` and `usePlayer`.

***

## Alternative: Using cURL + OBS

If you prefer to use cURL and OBS instead of the SDKs, here's how:

### 1. Create a Stream

<Note>
  Available models:

  * `stabilityai/sdxl-turbo` - SDXL, high quality (recommended)
  * `stabilityai/sd-turbo` - SD2.1, fastest
  * `Lykon/dreamshaper-8` - SD1.5, great for stylized effects
  * `prompthero/openjourney-v4` - SD1.5, artistic style
</Note>

```bash theme={null}
DAYDREAM_API_KEY="<YOUR_API_KEY>"

curl -X POST \
  "https://api.daydream.live/v1/streams" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer ${DAYDREAM_API_KEY}" \
  -d '{
    "pipeline": "streamdiffusion",
    "params": {
      "model_id": "stabilityai/sdxl-turbo",
      "prompt": "anime character"
    }
  }'
```

Response:

```json theme={null}
{
  "id": "str_gERGnGZE4331XBxW",
  "output_playback_id": "0d1crgzijlcsxpw4",
  "whip_url": "https://ai.livepeer.com/live/video-to-video/stk_abc123/whip"
}
```

### 2. Stream with OBS

1. Install [OBS](https://obsproject.com/)
2. Go to **Settings → Stream**
3. Set Service to `WHIP` and paste the `whip_url`
4. Add a video source and click **Start Streaming**
5. Watch at: `https://lvpr.tv/?v=<output_playback_id>`

<img alt="Streaming into Daydream via OBS" title="Streaming into Daydream via OBS" />

Or use the [Daydream OBS Plugin](/sdks/obs/installation) for built-in AI effects without needing to create streams manually.

***

## Update Parameters

Change the prompt or other settings in real-time:

<CodeGroup>
  ```typescript TypeScript SDK theme={null}
  await daydream.streams.update(streamId, {
    pipeline: "streamdiffusion",
    params: {
      modelId: "stabilityai/sdxl-turbo",
      prompt: "cyberpunk portrait, neon lights",
      guidanceScale: 1.2,
    },
  });
  ```

  ```bash cURL theme={null}
  curl -X PATCH \
    "https://api.daydream.live/v1/streams/${STREAM_ID}" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer ${DAYDREAM_API_KEY}" \
    -d '{
      "pipeline": "streamdiffusion",
      "params": {
        "model_id": "stabilityai/sdxl-turbo",
        "prompt": "cyberpunk portrait, neon lights",
        "guidance_scale": 1.2
      }
    }'
  ```
</CodeGroup>

<Note>
  You only need to include the parameters you want to change.
</Note>

## Add ControlNets

ControlNets preserve structure from your input video:

```json theme={null}
{
  "pipeline": "streamdiffusion",
  "params": {
    "model_id": "stabilityai/sdxl-turbo",
    "prompt": "oil painting portrait",
    "controlnets": [
      {
        "enabled": true,
        "model_id": "xinsir/controlnet-depth-sdxl-1.0",
        "preprocessor": "depth_tensorrt",
        "conditioning_scale": 0.5
      }
    ]
  }
}
```

<Warning>
  Set `conditioning_scale` to `0` to disable a ControlNet without triggering a pipeline reload.
</Warning>

### Available ControlNets

**SDXL Models** (`stabilityai/sdxl-turbo`):

* `xinsir/controlnet-depth-sdxl-1.0` - Depth guidance
* `xinsir/controlnet-canny-sdxl-1.0` - Edge detection
* `xinsir/controlnet-tile-sdxl-1.0` - Texture preservation

**SD1.5 Models** (`Lykon/dreamshaper-8`, `prompthero/openjourney-v4`):

* `lllyasviel/control_v11f1p_sd15_depth` - Depth
* `lllyasviel/control_v11f1e_sd15_tile` - Tile
* `lllyasviel/control_v11p_sd15_canny` - Canny edges

**SD2.1 Models** (`stabilityai/sd-turbo`):

* `thibaud/controlnet-sd21-depth-diffusers` - Depth
* `thibaud/controlnet-sd21-canny-diffusers` - Canny edges
* `thibaud/controlnet-sd21-openpose-diffusers` - Body poses
* `thibaud/controlnet-sd21-hed-diffusers` - Soft edges
* `thibaud/controlnet-sd21-color-diffusers` - Color composition

## What's Next?

* [TypeScript SDK](/sdks/typescript/installation) - Full server-side API
* [Browser SDK](/sdks/browser/installation) - WebRTC broadcasting
* [TouchDesigner Plugin](/sdks/touchdesigner/installation) - For VJ and creative apps
* [OBS Plugin](/sdks/obs/installation) - Add AI to OBS streams
* [Parameters Reference](/parameters/SDXL) - All available parameters
* [API Reference](/api-reference/create-stream) - Full API documentation


# Capability Map
Source: https://docs.daydream.live/reference/capability-map

Technical specifications for the Daydream API

<AccordionGroup>
  <Accordion title="Aspect Ratios" icon="window-maximize">
    | **StreamDiffusion** | **Aspect Ratio** |
    | :-----------------: | :--------------: |
    |       SD-Turbo      |      512x512     |
  </Accordion>

  <Accordion title="FPS" icon="video">
    <Note>Using RTX 4090</Note>

    | **Model** | **Denoising Step** | **FPS on Txt2Img** | **FPS on Img2Img** |
    | :-------: | :----------------: | :----------------: | :----------------: |
    |  SD-Turbo |          1         |       106.16       |       93.897       |
  </Accordion>
</AccordionGroup>


# Glossary
Source: https://docs.daydream.live/reference/glossary

List of terms and definitions encountered in the documentation

# Glossary

| Term            | Definition                                                                                                                                                                                                                                                                                                                                                                                                                         |
| --------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ControlNet      | An enhancement for Stable Diffusion models that allows users to accurately steer image creation by providing extra input maps such as depth, pose, edges, or segmentation.                                                                                                                                                                                                                                                         |
| HLS             | HLS (HTTP Live Streaming) is a streaming protocol developed by Apple that delivers audio and video over the internet by breaking media into small HTTP-based file segments listed in a playlist.                                                                                                                                                                                                                                   |
| Model           | A trained model is a mathematical system that has learned patterns from data, enabling it to make predictions or decisions on new inputs. After training adjusts and fine-tunes its internal settings, it can accurately perform specific tasks such as recognizing images or understanding text                                                                                                                                   |
| Negative Prompt | A content filter is a mechanism that directs the AI model to avoid generating certain types of content. It acts as a safeguard, steering the output away from undesired material, styles, or features to help ensure the results remain appropriate and aligned with the intended goals.                                                                                                                                           |
| Pipeline        | A pipeline is a composable workflow that processes live video in real time using AI. Each stage in the pipeline performs a specific AI task—such as style transfer, object detection, or live translation—on the video stream in sequence.                                                                                                                                                                                         |
| Prompt          | A prompt is a clear instruction that initiates the AI model’s response. It serves as the starting signal that tells the AI what task to perform or what type of output you want, guiding the direction of everything it generates.                                                                                                                                                                                                 |
| Seed            | A seed is a special number that determines the starting point for randomness in an AI’s image generation. Using the same seed with identical settings will produce the exact same image each time, ensuring that creative experiments can be repeated with consistent results.                                                                                                                                                     |
| StreamDiffusion | StreamDiffusion is a real-time image generation pipeline built for interactive and continuous image creation, especially in contexts where continuous input is expected—such as augmented reality, live streaming, and video game graphics—where maintaining high processing speed and throughput is essential.                                                                                                                    |
| TouchDesigner   | TouchDesigner is a visual programming tool created by Derivative for building real-time, interactive multimedia experiences. It’s widely used by artists and designers to create installations, live performances, generative art, and immersive environments, with support for video, audio, and hardware integration. Its procedural, node-based workflow lets users design complex projects visually and see results instantly. |
| WebRTC          | WebRTC (Web Real-Time Communication) is a technology that enables direct, peer-to-peer transmission of audio, video, and data between browsers or devices without requiring a server for the actual media flow. It’s optimized for low-latency, real-time communication such as video calls, live streaming, and interactive apps.                                                                                                 |
| WHIP            | WHIP (WebRTC-HTTP Ingestion Protocol) is a simple protocol for sending real-time media from a broadcaster to a media server using WebRTC over standard HTTP for signaling. It’s designed to make low-latency live streaming setups easier and more interoperable.                                                                                                                                                                  |


# FFMpeg Installation
Source: https://docs.daydream.live/reference/installing-ffmpeg

How to check if FFMpeg is installed and install it if needed

<Tip>
  This guide is ONLY necessary for users of StreamDiffusionTD
</Tip>

**Check if ffmpeg is installed:**

```bash theme={null}
ffmpeg -version
```

**Installation:**

**Windows**

1. Visit the [ffmpeg download page](https://www.gyan.dev/ffmpeg/)
2. Download the latest release build
3. Extract to `C:\ffmpeg`
4. Add `C:\ffmpeg\bin` to your system PATH

<img alt="Mac FFMpeg Setup" />

**macOS**

```bash theme={null}
# Using Homebrew
brew install ffmpeg

# Using MacPorts
sudo port install ffmpeg
```

<img alt="Mac FFMpeg Setup" />

**Linux (Ubuntu/Debian)**

```bash theme={null}
sudo apt update
sudo apt install ffmpeg
```


# Quick Start
Source: https://docs.daydream.live/scope/getting-started/quickstart

Get Scope running and generate your first AI video

# Get Started with Scope

Daydream Scope is an open-source tool for running and customizing real-time interactive generative AI pipelines and models.

Watch this 10-minute walkthrough covering the main Scope features and how to get your first generation running:

<iframe title="Daydream Scope Tutorial" />

Follow the steps below to install Scope and create your first generation.

***

## Prerequisites

**For Desktop App or Local Installation:**

* NVIDIA GPU with ≥24GB VRAM (RTX 4090/5090 or similar)
* CUDA 12.8+ drivers
* Windows or Linux

<Tip>
  **No powerful GPU?** Daydream Scope Cloud lets you run inference remotely - right from the app - without needing a local GPU. This means Mac users and anyone without a dedicated NVIDIA card can still use Scope. See the [Remote Inference guide](/scope/guides/remote-inference) to get started.
</Tip>

**For Cloud (RunPod):**

* RunPod account with credits
* Similar GPU requirements apply to your instance selection

<Card title="Full System Requirements" icon="server" href="/scope/reference/system-requirements">
  View detailed hardware specs, pipeline-specific VRAM needs, and software dependencies
</Card>

<Note>
  **Krea Realtime** requires ≥32GB VRAM and uses the **fp8 quantized model** at that tier (e.g. RTX 5090). For higher resolutions without quantization, ≥40GB VRAM is recommended (e.g. H100, RTX 6000 Pro). A 24GB GPU like the RTX 4090 cannot run Krea Realtime - use LongLive or StreamDiffusion V2 instead.
</Note>

***

## Step 1: Install Scope

Choose your installation method:

<Tabs>
  <Tab title="Desktop App" icon="download">
    <Tip>
      **Best for:** Windows users who want the easiest installation experience
    </Tip>

    The Daydream Scope desktop app is an Electron-based application that provides the simplest way to get Scope running on your Windows machine.

    <Card title="Download Daydream Scope for Windows" icon="download" href="https://github.com/daydreamlive/scope/releases/latest/download/DaydreamScope-Setup.exe">
      Download the latest Windows installer (.exe). This link always points to the most recent release.
    </Card>

    <Steps>
      <Step title="Install the application">
        Run the downloaded `.exe` file and follow the standard Windows installation prompts.
      </Step>

      <Step title="Launch Scope">
        Once installed, launch Daydream Scope from your Start menu or desktop shortcut.
      </Step>
    </Steps>

    <Accordion title="Looking for a specific version?">
      Visit the [Daydream Scope releases page](https://github.com/daydreamlive/scope/releases) on GitHub. Select the release you want, expand the **Assets** section at the bottom, and download the file ending in `.exe`.
    </Accordion>
  </Tab>

  <Tab title="Local Install" icon="desktop">
    <Tip>
      **Best for:** Developers who want to build on top of Scope - custom plugins, API integration, or running bleeding-edge code from the `main` branch before it ships in a release. You get full access to the Python environment and source code, but you manage dependencies (UV, Node.js, CUDA drivers) yourself. There are no auto-updates.
    </Tip>

    <Steps>
      <Step title="Check your GPU drivers">
        Verify that your NVIDIA drivers support CUDA 12.8 or higher:

        ```bash theme={null}
        nvidia-smi
        ```

        The output should show your GPU and a CUDA version of at least 12.8.
      </Step>

      <Step title="Install dependencies">
        You'll need:

        * **UV** (Python package manager)
        * **Node.js** and **npm**

        If you don't have these installed, visit their respective websites for installation instructions.
      </Step>

      <Step title="Clone the repository">
        ```bash theme={null}
        git clone git@github.com:daydreamlive/scope.git
        cd scope
        ```
      </Step>

      <Step title="Build frontend and install dependencies">
        ```bash theme={null}
        uv run build
        ```

        This installs all required Python packages including Torch and FlashAttention. First-time install may take a while.
      </Step>

      <Step title="Start the Scope server">
        ```bash theme={null}
        uv run daydream-scope
        ```

        On the first run, model weights will download automatically to `~/.daydream-scope/models`.
      </Step>

      <Step title="Access the UI">
        Open your browser and navigate to **[http://localhost:8000](http://localhost:8000)**
      </Step>
    </Steps>

    <Card title="Building custom plugins?" icon="puzzle-piece" href="/scope/guides/plugin-development">
      Local install is the recommended path for plugin development. See the plugin development guide to get started.
    </Card>
  </Tab>

  <Tab title="Cloud (RunPod)" icon="cloud">
    <Tip>
      **Best for:** Researchers and developers without access to local high-end GPUs
    </Tip>

    RunPod is a third-party cloud GPU service. We've created a template to make deployment simple.

    You'll need a RunPod account with credits. Create an account at [runpod.io](https://runpod.io) and add funds to get started.

    **Prefer video?** Watch the RunPod tutorial:

    <iframe title="RunPod Deployment Tutorial" />

    <Steps>
      <Step title="Access the Daydream Scope template">
        Click to deploy: [**Deploy Daydream Scope on RunPod**](https://console.runpod.io/deploy?template=aca8mw9ivw\&ref=5k8hxjq3)
      </Step>

      <Step title="Create a HuggingFace token">
        Required for TURN server functionality (WebRTC in cloud environments):

        1. Create account at [huggingface.co](https://huggingface.co)
        2. Navigate to **Settings > Access Tokens**
        3. Create a token with **read** permissions
        4. Copy the token

        <Info>
          HuggingFace provides 10GB of free streaming per month via Cloudflare TURN servers. For more details on authentication methods and troubleshooting, see the [HuggingFace Auth](/scope/guides/huggingface) guide.
        </Info>
      </Step>

      <Step title="Select your GPU">
        Choose a GPU with:

        * **Minimum:** ≥24GB VRAM
        * **Recommended:** NVIDIA RTX 4090/5090 or similar
        * **CUDA:** 12.8+ support
      </Step>

      <Step title="Configure environment variables">
        1. Click **"Edit Template"**
        2. Add variable: `HF_TOKEN` = your HuggingFace token
        3. Click **"Save"**
      </Step>

      <Step title="Deploy your instance">
        Click **"Deploy On-Demand"** and wait for deployment to complete.
      </Step>

      <Step title="Access your Scope instance">
        Open the app at **port 8000**: `https://your-instance-id.runpod.io:8000`
      </Step>
    </Steps>
  </Tab>
</Tabs>

***

## Step 2: Your First Generation

Once Scope is running, open the interface at `localhost:8000` (or your RunPod URL).

### Text-to-Video

The LongLive pipeline is pre-selected with a prompt describing a 3D animated panda walking through a park. Just hit **play** - you'll see the generation running in real-time, frame by frame, with no render queue or progress bar.

Stop the generation, try changing the prompt to something completely different, and hit play again:

* "a dragon flying through clouds over a volcano"
* "a robot walking on mars"
* "an astronaut floating through a neon city"

The generation adapts in real-time - you're steering it live, not queueing up renders.

### Video-to-Video

Now try switching the input mode from **Text** to **Video**. A looping cat test video is loaded by default. Hit play and watch the model transform the video based on your prompt while preserving its structure and motion.

Experiment with different prompts to see how the same source video gets reinterpreted:

* "a cow sitting in the grass"
* "a fish sitting in the grass"
* "a dragon sitting in the grass"

<Note>
  You can also use your **webcam** as a live input, load your own **video file**, or receive video from other applications via **Spout** (Windows only).
</Note>

### Explore Community Projects

See what others are creating with Scope:

<CardGroup>
  <Card title="Realtime Camera Restyle" icon="camera" href="https://app.daydream.live/creators/ericxtang/realtime-camera-restyle-on-ipad?utm_source=docs&utm_medium=web&utm_campaign=docs2com">
    Live camera feed restyled in real-time using VACE on an iPad
  </Card>

  <Card title="Flower Transformation" icon="seedling" href="https://app.daydream.live/creators/as.ws__/streamdiffusiontd-to-scope-seeded-case-study?utm_source=docs&utm_medium=web&utm_campaign=docs2com">
    A seeded case study exploring StreamDiffusion to Scope transition
  </Card>
</CardGroup>

<Card title="Browse all community projects" icon="globe" href="https://app.daydream.live/discover?utm_source=docs&utm_medium=web&utm_campaign=docs2com">
  Explore more creations, download timelines, and share your own work on the Daydream Community Hub
</Card>

***

## Step 3: Next Steps

Now that you're generating, here are some things to try:

<CardGroup>
  <Card title="Using LoRAs" icon="wand-magic-sparkles" href="/scope/guides/loras">
    Add style adapters to transform your generations - from photorealistic to Pixar with a single file
  </Card>

  <Card title="Using VACE" icon="image" href="/scope/guides/vace">
    Guide generation with reference images and control videos for character consistency
  </Card>

  <Card title="Using Spout" icon="share-nodes" href="/scope/guides/spout">
    Share real-time video with TouchDesigner, Unity, and other Windows applications
  </Card>
</CardGroup>

### Go Deeper

Ready to build programmatically? Scope exposes a powerful API for integration into your own applications.

<CardGroup>
  <Card title="API Reference" icon="brackets-curly" href="/scope/reference/api/index">
    Set up the server, connect via WebRTC, and control generation in real-time
  </Card>

  <Card title="Pipelines" icon="layer-group" href="/scope/reference/pipelines">
    Explore each pipeline's capabilities, parameters, and hardware requirements
  </Card>
</CardGroup>

### Supported Pipelines

Scope currently ships with five autoregressive video diffusion pipelines: [StreamDiffusion V2](/scope/reference/pipelines/streamdiffusion-v2), [LongLive](/scope/reference/pipelines/longlive), [Krea Realtime](/scope/reference/pipelines/krea-realtime), [RewardForcing](/scope/reference/pipelines/reward-forcing), and [MemFlow](/scope/reference/pipelines/memflow). Four run on 24GB GPUs, while Krea Realtime needs 32GB+ for its larger 14B model.

<Card title="Pipelines Overview" icon="layer-group" href="/scope/reference/pipelines">
  Compare all pipelines - features, VRAM requirements, and use cases at a glance
</Card>

***

## Step 4: Connect, Share & Contribute

<CardGroup>
  <Card title="Community Hub" icon="users" href="https://app.daydream.live/discover?utm_source=docs&utm_medium=web&utm_campaign=docs2com">
    Browse creations, download timelines, and share your work
  </Card>

  <Card title="Ask on Discord" icon="discord" href="https://discord.com/invite/5sZu8xmn6U">
    Have questions or want to connect with others? Join our friendly community
  </Card>

  <Card title="Contribute on GitHub" icon="github" href="https://github.com/daydreamlive/scope">
    Report issues, suggest features, or contribute code
  </Card>
</CardGroup>

***

## Troubleshooting

<AccordionGroup>
  <Accordion title="CUDA version mismatch">
    Run `nvidia-smi` and verify CUDA version is ≥ 12.8. Update your NVIDIA drivers if needed.
  </Accordion>

  <Accordion title="Build fails or dependencies won't install">
    * Ensure UV, Node.js, and npm are properly installed
    * Try clearing the cache: `uv cache clean && uv run build`
  </Accordion>

  <Accordion title="Python.h: No such file or directory">
    Install the Python development package:

    ```bash theme={null}
    # Debian/Ubuntu
    sudo apt-get install python3-dev
    ```
  </Accordion>

  <Accordion title="Models won't download">
    * Check your internet connection
    * Verify disk space in `~/.daydream-scope/models`
    * Model downloads can be large - be patient on first run
  </Accordion>

  <Accordion title="Can't connect to RunPod UI">
    * Verify the instance is fully deployed in RunPod dashboard
    * Ensure you're accessing port 8000
    * Check that `HF_TOKEN` is correctly set
  </Accordion>

  <Accordion title="WebRTC connection fails">
    * Verify your `HF_TOKEN` is valid with read permissions
    * Try redeploying the instance with the correct token
  </Accordion>
</AccordionGroup>


# HuggingFace Auth
Source: https://docs.daydream.live/scope/guides/huggingface

Set up your HuggingFace token for gated model downloads and WebRTC connections

# Setting up HuggingFace authentication

Some pipelines use gated models on HuggingFace that require authentication to download. A HuggingFace token is also used to obtain Cloudflare TURN credentials for WebRTC connections behind firewalls. You can create a token at [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens).

Scope supports two ways to provide your token: through the Settings UI or via an environment variable.

***

## Using the API Keys Tab

<Steps>
  <Step title="Open Settings">
    Click the **gear icon** in the app header to open the Settings dialog.
  </Step>

  <Step title="Navigate to API Keys">
    Select the **API Keys** tab.
  </Step>

  <Step title="Enter your token">
    Find the **HuggingFace** entry, paste your token into the field, and click the **save** button.
  </Step>
</Steps>

To remove a stored token, click the **delete** button next to the entry.

<Note>
  If the `HF_TOKEN` environment variable is already set, the input field is disabled.
</Note>

***

## Using an Environment Variable

Setting the `HF_TOKEN` environment variable is the preferred method for headless, cloud, and CI deployments.

<Tabs>
  <Tab title="Linux / macOS" icon="terminal">
    ```bash theme={null}
    export HF_TOKEN=hf_your_token_here
    ```
  </Tab>

  <Tab title="Windows (PowerShell)" icon="windows">
    ```powershell theme={null}
    $env:HF_TOKEN = "hf_your_token_here"
    ```
  </Tab>
</Tabs>

<Note>
  The environment variable takes precedence over a token stored through the UI.
</Note>

***

## When Is a Token Needed?

| Scenario                     | Why                                                                                                                                                                  |
| :--------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Gated model downloads**    | Pipelines that depend on gated HuggingFace models will fail with an authentication error without a valid token                                                       |
| **Cloudflare TURN (WebRTC)** | A token is used to obtain TURN server credentials for NAT traversal. Without it, Scope falls back to a public STUN server which may not work behind strict firewalls |

***

## Troubleshooting

<AccordionGroup>
  <Accordion title="Authentication errors during model download">
    Verify that your token is set in **Settings > API Keys** or via the `HF_TOKEN` environment variable.
  </Accordion>

  <Accordion title="Token is set but downloads still fail">
    Ensure you have accepted the model's license agreement on its HuggingFace page. Gated models might require explicit approval before access is granted.
  </Accordion>

  <Accordion title="WebRTC connection issues behind firewalls">
    Without a valid HuggingFace token, Scope cannot obtain Cloudflare TURN credentials and falls back to STUN-only mode. Set your token and restart Scope to enable TURN-based NAT traversal.
  </Accordion>
</AccordionGroup>

***

## See Also

<CardGroup>
  <Card title="Quick Start" icon="rocket" href="/scope/getting-started/quickstart">
    Get Scope running and generate your first AI video
  </Card>

  <Card title="System Requirements" icon="server" href="/scope/reference/system-requirements">
    GPU, RAM, and OS requirements per pipeline
  </Card>
</CardGroup>


# How-to Guides
Source: https://docs.daydream.live/scope/guides/index

Task-oriented guides for accomplishing specific goals with Scope

# Scope How-to Guides

Practical, task-oriented guides to help you solve specific problems and accomplish particular goals with Daydream Scope. These guides assume you have basic familiarity with Scope from the [Quick Start](/scope/getting-started/quickstart).

***

## Extend Scope with Plugins

<CardGroup>
  <Card title="Using Plugins" icon="plug" href="/scope/guides/plugins">
    Install and manage third-party pipeline plugins to extend Scope
  </Card>

  <Card title="Developing Plugins" icon="code" href="/scope/guides/plugin-development">
    Create your own custom pipelines and extend Scope's capabilities
  </Card>
</CardGroup>

## Generation Guides

<CardGroup>
  <Card title="Using LoRAs" icon="wand-magic-sparkles" href="/scope/guides/loras">
    Download and use LoRA adapters to customize concepts and styles in your generations
  </Card>

  <Card title="Using VACE" icon="image" href="/scope/guides/vace">
    Guide generation with reference images and control videos for advanced editing
  </Card>

  <Card title="Using Spout" icon="share-nodes" href="/scope/guides/spout">
    Share real-time video with TouchDesigner, Unity, and other Windows applications
  </Card>
</CardGroup>

## Setup

<CardGroup>
  <Card title="Remote Inference" icon="cloud" href="/scope/guides/remote-inference">
    Run Scope on cloud-hosted GPUs without local hardware (Beta)
  </Card>

  <Card title="HuggingFace Auth" icon="key" href="/scope/guides/huggingface">
    Set up your HuggingFace token for gated model downloads and WebRTC connections
  </Card>
</CardGroup>

***

## Go Deeper

Ready to go beyond the UI? Scope exposes a powerful API for programmatic control and integration into your own applications.

<CardGroup>
  <Card title="API Reference" icon="brackets-curly" href="/scope/reference/api/index">
    Set up the server, connect via WebRTC, and control generation in real-time
  </Card>

  <Card title="Pipelines" icon="layer-group" href="/scope/reference/pipelines">
    Explore each pipeline's capabilities, parameters, and hardware requirements
  </Card>
</CardGroup>

***

<Note>
  Have a specific guide request? Let us know on [Discord](https://discord.com/invite/5sZu8xmn6U) or [GitHub](https://github.com/daydreamlive/scope).
</Note>

***

## Open Source Contributions

Scope is open source, and we welcome your contributions!

<CardGroup>
  <Card title="Contribute on GitHub" icon="github" href="https://github.com/daydreamlive/scope">
    Submit code, tackle open issues, and help build the future of real-time AI video
  </Card>

  <Card title="Ask on Discord" icon="discord" href="https://discord.com/invite/5sZu8xmn6U">
    Have questions or want to connect with others? Join our friendly community
  </Card>
</CardGroup>


# Using LoRAs
Source: https://docs.daydream.live/scope/guides/loras

Customize concepts and styles in your generations with LoRA adapters

# Using LoRAs in Scope

LoRA (Low-Rank Adaptation) adapters allow you to customize the concepts and styles used in your generations. Scope supports loading one or multiple LoRAs to fine-tune your output.

***

## Pipeline Compatibility

Different pipelines support different LoRA model types. Make sure you download LoRAs that match your pipeline:

| Pipeline           | Compatible LoRAs |
| ------------------ | ---------------- |
| StreamDiffusion V2 | Wan2.1-T2V-1.3B  |
| LongLive           | Wan2.1-T2V-1.3B  |
| RewardForcing      | Wan2.1-T2V-1.3B  |
| MemFlow            | Wan2.1-T2V-1.3B  |
| Krea Realtime      | Wan2.1-T2V-14B   |

<Note>
  Krea Realtime uses the larger 14B model, so it requires different LoRAs than the other pipelines.
</Note>

***

## Recommended LoRAs

Here are some LoRAs to get you started:

### For 1.3B Pipelines (StreamDiffusion V2, LongLive, RewardForcing, MemFlow)

<CardGroup>
  <Card title="Arcane Jinx" icon="user" href="https://civitai.com/models/1332383/wan-lora-arcane-jinx-v1-wan-13b">
    Character LoRA for the Arcane art style
  </Card>

  <Card title="Genshin TCG" icon="cards" href="https://civitai.com/models/1728768/genshin-tcg-style-wan-13b">
    Stylized trading card game aesthetic
  </Card>
</CardGroup>

### For 14B Pipeline (Krea Realtime)

<CardGroup>
  <Card title="Origami" icon="paper-plane" href="https://huggingface.co/shauray/Origami_WanLora/tree/main">
    Paper-craft folded aesthetic
  </Card>

  <Card title="Film Noir" icon="film" href="https://huggingface.co/Remade-AI/Film-Noir">
    Classic black-and-white cinema style
  </Card>

  <Card title="Pixar" icon="clapperboard" href="https://huggingface.co/Remade-AI/Pixar">
    3D animated movie aesthetic
  </Card>
</CardGroup>

***

## Downloading LoRAs

Scope supports LoRAs from popular hubs like [HuggingFace](https://huggingface.co/) and [CivitAI](https://civitai.com/). The download process differs depending on whether you're running Scope locally or in the cloud.

<Tabs>
  <Tab title="Local Installation" icon="desktop">
    When running Scope on your local machine, you can download LoRA files directly through your browser.

    ### From HuggingFace

    <Steps>
      <Step title="Find the LoRA page">
        Navigate to the LoRA model page on HuggingFace.
      </Step>

      <Step title="Download the file">
        Click the download button next to the `.safetensors` file.

        <Frame>
          <img alt="HuggingFace download button" />
        </Frame>
      </Step>

      <Step title="Move to LoRA folder">
        Move the downloaded file to:

        ```text theme={null}
        ~/.daydream-scope/models/lora
        ```
      </Step>
    </Steps>

    ### From CivitAI

    <Steps>
      <Step title="Find the LoRA page">
        Navigate to the LoRA model page on CivitAI.
      </Step>

      <Step title="Download the file">
        Click the download button.

        <Frame>
          <img alt="CivitAI download button" />
        </Frame>
      </Step>

      <Step title="Move to LoRA folder">
        Move the downloaded file to:

        ```text theme={null}
        ~/.daydream-scope/models/lora
        ```
      </Step>
    </Steps>
  </Tab>

  <Tab title="Cloud (RunPod)" icon="cloud">
    When running Scope on a remote machine, you'll need to download LoRAs programmatically using `wget`.

    ### From HuggingFace

    <Steps>
      <Step title="Copy the download link">
        On the LoRA page, go to **Files and versions** and copy the link address for the `.safetensors` file.

        <video />
      </Step>

      <Step title="Download via terminal">
        Navigate to the LoRA folder and download:

        ```bash theme={null}
        cd ~/.daydream-scope/models/lora
        wget -O <filename>.safetensors "<link_address>"
        ```

        **Example:**

        ```bash theme={null}
        wget -O pixar_10_epochs.safetensors "https://huggingface.co/Remade-AI/Pixar/resolve/main/pixar_10_epochs.safetensors?download=true"
        ```
      </Step>
    </Steps>

    ### From CivitAI

    <Steps>
      <Step title="Get a CivitAI API key">
        CivitAI requires an API key for programmatic downloads.

        1. Create an account at [civitai.com](https://civitai.com)
        2. Go to [Developer Settings](https://developer.civitai.com/docs/getting-started/setup-profile)
        3. Generate an API key
      </Step>

      <Step title="Copy the download link">
        On the LoRA page, copy the download link address.

        <video />
      </Step>

      <Step title="Download via terminal">
        Navigate to the LoRA folder and download with your API token:

        ```bash theme={null}
        cd ~/.daydream-scope/models/lora
        wget -O <filename>.safetensors "<link_address>&token=<YOUR_API_KEY>"
        ```

        **Example:**

        ```bash theme={null}
        wget -O arcane-jinx.safetensors "https://civitai.com/api/download/models/1679582?type=Model&format=SafeTensor&token=YOUR_API_KEY"
        ```

        <Warning>
          Make sure to wrap the URL in double quotes - the `&` character will cause issues otherwise.
        </Warning>
      </Step>
    </Steps>
  </Tab>
</Tabs>

***

## Loading LoRAs

Once downloaded, your LoRAs will appear in the Scope interface under the LoRA section. You can:

* **Load multiple LoRAs** simultaneously for combined effects
* **Adjust the scale** of each LoRA to control its influence
* **Hot-swap LoRAs** without restarting the pipeline (in runtime mode)

<Note>
  For runtime LoRA scale adjustments, load your pipeline with `lora_merge_mode: "runtime_peft"`. This enables dynamic scale updates but may slightly reduce FPS compared to permanent merge mode.
</Note>

***

## Try Community Examples

Want to see LoRAs in action? Check out this walkthrough from the Community Hub:

<Card title="From Photorealistic to Pixar in Real-Time" icon="wand-magic-sparkles" href="https://app.daydream.live/creators/viborc/lora-adapters-in-scope-from-photorealistic-to-pixar-in-real-time?utm_source=docs&utm_medium=web&utm_campaign=docs2com">
  Watch how a single LoRA transforms a puppy scene from photorealistic to Pixar animation style - includes downloadable timeline files
</Card>

***

## See Also

<CardGroup>
  <Card title="Quick Start" icon="rocket" href="/scope/getting-started/quickstart">
    Get Scope running if you haven't already
  </Card>

  <Card title="VACE Guide" icon="image" href="/scope/guides/vace">
    Use reference images to guide your generations
  </Card>
</CardGroup>


# Developing Plugins
Source: https://docs.daydream.live/scope/guides/plugin-development

Create custom pipelines and extend Scope with your own plugins

# Developing plugins for Scope

Create your own Scope plugins to add custom pipelines, preprocessors, or postprocessors. This guide walks through building a plugin from scratch with working examples.

***

## Prerequisites

* Python 3.12 or newer
* [uv](https://docs.astral.sh/uv/) package manager
* Scope installed locally for testing

***

## Project Setup

Create a new directory with the following structure:

```
my-scope-plugin/
├── pyproject.toml
└── my_scope_plugin/
    ├── __init__.py
    ├── plugin.py
    └── pipelines/
        ├── __init__.py
        ├── schema.py
        └── pipeline.py
```

### pyproject.toml

```toml theme={null}
[project]
name = "my-scope-plugin"
version = "0.1.0"
requires-python = ">=3.12"

[project.entry-points."scope"]
my_scope_plugin = "my_scope_plugin.plugin"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"
```

The `[project.entry-points."scope"]` section registers your plugin with Scope. The key (`my_scope_plugin`) is your plugin name, and the value points to the module containing your hook implementation.

<Note>
  If your plugin needs additional third-party packages, add them to `[project.dependencies]` in `pyproject.toml` - Scope installs them automatically. You don't need to declare packages that Scope already provides (e.g. `torch`, `pydantic`) since they're available from the host environment.
</Note>

### plugin.py

```python theme={null}
from scope.core.plugins.hookspecs import hookimpl


@hookimpl
def register_pipelines(register):
    from .pipelines.pipeline import MyPipeline

    register(MyPipeline)
```

The `register_pipelines` hook is called when Scope loads your plugin. Call `register()` for each pipeline class you want to make available.

***

## Creating a Text-Only Pipeline

A text-only pipeline generates video without requiring input video. This is the simplest type of pipeline.

### Example: Color Generator

This pipeline generates solid color frames based on configurable RGB values.

**pipelines/schema.py:**

```python theme={null}
from pydantic import Field

from scope.core.pipelines.base_schema import BasePipelineConfig, ModeDefaults


class ColorGeneratorConfig(BasePipelineConfig):
    """Configuration for the Color Generator pipeline."""

    pipeline_id = "color-generator"
    pipeline_name = "Color Generator"
    pipeline_description = "Generates solid color frames"

    # No prompts needed
    supports_prompts = False

    # Text mode only (no video input required)
    modes = {"text": ModeDefaults(default=True)}

    # Custom parameters: RGB values 0-255
    color_r: int = Field(default=128, ge=0, le=255, description="Red component")
    color_g: int = Field(default=128, ge=0, le=255, description="Green component")
    color_b: int = Field(default=128, ge=0, le=255, description="Blue component")
```

**pipelines/pipeline.py:**

```python theme={null}
from typing import TYPE_CHECKING

import torch

from scope.core.pipelines.interface import Pipeline

from .schema import ColorGeneratorConfig

if TYPE_CHECKING:
    from scope.core.pipelines.base_schema import BasePipelineConfig


class ColorGeneratorPipeline(Pipeline):
    """Generates solid color frames."""

    @classmethod
    def get_config_class(cls) -> type["BasePipelineConfig"]:
        return ColorGeneratorConfig

    def __init__(
        self,
        height: int = 512,
        width: int = 512,
        **kwargs,
    ):
        self.height = height
        self.width = width

    def __call__(self, **kwargs) -> dict:
        """Generate a solid color frame.

        Returns:
            Dict with "video" key containing tensor of shape (1, H, W, 3) in [0, 1] range.
        """
        # Read runtime parameters from kwargs (with defaults)
        color_r = kwargs.get("color_r", 128)
        color_g = kwargs.get("color_g", 128)
        color_b = kwargs.get("color_b", 128)

        # Create color tensor from current values
        color = torch.tensor([color_r / 255.0, color_g / 255.0, color_b / 255.0])

        # Create a single frame filled with our color
        frame = color.view(1, 1, 1, 3).expand(1, self.height, self.width, 3)
        return {"video": frame.clone()}
```

**Key points:**

* **No `prepare()` method**: Text-only pipelines don't need to request input frames
* **`modes = {"text": ModeDefaults(default=True)}`**: Declares this pipeline only supports text mode
* **`__call__` returns `{"video": tensor}`**: Tensor must be in THWC format with values in \[0, 1] range
* **Runtime parameters are read from `kwargs`**: Parameters like `color_r` are passed to `__call__()` and should be read using `kwargs.get()`

***

## Creating a Video Input Pipeline

A video input pipeline processes incoming video frames. It must implement `prepare()` to tell Scope how many input frames it needs.

### Example: Invert Colors

This pipeline inverts the colors of input video frames.

**pipelines/schema.py:**

```python theme={null}
from scope.core.pipelines.base_schema import BasePipelineConfig, ModeDefaults


class InvertConfig(BasePipelineConfig):
    """Configuration for the Invert Colors pipeline."""

    pipeline_id = "invert"
    pipeline_name = "Invert Colors"
    pipeline_description = "Inverts the colors of input video frames"

    supports_prompts = False

    # Video mode only (requires video input)
    modes = {"video": ModeDefaults(default=True)}
```

**pipelines/pipeline.py:**

```python theme={null}
from typing import TYPE_CHECKING

import torch

from scope.core.pipelines.interface import Pipeline, Requirements

from .schema import InvertConfig

if TYPE_CHECKING:
    from scope.core.pipelines.base_schema import BasePipelineConfig


class InvertPipeline(Pipeline):
    """Inverts the colors of input video frames."""

    @classmethod
    def get_config_class(cls) -> type["BasePipelineConfig"]:
        return InvertConfig

    def __init__(
        self,
        device: torch.device | None = None,
        **kwargs,
    ):
        self.device = (
            device
            if device is not None
            else torch.device("cuda" if torch.cuda.is_available() else "cpu")
        )

    def prepare(self, **kwargs) -> Requirements:
        """Declare that we need 1 input frame."""
        return Requirements(input_size=1)

    def __call__(self, **kwargs) -> dict:
        """Invert the colors of input frames.

        Args:
            video: List of input frame tensors, each (1, H, W, C) in [0, 255] range.

        Returns:
            Dict with "video" key containing inverted frames in [0, 1] range.
        """
        video = kwargs.get("video")
        if video is None:
            raise ValueError("Input video cannot be None for InvertPipeline")

        # Stack frames into a single tensor: (T, H, W, C)
        frames = torch.stack([frame.squeeze(0) for frame in video], dim=0)
        frames = frames.to(device=self.device, dtype=torch.float32) / 255.0

        # Invert: white becomes black, black becomes white
        inverted = 1.0 - frames

        return {"video": inverted.clamp(0, 1)}
```

**Key points:**

* **`prepare()` returns `Requirements(input_size=N)`**: Tells Scope to collect N frames before calling `__call__`
* **`modes = {"video": ModeDefaults(default=True)}`**: Declares this pipeline only supports video mode
* **`video` parameter**: A list of tensors, one per frame, each with shape (1, H, W, C) in \[0, 255] range
* **Output normalization**: Input is \[0, 255], output must be \[0, 1]

***

## Adding UI Parameters

Expose pipeline parameters in the Scope UI by adding fields to your config with `ui_field_config()`.

### Example: Adding an Intensity Slider

```python theme={null}
from pydantic import Field

from scope.core.pipelines.base_schema import (
    BasePipelineConfig,
    ModeDefaults,
    ui_field_config,
)


class InvertConfig(BasePipelineConfig):
    pipeline_id = "invert"
    pipeline_name = "Invert Colors"
    pipeline_description = "Inverts the colors of input video frames"
    supports_prompts = False
    modes = {"video": ModeDefaults(default=True)}

    # Add a slider that appears in the Settings panel
    intensity: float = Field(
        default=1.0,
        ge=0.0,
        le=1.0,
        description="How strongly to invert the colors (0 = original, 1 = fully inverted)",
        json_schema_extra=ui_field_config(order=1, label="Intensity"),
    )
```

Then read the parameter in `__call__()`:

```python theme={null}
def __call__(self, **kwargs) -> dict:
    video = kwargs.get("video")
    intensity = kwargs.get("intensity", 1.0)

    # ... process with intensity blending
    inverted = 1.0 - frames
    result = frames * (1 - intensity) + inverted * intensity

    return {"video": result.clamp(0, 1)}
```

### ui\_field\_config Options

| Option          | Type        | Description                                                                    |
| :-------------- | :---------- | :----------------------------------------------------------------------------- |
| `order`         | `int`       | Display order (lower values appear first)                                      |
| `modes`         | `list[str]` | Restrict to specific modes, e.g., `["video"]`                                  |
| `is_load_param` | `bool`      | If `True`, parameter is set at load time and disabled during streaming         |
| `label`         | `str`       | Short display label (description becomes tooltip)                              |
| `category`      | `str`       | `"configuration"` for Settings panel (default), `"input"` for Input & Controls |

***

## Load-time vs Runtime Parameters

Parameters behave differently depending on `is_load_param`:

| Type      | `is_load_param`   | Editable During Streaming | Where to Read       |
| :-------- | :---------------- | :------------------------ | :------------------ |
| Load-time | `True`            | No                        | `__init__()`        |
| Runtime   | `False` (default) | Yes                       | `__call__()` kwargs |

**Load-time parameters** are passed when the pipeline loads and require a restart to change. Use for resolution, model selection, device configuration.

**Runtime parameters** are passed to `__call__()` on every frame. Use for effects, strengths, colors.

<Warning>
  Runtime parameters must be read from `kwargs` in `__call__()`, not stored in `__init__()`:

  ```python theme={null}
  # Correct: Read runtime params from kwargs in __call__()
  def __call__(self, **kwargs) -> dict:
      intensity = kwargs.get("intensity", 1.0)

  # Incorrect: Runtime params are NOT passed to __init__()
  def __init__(self, intensity: float = 1.0, **kwargs):
      self.intensity = intensity  # Always gets the default value!
  ```
</Warning>

***

## Creating Preprocessors

Preprocessors transform input video before the main pipeline processes it. Useful for generating control signals (depth maps, edges) for VACE V2V workflows.

```python theme={null}
from scope.core.pipelines.base_schema import BasePipelineConfig, ModeDefaults, UsageType


class MyPreprocessorConfig(BasePipelineConfig):
    pipeline_id = "my-preprocessor"
    pipeline_name = "My Preprocessor"
    usage = [UsageType.PREPROCESSOR]  # Makes it appear in Preprocessor dropdown

    modes = {"video": ModeDefaults(default=True)}
```

Preprocessors must:

* Set `usage = [UsageType.PREPROCESSOR]`
* Use `modes = {"video": ModeDefaults(default=True)}` (video input required)
* Implement `prepare()` returning `Requirements(input_size=N)`

***

## Testing Your Plugin

<Steps>
  <Step title="Install locally">
    In the Scope desktop app or UI, install your plugin using the local path to your plugin directory.
  </Step>

  <Step title="Make changes">
    Edit your plugin source code as needed.
  </Step>

  <Step title="Reload">
    Click the reload button next to your plugin in the Settings dialog.
  </Step>

  <Step title="Test">
    Select your pipeline and verify it works as expected.
  </Step>
</Steps>

<Tip>
  In the desktop app, you can use the **Browse** button to select your local plugin directory. Without the desktop app, run `pwd` in the plugin directory to get the full path to paste into the install field - this also works if you are running the server on a remote machine (e.g. RunPod).
</Tip>

***

## See Also

<CardGroup>
  <Card title="Using Plugins" icon="plug" href="/scope/guides/plugins">
    Install and manage plugins
  </Card>

  <Card title="Pipeline Architecture" icon="sitemap" href="/scope/reference/architecture/pipelines">
    Technical details of the pipeline system
  </Card>

  <Card title="Plugin Architecture" icon="puzzle-piece" href="/scope/reference/architecture/plugins">
    How plugin discovery, installation, and lifecycle work
  </Card>

  <Card title="Tutorial: Build a VFX Plugin" icon="wand-magic-sparkles" href="/scope/tutorials/build-video-effects-plugin">
    Step-by-step tutorial building a complete plugin from scratch
  </Card>
</CardGroup>


# Using Plugins
Source: https://docs.daydream.live/scope/guides/plugins

Install and manage third-party pipeline plugins to extend Scope

# Using plugins in Scope

The Scope plugin system enables third-party extensions to provide custom pipelines. Install plugins to access additional models, preprocessors, and capabilities beyond the built-in pipelines.

***

## Plugin Sources

Plugins can be installed from three sources:

| Source                | Format                         | Description                                  |
| :-------------------- | :----------------------------- | :------------------------------------------- |
| **Git** (Recommended) | `https://github.com/user/repo` | Install directly from a Git repository       |
| **PyPI**              | `my-scope-plugin`              | Install from the Python Package Index        |
| **Local**             | `/path/to/plugin`              | Install from a local directory (development) |

### Git Installation

You can paste the URL directly from your browser:

```
https://github.com/user/plugin-repo
```

Optionally specify a branch, tag, or commit:

```
https://github.com/user/plugin-repo@v1.0.0
https://github.com/user/plugin-repo@main
https://github.com/user/plugin-repo@abc1234
```

### PyPI Installation

```
my-scope-plugin
my-scope-plugin==1.0.0
```

### Local Installation

```
/path/to/my-plugin
C:\Users\username\projects\my-plugin
```

<Note>
  Local plugins are installed in editable mode, meaning code changes take effect after reloading the plugin.
</Note>

***

## Installing Plugins

<Tabs>
  <Tab title="Desktop App" icon="desktop">
    <Steps>
      <Step title="Open Plugin Settings">
        Click the **gear icon** in the app header to open the Settings dialog.

        <img alt="Settings gear button" />

        Navigate to the **Plugins** tab.

        <img alt="Plugins tab" />
      </Step>

      <Step title="Enter package spec">
        In the installation input field, enter the plugin source (Git URL, PyPI package name, or local path).

        You can also click **Browse** to select a local plugin directory.

        <img alt="Install button" />
      </Step>

      <Step title="Install">
        Click the **Install** button and wait for installation to complete.

        <img alt="Install progress" />

        The server will restart automatically to load the new plugin.
      </Step>

      <Step title="Verify installation">
        The plugin should appear in the installed plugins list, and its pipelines will be available in the pipeline selector.
      </Step>
    </Steps>
  </Tab>

  <Tab title="Manual Installation" icon="terminal">
    When running Scope via manual installation (`uv run daydream-scope`), the plugin management experience is similar to the desktop app with these differences:

    * No deep link support (websites cannot auto-open the UI for plugin installation)
    * No **Browse** button for selecting local plugin directories (you can still type a local path manually into the install field)

    Use the Settings dialog to install plugins from Git, PyPI, or local sources.
  </Tab>
</Tabs>

***

## Managing Plugins

### Viewing Installed Plugins

The Plugins tab in Settings displays all installed plugins with their source information.

### Uninstalling a Plugin

<Steps>
  <Step title="Find the plugin">
    In the Plugins tab, locate the plugin you want to remove.
  </Step>

  <Step title="Uninstall">
    Click the **trash icon** next to the plugin.

    <img alt="Uninstall button" />
  </Step>

  <Step title="Wait for restart">
    The server will restart to unload the plugin.
  </Step>
</Steps>

### Updating a Plugin

Scope automatically checks for updates when you open the Plugins tab.

<Steps>
  <Step title="Open the Plugins tab">
    Any plugin with a newer version available shows an **Update** button.
  </Step>

  <Step title="Update">
    Click the **Update** button next to the plugin.
  </Step>

  <Step title="Wait for restart">
    The server will restart to load the updated plugin.
  </Step>
</Steps>

<Note>
  Local plugins do not support update detection. To pick up code changes for a local plugin, use Reload instead.
</Note>

### Reloading a Plugin (Local Only)

When developing a local plugin, you can reload it after making code changes without reinstalling:

<Steps>
  <Step title="Make your code changes">
    Edit the plugin source files.
  </Step>

  <Step title="Reload">
    Click the **reload icon** next to your locally installed plugin.

    <img alt="Reload button" />
  </Step>

  <Step title="Wait for restart">
    The server will restart to pick up the changes.
  </Step>
</Steps>

<Note>
  The reload button only appears for plugins installed from local paths.
</Note>

***

## Deep Link Installation

External sources can facilitate plugin installation via protocol URLs:

```
daydream-scope://install-plugin?package=<spec>
```

When you click a deep link:

1. The desktop app opens (or launches if not running)
2. The Settings dialog opens with the Plugins tab selected
3. The package spec is pre-filled in the input field
4. You confirm to begin installation

<Warning>
  Deep links are only supported in the Desktop App, not manual installations.
</Warning>

***

## CLI Commands

The `daydream-scope` CLI provides commands for managing plugins directly from the terminal.

### Listing Installed Plugins

```bash theme={null}
uv run daydream-scope plugins
```

Shows all installed plugins with their name, version, source, and registered pipelines.

### Installing a Plugin

```bash theme={null}
uv run daydream-scope install <package>
```

Examples for each source type:

```bash theme={null}
# From a Git repository
uv run daydream-scope install https://github.com/user/plugin-repo

# From PyPI
uv run daydream-scope install my-scope-plugin

# From a local directory (editable mode)
uv run daydream-scope install -e /path/to/my-plugin
```

| Option             | Description                                          |
| :----------------- | :--------------------------------------------------- |
| `--upgrade`        | Upgrade the plugin to the latest version             |
| `-e`, `--editable` | Install a project in editable mode from a local path |

### Uninstalling a Plugin

```bash theme={null}
uv run daydream-scope uninstall <name>
```

Removes the plugin and unloads any active pipelines it provided.

***

## Troubleshooting

<AccordionGroup>
  <Accordion title="Plugin installation fails">
    * Check that the package spec is correct (valid Git URL or PyPI package name)
    * Verify your internet connection
    * Check for dependency conflicts in the installation error message
  </Accordion>

  <Accordion title="Plugin pipelines don't appear">
    * Ensure the server restarted after installation
    * Check that the plugin implements the `register_pipelines` hook correctly
    * Verify the plugin's pipelines meet your GPU's VRAM requirements
  </Accordion>

  <Accordion title="Server won't start after plugin install">
    * The plugin may have introduced a dependency conflict
    * Try uninstalling the plugin via the Settings dialog
    * If that fails, manually remove the plugin from `~/.daydream-scope/plugins/plugins.txt`
  </Accordion>
</AccordionGroup>

***

## See Also

<CardGroup>
  <Card title="Developing Plugins" icon="code" href="/scope/guides/plugin-development">
    Create your own custom pipelines
  </Card>

  <Card title="Plugin Architecture" icon="puzzle-piece" href="/scope/reference/architecture/plugins">
    Technical details of the plugin system
  </Card>

  <Card title="Tutorial: Build a VFX Plugin" icon="wand-magic-sparkles" href="/scope/tutorials/build-video-effects-plugin">
    Step-by-step tutorial building a complete plugin from scratch
  </Card>
</CardGroup>


# Remote Inference
Source: https://docs.daydream.live/scope/guides/remote-inference

Run Scope pipelines on cloud-hosted GPUs without local hardware

# Remote Inference (Beta)

<Warning>
  Remote Inference is currently in **beta**. The feature is free during the beta period, but functionality, availability, and pricing are subject to change.
</Warning>

The Remote Inference feature enables you to run Scope pipelines on cloud-hosted H100 GPUs provided by Daydream, eliminating the need for local GPU hardware. This means **Mac users** and anyone without a dedicated NVIDIA card can use Scope.

***

## Setup

<Steps>
  <Step title="Open Account settings">
    Navigate to **Settings** (gear icon, top-right) → **Account**
  </Step>

  <Step title="Sign in with Daydream">
    Select **Sign in with Daydream** - a browser authentication window will open.
  </Step>

  <Step title="Enable Remote Inference">
    After signing in, enable the **Remote Inference** toggle.
  </Step>

  <Step title="Start generating">
    Select your desired pipeline and press **Play**.
  </Step>
</Steps>

Once setup is complete, your Account settings should look like this:

<Frame>
  <img alt="Account settings with Remote Inference enabled and connected" />
</Frame>

***

## Usage Details

* **Session duration** - Each session provides up to 1 hour of GPU access. Upon disconnection, you may reconnect immediately to begin a new session. There is no charge and no limit on the number of sessions during the beta period.
* **Startup time** - Please allow approximately 2-3 minutes for the remote GPU to provision, followed by an additional 1-2 minutes for the selected pipeline to initialize and begin producing output.

***

## Known Limitations

<Note>
  These limitations are specific to the beta and may be resolved in future releases.
</Note>

* [**LoRAs**](/scope/guides/loras) and [**plugins**](/scope/guides/plugins) are not supported in remote inference sessions at this time.
* **[Krea Realtime](/scope/reference/pipelines/krea-realtime) pipeline switching requires an application restart** - to switch to `krea-realtime-video` from another pipeline, close the application, relaunch it, enable the Remote Inference toggle in the settings panel, select the Krea pipeline, and press Play.


# Using Spout
Source: https://docs.daydream.live/scope/guides/spout

Share real-time video with other applications on Windows

# Using Spout

Scope supports near-zero latency video sharing with other local applications on Windows via [Spout](https://spout.zeal.co/). This enables powerful workflows like sending Scope's output to TouchDesigner, Unity, or Blender in real-time.

<Warning>
  **Windows only.** The Scope server must be running on a Windows machine to use Spout. Spout is not available on Linux (including RunPod).
</Warning>

***

## What is Spout?

Spout is a real-time video sharing framework for Windows that allows applications to share GPU textures with minimal overhead. Unlike screen capture or video streaming, Spout shares frames directly through GPU memory, resulting in:

* **Near-zero latency** - Frames are available instantly
* **Full quality** - No compression artifacts
* **Minimal CPU overhead** - All processing stays on the GPU

***

## Spout Receiver

Configure Scope to receive video from other applications via Spout. This is useful for using external video sources (like TouchDesigner or OBS) as input for your generations.

<Steps>
  <Step title="Set input mode to Video">
    Under **Input & Controls**, set **Input Mode** to **Video**.
  </Step>

  <Step title="Select Spout as source">
    Set **Video Source** to **Spout Receiver**.
  </Step>

  <Step title="Configure sender name (optional)">
    The **Sender Name** field can be:

    * **Empty** - Receive from any available Spout sender
    * **Specific name** - Receive only from a sender with that exact name

    <Frame>
      <img alt="Spout Receiver settings" />
    </Frame>
  </Step>
</Steps>

***

## Spout Sender

Configure Scope to send its output to other applications via Spout. This is useful for post-processing in TouchDesigner, recording in OBS, or using Scope's output in Unity/Unreal projects.

<Steps>
  <Step title="Open Settings">
    Click the **Settings** panel in the Scope interface.
  </Step>

  <Step title="Enable Spout Sender">
    Toggle **Spout Sender** to **On**.
  </Step>

  <Step title="Configure sender name (optional)">
    The default sender name is `ScopeOut`. You can change this if you need to identify multiple Scope instances.

    <Frame>
      <img alt="Spout Sender settings" />
    </Frame>
  </Step>
</Steps>

***

## Compatible Applications

Scope can share real-time video with any application that supports Spout. The following applications have been tested:

<CardGroup>
  <Card title="TouchDesigner" icon="bezier-curve" href="https://derivative.ca/">
    Use [Syphon Spout In](https://derivative.ca/UserGuide/Syphon_Spout_In_TOP) and [Syphon Spout Out](https://docs.derivative.ca/Syphon_Spout_Out_TOP) TOPs
  </Card>

  <Card title="Unity" icon="cube" href="https://unity.com/">
    Use the [KlakSpout plugin](https://github.com/keijiro/KlakSpout)
  </Card>

  <Card title="Blender" icon="cube" href="https://www.blender.org/">
    Use the [TextureSharing add-on](https://github.com/maybites/TextureSharing)
  </Card>
</CardGroup>

<Tip>
  Any application that supports Spout should work with Scope. Check your application's documentation for Spout integration instructions.
</Tip>

***

## Example: TouchDesigner Integration

This example shows Scope sending real-time AI video to TouchDesigner for further processing and output.

<video />

### Basic Setup

1. In Scope, enable **Spout Sender** with name `ScopeOut`
2. In TouchDesigner, add a **Syphon Spout In** TOP
3. Set the source to `ScopeOut`
4. The AI-generated video now flows into your TouchDesigner network

### Use Cases

<CardGroup>
  <Card title="Live VJ Performance" icon="music">
    Use TouchDesigner to mix Scope's output with other visuals, apply effects, and output to projectors or LED walls
  </Card>

  <Card title="Interactive Installations" icon="hand-pointer">
    Combine Scope with TouchDesigner's sensor inputs for reactive AI art
  </Card>

  <Card title="Broadcast Graphics" icon="tv">
    Route Scope through TouchDesigner to NDI or SDI for live broadcast
  </Card>

  <Card title="Game Development" icon="gamepad">
    Use Unity or Unreal to incorporate real-time AI video into game environments
  </Card>
</CardGroup>

***

## Troubleshooting

<AccordionGroup>
  <Accordion title="Spout sender not appearing in other apps">
    * Verify Spout Sender is enabled in Scope settings
    * Make sure both applications are running on the same Windows machine
    * Try restarting the receiving application after enabling Spout in Scope
  </Accordion>

  <Accordion title="Spout receiver not getting video">
    * Verify the sending application has Spout output enabled
    * Check that the sender name matches (or leave empty to receive from any sender)
    * Ensure both applications are using the same Spout version
  </Accordion>

  <Accordion title="Performance issues">
    * Spout sharing is GPU-memory based and should have minimal overhead
    * If you see frame drops, check GPU memory usage - you may need to reduce resolution
    * Close other GPU-intensive applications if VRAM is limited
  </Accordion>
</AccordionGroup>

***

## See Also

<CardGroup>
  <Card title="Quick Start" icon="rocket" href="/scope/getting-started/quickstart">
    Get Scope running if you haven't already
  </Card>

  <Card title="VACE Guide" icon="image" href="/scope/guides/vace">
    Use reference images and control videos with Spout input
  </Card>
</CardGroup>


# Using VACE
Source: https://docs.daydream.live/scope/guides/vace

Guide generation with reference images and control videos

# Using VACE

VACE (Video All-in-One Creation and Editing) enables advanced video creation and editing tasks in Scope. Use reference images to define characters and styles, or control videos to guide the structure and motion of your generations.

<Warning>
  VACE support is still experimental and the implementation is incomplete. Some features may not work as expected.
</Warning>

***

## Pipeline Compatibility

VACE is supported on the following pipelines:

### Wan2.1 1.3B Pipelines

* LongLive
* RewardForcing
* MemFlow

### Wan2.1 14B Pipeline

* Krea Realtime

<Note>
  StreamDiffusion V2 also has VACE capabilities, but quality is currently limited.
</Note>

<Warning>
  **Krea Realtime + VACE** requires approximately **55GB of VRAM**.

  FP8 quantization is not currently supported with VACE. Continued prompting with Krea + VACE may require resetting the cache due to cache recomputation limitations.
</Warning>

***

## Supported Features

<CardGroup>
  <Card title="Reference-to-Video (R2V)" icon="image">
    Use reference images to guide the character, style, and aesthetic of your generation
  </Card>

  <Card title="Video-to-Video (V2V)" icon="video">
    Use control videos (depth, pose, scribble, optical flow) to guide the structure and motion
  </Card>

  <Card title="Animate Anything" icon="wand-magic-sparkles">
    Combine R2V + V2V: reference image defines the look, control video provides the movement
  </Card>

  <Card title="Real-time Depth" icon="layer-group">
    Built-in `video-depth-anything` preprocessor generates depth maps from source videos automatically
  </Card>
</CardGroup>

### Built-in Preprocessors

For real-time V2V workflows, these preprocessors automatically generate control signals from your video input (webcam, screen capture, or uploaded video):

| Pipeline               | Description                                           | Model Required   |
| :--------------------- | :---------------------------------------------------- | :--------------- |
| `video-depth-anything` | Depth estimation for temporally consistent depth maps | Yes (\~1GB VRAM) |
| `optical-flow`         | RAFT optical flow for motion visualization            | No (torchvision) |
| `scribble`             | Contour/line art extraction                           | Yes              |
| `gray`                 | Grayscale conversion                                  | No               |

Select a preprocessor from the **Preprocessor** dropdown in the UI when using Video input mode. The preprocessor output becomes the control signal for V2V generation.

<Note>
  Additional preprocessors will be available via plugins in the future.
</Note>

### Not Yet Supported

The following features are being investigated but not currently available:

* Multiple reference images for R2V
* Masked video-to-video (MV2V) for inpainting, outpainting, and video extension
* Complex tasks like Swap Anything, Reference Anything, Move Anything, Expand Anything

***

## Enabling VACE

Before using any VACE features, make sure VACE is enabled in your pipeline settings.

<Steps>
  <Step title="Open Settings">
    Click the **Settings** panel in the Scope interface.
  </Step>

  <Step title="Enable VACE">
    Toggle **VACE** to **On**.

    <Frame>
      <img alt="VACE toggle in Settings panel" />
    </Frame>
  </Step>
</Steps>

***

## Reference-to-Video (R2V)

Use a reference image to guide the character, style, or aesthetic of your generation. The model will try to maintain consistency with the reference throughout the video.

<Steps>
  <Step title="Add a reference image">
    In the Settings panel, find **Reference Images** and click **Add Image**.

    <Frame>
      <img alt="Add Image button under Reference Images" />
    </Frame>
  </Step>

  <Step title="Select your image">
    Use the media picker to either:

    * Upload a new image
    * Select from your previously uploaded assets

    <Frame>
      <img alt="Media picker for selecting reference images" />
    </Frame>
  </Step>

  <Step title="Verify the reference">
    You should see a preview of your selected reference image in the panel.

    <Frame>
      <img alt="Reference image preview" />
    </Frame>
  </Step>
</Steps>

<Note>
  Only a single reference image is supported at this time. Multi-reference support is planned for a future release.
</Note>

***

## Video-to-Video (V2V)

Use a control video to guide the structure and motion of your generation. Control videos can be depth maps, pose estimations, scribbles, or optical flow visualizations.

<Steps>
  <Step title="Set input mode to Video">
    Under **Input & Controls**, set **Input Mode** to **Video**.
  </Step>

  <Step title="Upload a control video">
    Upload your control video (e.g., a depth map or pose estimation video).

    <Frame>
      <img alt="V2V input settings" />
    </Frame>
  </Step>

  <Step title="Generate">
    The output will follow the structure of your control video while applying the style from your prompt.
  </Step>
</Steps>

### Example Control Video

A pose estimation video can be used to transfer motion to AI-generated characters:

<video />

***

## Animate Anything

Combine Reference-to-Video and Video-to-Video for the best of both worlds:

* **Reference image** → Defines the character, style, and aesthetic
* **Control video** → Provides the structure and motion

This is powerful for animating still images or transferring motion to custom characters.

### Example Workflow

<Steps>
  <Step title="Add a reference image">
    Upload an image of your character or style reference.

    <Frame>
      <img alt="Reference image for Animate Anything" />
    </Frame>
  </Step>

  <Step title="Add a control video">
    Upload a pose or depth video that contains the motion you want.
  </Step>

  <Step title="Generate">
    The output combines the look from your reference with the motion from your control video.
  </Step>
</Steps>

### Example Results

**Basic Animate Anything:**

<video />

**With LoRA enhancement:**

For even better character consistency, combine Animate Anything with a LoRA. This example uses the [Arcane Jinx LoRA](https://civitai.com/models/1332383/wan-lora-arcane-jinx-v1-wan-13b):

<video />

***

## Tips for Best Results

<AccordionGroup>
  <Accordion title="Reference image quality">
    Use high-quality reference images with clear subjects. The model works best when the reference has good lighting and a clean background.
  </Accordion>

  <Accordion title="Control video resolution">
    Match your control video resolution to your pipeline's output resolution for best structural accuracy.
  </Accordion>

  <Accordion title="Combine with LoRAs">
    For character consistency, pair VACE with a relevant LoRA. This helps maintain style and identity across the generation.
  </Accordion>

  <Accordion title="Manage cache for long sessions">
    If quality degrades during long sessions with Krea + VACE, try resetting the cache to restore output quality.
  </Accordion>
</AccordionGroup>

***

## API Usage

For programmatic control of VACE features, see the API reference:

<Card title="VACE API Reference" icon="brackets-curly" href="/scope/reference/api/vace">
  Upload reference images, set context scale, and combine with control videos via WebRTC
</Card>

***

## See Also

<CardGroup>
  <Card title="LoRAs Guide" icon="wand-magic-sparkles" href="/scope/guides/loras">
    Enhance VACE with style-consistent LoRAs
  </Card>

  <Card title="Quick Start" icon="rocket" href="/scope/getting-started/quickstart">
    Get Scope running if you haven't already
  </Card>
</CardGroup>


# Server API
Source: https://docs.daydream.live/scope/reference/api/index

REST and WebRTC API for real-time video generation

# Scope Server API

The Scope Server provides a REST API for pipeline management and a WebRTC API for real-time video streaming. Build custom applications that generate video in real-time using text prompts or input video.

## What You Can Build

* **Text-to-Video streaming** - Generate live video from text prompts
* **Video-to-Video transformation** - Transform input video in real-time
* **Custom creative tools** - Build VJ software, live performance tools, interactive installations
* **Integrations** - Connect Scope to your existing applications and workflows

<Tip>
  When the server is running locally, interactive API documentation (Swagger UI) is available at `http://localhost:8000/docs`. A full OpenAPI reference will be added to these docs soon.
</Tip>

***

## Quick Start

Get streaming video from the Scope API in under 5 minutes. You'll create an `index.html` file that streams real-time generated video using the `longlive` pipeline.

<Steps>
  <Step title="Start the server">
    ```bash theme={null}
    uv run daydream-scope
    ```

    The server runs on `http://localhost:8000` by default.
  </Step>

  <Step title="Download models">
    If not already downloaded:

    ```bash theme={null}
    uv run download_models --pipeline longlive
    ```
  </Step>

  <Step title="Create index.html">
    Create a file named `index.html` with the following content:

    ```html theme={null}
    <!DOCTYPE html>
    <html lang="en">
    <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <title>Scope API Quick Start</title>
      <style>
        body {
          margin: 0;
          background: #000;
          display: flex;
          justify-content: center;
          align-items: center;
          min-height: 100vh;
        }
        video {
          max-width: 100%;
          max-height: 100vh;
        }
        #status {
          position: fixed;
          top: 10px;
          left: 10px;
          color: #fff;
          font-family: monospace;
          background: rgba(0,0,0,0.7);
          padding: 10px;
          border-radius: 4px;
        }
      </style>
    </head>
    <body>
      <div id="status">Connecting...</div>
      <video id="video" autoplay muted playsinline></video>

      <script>
        const API_BASE = "http://localhost:8000";
        const statusEl = document.getElementById("status");
        const videoEl = document.getElementById("video");

        async function loadPipeline() {
          statusEl.textContent = "Loading pipeline...";

          // Load the longlive pipeline
          await fetch(`${API_BASE}/api/v1/pipeline/load`, {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({
              pipeline_ids: ["longlive"]
            })
          });

          // Wait for pipeline to finish loading
          while (true) {
            const response = await fetch(`${API_BASE}/api/v1/pipeline/status`);
            const { status } = await response.json();

            if (status === "loaded") break;
            if (status === "error") throw new Error("Pipeline failed to load");

            await new Promise(r => setTimeout(r, 1000));
          }

          statusEl.textContent = "Pipeline loaded";
        }

        async function startStream() {
          // Get ICE servers
          const iceResponse = await fetch(`${API_BASE}/api/v1/webrtc/ice-servers`);
          const { iceServers } = await iceResponse.json();

          // Create peer connection
          const pc = new RTCPeerConnection({ iceServers });

          // Store session ID for ICE candidates
          let sessionId = null;
          const queuedCandidates = [];

          // Create data channel for parameters
          const dataChannel = pc.createDataChannel("parameters", { ordered: true });

          dataChannel.onopen = () => {
            statusEl.textContent = "Connected - Streaming";
          };

          dataChannel.onmessage = (event) => {
            const data = JSON.parse(event.data);
            if (data.type === "stream_stopped") {
              statusEl.textContent = "Stream stopped: " + (data.error_message || "Unknown error");
              pc.close();
            }
          };

          // Add video transceiver (receive-only mode)
          pc.addTransceiver("video");

          // Handle incoming video track
          pc.ontrack = (event) => {
            if (event.streams && event.streams[0]) {
              videoEl.srcObject = event.streams[0];
            }
          };

          pc.onconnectionstatechange = () => {
            if (pc.connectionState === "connected") {
              statusEl.textContent = "Connected - Streaming";
            } else if (pc.connectionState === "failed" || pc.connectionState === "disconnected") {
              statusEl.textContent = "Disconnected";
            }
          };

          // Send ICE candidates as they arrive (Trickle ICE)
          pc.onicecandidate = async (event) => {
            if (event.candidate) {
              if (sessionId) {
                await fetch(`${API_BASE}/api/v1/webrtc/offer/${sessionId}`, {
                  method: "PATCH",
                  headers: { "Content-Type": "application/json" },
                  body: JSON.stringify({
                    candidates: [{
                      candidate: event.candidate.candidate,
                      sdpMid: event.candidate.sdpMid,
                      sdpMLineIndex: event.candidate.sdpMLineIndex
                    }]
                  })
                });
              } else {
                queuedCandidates.push(event.candidate);
              }
            }
          };

          // Create and send offer
          const offer = await pc.createOffer();
          await pc.setLocalDescription(offer);

          const sdpResponse = await fetch(`${API_BASE}/api/v1/webrtc/offer`, {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({
              sdp: pc.localDescription.sdp,
              type: pc.localDescription.type,
              initialParameters: {
                prompts: [{ text: "A 3D animated scene. A **panda** walks along a path towards the camera in a park on a spring day.", weight: 1.0 }],
                denoising_step_list: [1000, 750, 500, 250],
                manage_cache: true
              }
            })
          });

          const answer = await sdpResponse.json();
          sessionId = answer.sessionId;

          await pc.setRemoteDescription({
            type: answer.type,
            sdp: answer.sdp
          });

          // Send any queued ICE candidates
          if (queuedCandidates.length > 0) {
            await fetch(`${API_BASE}/api/v1/webrtc/offer/${sessionId}`, {
              method: "PATCH",
              headers: { "Content-Type": "application/json" },
              body: JSON.stringify({
                candidates: queuedCandidates.map(c => ({
                  candidate: c.candidate,
                  sdpMid: c.sdpMid,
                  sdpMLineIndex: c.sdpMLineIndex
                }))
              })
            });
          }
        }

        // Main
        (async () => {
          try {
            await loadPipeline();
            await startStream();
          } catch (error) {
            statusEl.textContent = "Error: " + error.message;
            console.error(error);
          }
        })();
      </script>
    </body>
    </html>
    ```
  </Step>

  <Step title="Open in browser">
    Open `index.html` in your browser. The page will:

    * Load the `longlive` pipeline
    * Establish a WebRTC connection
    * Display real-time generated video based on the prompt
  </Step>
</Steps>

***

## Configuration

### Server Options

```bash theme={null}
uv run daydream-scope [OPTIONS]

Options:
  --host HOST       Host to bind to (default: 0.0.0.0)
  --port PORT       Port to bind to (default: 8000)
  --reload          Enable auto-reload for development
  -N, --no-browser  Don't open browser automatically
  --version         Show version and exit
```

### Environment Variables

| Variable          | Description                                                   |
| :---------------- | :------------------------------------------------------------ |
| `PIPELINE`        | Default pipeline to pre-warm on startup                       |
| `HF_TOKEN`        | Hugging Face token for downloading models and Cloudflare TURN |
| `VERBOSE_LOGGING` | Enable verbose logging for debugging                          |

### TURN Server Credentials

For WebRTC connections that need to traverse firewalls (NAT traversal), TURN servers are used. The server automatically configures TURN credentials when environment variables are set:

**Using Cloudflare (via Hugging Face)**:

```bash theme={null}
export HF_TOKEN=your_huggingface_token
```

If no TURN credentials are configured, the server falls back to Google's public STUN server, which works for direct connections but may not work behind strict firewalls.

***

## API Endpoints

### Health & Info

| Endpoint                | Method | Description                                  |
| :---------------------- | :----- | :------------------------------------------- |
| `/health`               | GET    | Health check                                 |
| `/api/v1/hardware/info` | GET    | Get hardware info (VRAM, Spout availability) |
| `/docs`                 | GET    | Interactive API documentation (Swagger UI)   |

### Pipeline Management

| Endpoint                    | Method | Description                             |
| :-------------------------- | :----- | :-------------------------------------- |
| `/api/v1/pipeline/load`     | POST   | Load a pipeline                         |
| `/api/v1/pipeline/status`   | GET    | Get current pipeline status             |
| `/api/v1/pipelines/schemas` | GET    | Get schemas for all available pipelines |

### Model Management

| Endpoint                  | Method | Description                                   |
| :------------------------ | :----- | :-------------------------------------------- |
| `/api/v1/models/status`   | GET    | Check if models are downloaded for a pipeline |
| `/api/v1/models/download` | POST   | Start downloading models for a pipeline       |

### WebRTC

| Endpoint                            | Method | Description                       |
| :---------------------------------- | :----- | :-------------------------------- |
| `/api/v1/webrtc/ice-servers`        | GET    | Get ICE server configuration      |
| `/api/v1/webrtc/offer`              | POST   | Send WebRTC offer, receive answer |
| `/api/v1/webrtc/offer/{session_id}` | PATCH  | Add ICE candidates (Trickle ICE)  |

### Assets

| Endpoint                | Method | Description                           |
| :---------------------- | :----- | :------------------------------------ |
| `/api/v1/assets`        | GET    | List available assets (images/videos) |
| `/api/v1/assets`        | POST   | Upload an asset                       |
| `/api/v1/assets/{path}` | GET    | Serve an asset file                   |

### LoRA

| Endpoint            | Method | Description               |
| :------------------ | :----- | :------------------------ |
| `/api/v1/lora/list` | GET    | List available LoRA files |

***

## Next Steps

Learn how to use the API for specific workflows:

<CardGroup>
  <Card title="Load Pipeline" href="/scope/reference/api/load-pipeline">
    Load and configure pipelines with custom parameters, LoRAs, and VAE types
  </Card>

  <Card title="Send Parameters" href="/scope/reference/api/parameters">
    Update prompts, transitions, and settings in real-time during streaming
  </Card>

  <Card title="Receive Video (T2V)" href="/scope/reference/api/receive-video">
    Set up WebRTC to receive generated video from text prompts
  </Card>

  <Card title="Send & Receive Video (V2V)" href="/scope/reference/api/send-receive">
    Transform input video in real-time with bidirectional WebRTC
  </Card>
</CardGroup>

<Card title="VACE" href="/scope/reference/api/vace">
  Guide generation with reference images and control videos
</Card>


# Load Pipeline
Source: https://docs.daydream.live/scope/reference/api/load-pipeline

Load and configure pipelines with custom parameters, LoRAs, and VAE types

# Load Pipeline

Before streaming video, you need to load a pipeline. Pipeline loading is asynchronous - the API initiates loading and returns immediately, so you must poll for completion.

## Overview

Loading a pipeline:

1. Loads model weights to GPU memory
2. Initializes the inference pipeline

## Load a Pipeline

```javascript theme={null}
async function loadPipeline(request) {
  const response = await fetch("http://localhost:8000/api/v1/pipeline/load", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify(request)
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Failed to load pipeline: ${error}`);
  }

  return await response.json();
}

// Example: Load longlive with default settings
await loadPipeline({
  pipeline_ids: ["longlive"]
});

// Example: Load with custom resolution
await loadPipeline({
  pipeline_ids: ["longlive"],
  load_params: {
    height: 512,
    width: 512,
    seed: 42
  }
});

// Example: Load with preprocessor
await loadPipeline({
  pipeline_ids: ["video-depth-anything", "longlive"],
  load_params: {
    height: 512,
    width: 512,
    seed: 42
  }
});
```

### Request Body

```json theme={null}
{
  "pipeline_ids": ["longlive"],
  "load_params": {
    "height": 320,
    "width": 576,
    "seed": 42,
    "quantization": null,
    "vace_enabled": false,
    "vae_type": "wan",
    "loras": [
      {
        "path": "/path/to/lora.safetensors",
        "scale": 1.0
      }
    ],
    "lora_merge_mode": "permanent_merge"
  }
}
```

To load a pipeline with a preprocessor, include the preprocessor ID before the main pipeline in the `pipeline_ids` array:

```json theme={null}
{
  "pipeline_ids": ["video-depth-anything", "longlive"],
  "load_params": {
    "height": 320,
    "width": 576,
    "seed": 42,
    "vace_enabled": true
  }
}
```

### Load Parameters

| Parameter         | Type   | Default             | Description                             |
| :---------------- | :----- | :------------------ | :-------------------------------------- |
| `height`          | int    | 320                 | Output height (16-2048)                 |
| `width`           | int    | 576                 | Output width (16-2048)                  |
| `seed`            | int    | 42                  | Random seed for generation              |
| `vace_enabled`    | bool   | true                | Enable VACE                             |
| `vae_type`        | string | `"wan"`             | VAE type (see [VAE Types](#vae-types))  |
| `loras`           | array  | null                | LoRA adapters to load                   |
| `lora_merge_mode` | string | `"permanent_merge"` | `"permanent_merge"` or `"runtime_peft"` |

***

## Check Pipeline Status

Poll the status endpoint to know when loading completes:

```javascript theme={null}
async function getPipelineStatus() {
  const response = await fetch("http://localhost:8000/api/v1/pipeline/status");

  if (!response.ok) {
    throw new Error(`Failed to get status: ${response.statusText}`);
  }

  return await response.json();
}

const status = await getPipelineStatus();
console.log(status);
```

### Response Format

```json theme={null}
{
  "status": "loaded",
  "pipeline_id": "longlive",
  "load_params": {
    "height": 320,
    "width": 576,
    "seed": 42
  },
  "loaded_lora_adapters": [
    { "path": "/path/to/lora.safetensors", "scale": 1.0 }
  ],
  "error": null
}
```

### Status Values

| Status       | Description                          |
| :----------- | :----------------------------------- |
| `not_loaded` | No pipeline loaded                   |
| `loading`    | Pipeline is loading (wait and retry) |
| `loaded`     | Pipeline ready for streaming         |
| `error`      | Loading failed (check `error` field) |

***

## Wait for Pipeline Ready

Complete example that loads a pipeline and waits for it to be ready:

```javascript theme={null}
async function waitForPipelineLoaded(timeoutMs = 300000, pollIntervalMs = 1000) {
  const startTime = Date.now();

  while (Date.now() - startTime < timeoutMs) {
    const status = await getPipelineStatus();

    switch (status.status) {
      case "loaded":
        console.log("Pipeline loaded successfully");
        return status;

      case "loading":
        console.log("Pipeline loading...");
        await new Promise(r => setTimeout(r, pollIntervalMs));
        break;

      case "error":
        throw new Error(`Pipeline load failed: ${status.error}`);

      case "not_loaded":
        throw new Error("Pipeline not loading - was load request sent?");
    }
  }

  throw new Error("Timeout waiting for pipeline to load");
}

// Complete workflow
async function loadAndWait(pipelineIds, loadParams = {}) {
  console.log(`Loading ${pipelineIds.join(" -> ")}...`);
  await loadPipeline({
    pipeline_ids: pipelineIds,
    load_params: loadParams
  });
  return await waitForPipelineLoaded();
}

// Usage: Load single pipeline
const status = await loadAndWait(["longlive"], { height: 320, width: 576 });
console.log(`Pipeline ${status.pipeline_id} ready`);

// Usage: Load with preprocessor
const statusWithPreprocessor = await loadAndWait(
  ["video-depth-anything", "longlive"],
  { height: 320, width: 576, vace_enabled: true }
);
console.log(`Pipeline ${statusWithPreprocessor.pipeline_id} ready`);
```

***

## Switching Pipelines

When you load a different pipeline (or the same pipeline with different parameters), the previous pipeline is automatically unloaded:

```javascript theme={null}
// Load longlive
await loadAndWait(["longlive"]);

// This automatically unloads longlive and loads streamdiffusionv2
await loadAndWait(["streamdiffusionv2"]);

// Load with preprocessor
await loadAndWait(["video-depth-anything", "longlive"], {
  vace_enabled: true
});
```

***

## LoRA Configuration

Load LoRA adapters at pipeline load time:

```javascript theme={null}
await loadPipeline({
  pipeline_ids: ["longlive"],
  load_params: {
    loras: [
      {
        path: "/path/to/style.safetensors",
        scale: 0.8
      },
      {
        path: "/path/to/character.safetensors",
        scale: 1.0
      }
    ],
    // permanent_merge: Maximum FPS, no runtime updates
    // runtime_peft: Allows runtime scale updates, lower FPS
    lora_merge_mode: "runtime_peft"
  }
});
```

With `runtime_peft` mode, you can update LoRA scales during streaming via the data channel. See [Send Parameters](/scope/reference/api/parameters) for details.

***

## Preprocessor Configuration

Preprocessors can be configured to automatically process input videos before they reach the main pipeline. For example, the `video-depth-anything` preprocessor can generate depth maps from source videos in real-time for use with VACE V2V.

To configure a preprocessor, include it in the `pipeline_ids` array before the main pipeline:

```javascript theme={null}
// Load longlive with video-depth-anything preprocessor
await loadPipeline({
  pipeline_ids: ["video-depth-anything", "longlive"],
  load_params: {
    height: 320,
    width: 576,
    seed: 42
  }
});
```

The preprocessor will automatically process input videos and pass the results to the main pipeline. When using VACE V2V with a preprocessor, the preprocessor's output (e.g., depth maps) will be used as the control video.

```javascript theme={null}
// Example: Load with preprocessor for real-time depth estimation
async function loadWithPreprocessor(pipelineId, preprocessorId, loadParams = {}) {
  const pipelineIds = [];

  if (preprocessorId) {
    pipelineIds.push(preprocessorId);
  }
  pipelineIds.push(pipelineId);

  return await loadPipeline({
    pipeline_ids: pipelineIds,
    load_params: loadParams
  });
}

// Usage: Load longlive with depth preprocessor
await loadWithPreprocessor("longlive", "video-depth-anything", {
  height: 320,
  width: 576,
  vace_enabled: true
});
```

<Note>
  Currently, only the `video-depth-anything` preprocessor is available. Additional preprocessors will be available via plugins in the future.
</Note>

***

## VAE Types

The `vae_type` parameter controls which VAE (Variational Autoencoder) is used for encoding pixels to latents and decoding latents to pixels. Different VAE types offer tradeoffs between quality, speed, and memory usage.

| Type       | Quality | Speed  |
| :--------- | :------ | :----- |
| `wan`      | Best    | Slow   |
| `lightvae` | High    | Medium |
| `tae`      | Average | Fast   |
| `lighttae` | High    | Fast   |

See [VAE Types](/scope/reference/vae) for detailed descriptions of each type.

```javascript theme={null}
await loadPipeline({
  pipeline_ids: ["longlive"],
  load_params: {
    vae_type: "lighttae"
  }
});
```

***

## Error Handling

```javascript theme={null}
try {
  await loadPipeline({
    pipeline_ids: ["longlive"]
  });
  await waitForPipelineLoaded();
} catch (error) {
  if (error.message.includes("CUDA")) {
    console.error("GPU error - check CUDA availability");
  } else if (error.message.includes("not found")) {
    console.error("Models not downloaded - run download first");
  } else {
    console.error("Load failed:", error.message);
  }
}
```

***

## See Also

<CardGroup>
  <Card title="Receive Video" href="/scope/reference/api/receive-video">
    Start streaming after pipeline loads
  </Card>

  <Card title="Send Parameters" href="/scope/reference/api/parameters">
    Update parameters during streaming
  </Card>
</CardGroup>


# Send Parameters
Source: https://docs.daydream.live/scope/reference/api/parameters

Update prompts, transitions, and settings in real-time during streaming

# Send Parameters

After establishing a WebRTC connection, you can send real-time parameter updates via the data channel. This allows dynamic control of generation without reconnecting.

## Overview

Parameters are sent as JSON messages through the WebRTC data channel. Updates take effect on the next chunk that is generated.

***

## Setting Up the Data Channel

The data channel is created when establishing the WebRTC connection:

```javascript theme={null}
// Create data channel before creating offer
const dataChannel = pc.createDataChannel("parameters", { ordered: true });

dataChannel.onopen = () => {
  console.log("Data channel ready for parameter updates");
};

dataChannel.onmessage = (event) => {
  const data = JSON.parse(event.data);

  // Handle notifications from server
  if (data.type === "stream_stopped") {
    console.log("Stream stopped:", data.error_message);
    pc.close();
  }
};
```

***

## Sending Parameter Updates

```javascript theme={null}
function sendParameters(params) {
  if (dataChannel.readyState === "open") {
    dataChannel.send(JSON.stringify(params));
  } else {
    console.warn("Data channel not ready");
  }
}

// Example: Update prompt
sendParameters({
  prompts: [{ text: "A cat playing piano", weight: 1.0 }]
});

// Example: Update multiple parameters
sendParameters({
  prompts: [{ text: "A sunset over the ocean", weight: 1.0 }],
  denoising_step_list: [800, 600, 400],
  noise_scale: 0.7
});
```

***

## Available Parameters

### Prompts

Control what is being generated:

```javascript theme={null}
// Single prompt
sendParameters({
  prompts: [{ text: "A beautiful forest", weight: 1.0 }]
});

// Blended prompts (spatial blending within frame)
sendParameters({
  prompts: [
    { text: "A sunny day", weight: 0.7 },
    { text: "A rainy day", weight: 0.3 }
  ],
  prompt_interpolation_method: "linear"  // or "slerp"
});
```

| Parameter                     | Type   | Default    | Description                                 |
| :---------------------------- | :----- | :--------- | :------------------------------------------ |
| `prompts`                     | array  | -          | Array of `{ text: string, weight: number }` |
| `prompt_interpolation_method` | string | `"linear"` | `"linear"` or `"slerp"` for blending        |

### Prompt Transitions

Smoothly transition between prompts over multiple frames:

```javascript theme={null}
sendParameters({
  transition: {
    target_prompts: [
      { text: "A night sky with stars", weight: 1.0 }
    ],
    num_steps: 8,  // Transition over 8 chunks
    temporal_interpolation_method: "linear"
  }
});
```

| Parameter                                  | Type   | Default    | Description                             |
| :----------------------------------------- | :----- | :--------- | :-------------------------------------- |
| `transition.target_prompts`                | array  | -          | Target prompts to transition to         |
| `transition.num_steps`                     | int    | 4          | Frames to transition over (0 = instant) |
| `transition.temporal_interpolation_method` | string | `"linear"` | `"linear"` or `"slerp"`                 |

### Denoising Steps

Control quality vs speed tradeoff:

```javascript theme={null}
// More steps = higher quality, slower
sendParameters({
  denoising_step_list: [1000, 750, 500, 250]
});

// Fewer steps = faster, lower quality
sendParameters({
  denoising_step_list: [700, 400]
});
```

| Parameter             | Type  | Description                                     |
| :-------------------- | :---- | :---------------------------------------------- |
| `denoising_step_list` | array | Descending timesteps (e.g., `[1000, 750, 500]`) |

### Noise Control

```javascript theme={null}
sendParameters({
  noise_scale: 0.8,      // 0.0-1.0, amount of noise
  noise_controller: true  // Auto-adjust based on motion
});
```

| Parameter          | Type  | Range   | Description                       |
| :----------------- | :---- | :------ | :-------------------------------- |
| `noise_scale`      | float | 0.0-1.0 | Manual noise amount               |
| `noise_controller` | bool  | -       | Enable automatic noise adjustment |

### Cache Control

```javascript theme={null}
// Enable automatic cache management
sendParameters({
  manage_cache: true
});

// Or manual cache reset
sendParameters({
  reset_cache: true  // Trigger one-time cache reset
});
```

| Parameter      | Type | Description                  |
| :------------- | :--- | :--------------------------- |
| `manage_cache` | bool | Auto cache management        |
| `reset_cache`  | bool | Force cache reset (one-shot) |

### Playback Control

```javascript theme={null}
// Pause generation
sendParameters({ paused: true });

// Resume generation
sendParameters({ paused: false });
```

### LoRA Scale Updates

Update LoRA adapter scales at runtime (requires `lora_merge_mode: "runtime_peft"` at load time):

```javascript theme={null}
sendParameters({
  lora_scales: [
    { path: "/path/to/style.safetensors", scale: 0.5 },
    { path: "/path/to/character.safetensors", scale: 1.2 }
  ]
});
```

### VACE Parameters

Control strength of visual conditioning:

```javascript theme={null}
sendParameters({
  vace_ref_images: ["/path/to/reference.png"],
  vace_context_scale: 1.0  // 0.0-2.0
});
```

See [VACE](/scope/reference/api/vace) for detailed VACE usage.

### Spout (Windows)

Enable Spout output for sending frames to external applications:

```javascript theme={null}
sendParameters({
  spout_sender: {
    enabled: true,
    name: "ScopeOutput"
  }
});

// Receive from Spout instead of WebRTC input
sendParameters({
  spout_receiver: {
    enabled: true,
    name: "ExternalApp"
  }
});
```

***

## Initial Parameters

You can also send initial parameters when establishing the WebRTC connection:

```javascript theme={null}
const response = await fetch("http://localhost:8000/api/v1/webrtc/offer", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({
    sdp: offer.sdp,
    type: offer.type,
    initialParameters: {
      prompts: [{ text: "Initial prompt", weight: 1.0 }],
      denoising_step_list: [1000, 750, 500, 250],
      manage_cache: true
    }
  })
});
```

***

## See Also

<CardGroup>
  <Card title="Receive Video" href="/scope/reference/api/receive-video">
    Set up WebRTC connection
  </Card>

  <Card title="VACE" href="/scope/reference/api/vace">
    Reference image conditioning
  </Card>

  <Card title="Load Pipeline" href="/scope/reference/api/load-pipeline">
    Configure pipeline at load time
  </Card>

  <Card title="Spout Guide" href="/scope/guides/spout">
    Using Spout for external apps
  </Card>
</CardGroup>


# Receive Video (T2V)
Source: https://docs.daydream.live/scope/reference/api/receive-video

Set up WebRTC to receive generated video from text prompts

# Receive Video

This guide shows how to set up a WebRTC connection to receive video from the Scope API in **text-to-video mode** (no input video, just prompts).

## Overview

In receive-only mode:

* You send text prompts to control generation
* The server generates video and streams it back
* No input video required

## Prerequisites

1. Server is running: `uv run daydream-scope`
2. Models are downloaded for your pipeline
3. Pipeline is loaded (see [Load Pipeline](/scope/reference/api/load-pipeline))

***

## Complete Example

```javascript theme={null}
async function startReceiveStream(initialPrompt = "A beautiful landscape") {
  const API_BASE = "http://localhost:8000";

  // 1. Get ICE servers from backend
  const iceResponse = await fetch(`${API_BASE}/api/v1/webrtc/ice-servers`);
  const { iceServers } = await iceResponse.json();

  // 2. Create peer connection
  const pc = new RTCPeerConnection({ iceServers });

  // State management
  let sessionId = null;
  const queuedCandidates = [];

  // 3. Create data channel for parameters
  const dataChannel = pc.createDataChannel("parameters", { ordered: true });

  dataChannel.onopen = () => {
    console.log("Data channel opened - ready for parameter updates");
  };

  dataChannel.onmessage = (event) => {
    const data = JSON.parse(event.data);
    if (data.type === "stream_stopped") {
      console.log("Stream stopped:", data.error_message);
      pc.close();
    }
  };

  // 4. Add video transceiver (receive-only, no input)
  pc.addTransceiver("video");

  // 5. Handle incoming video track
  pc.ontrack = (event) => {
    if (event.streams && event.streams[0]) {
      const videoElement = document.getElementById("video");
      videoElement.srcObject = event.streams[0];
    }
  };

  // 6. Connection state monitoring
  pc.onconnectionstatechange = () => {
    console.log("Connection state:", pc.connectionState);
  };

  pc.oniceconnectionstatechange = () => {
    console.log("ICE state:", pc.iceConnectionState);
  };

  // 7. Handle ICE candidates (Trickle ICE)
  pc.onicecandidate = async (event) => {
    if (event.candidate) {
      if (sessionId) {
        await sendIceCandidate(sessionId, event.candidate);
      } else {
        queuedCandidates.push(event.candidate);
      }
    }
  };

  // 8. Create and send offer
  const offer = await pc.createOffer();
  await pc.setLocalDescription(offer);

  const response = await fetch(`${API_BASE}/api/v1/webrtc/offer`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      sdp: pc.localDescription.sdp,
      type: pc.localDescription.type,
      initialParameters: {
        prompts: [{ text: initialPrompt, weight: 1.0 }],
        denoising_step_list: [1000, 750, 500, 250],
        manage_cache: true
      }
    })
  });

  const answer = await response.json();
  sessionId = answer.sessionId;

  // 9. Set remote description
  await pc.setRemoteDescription({
    type: answer.type,
    sdp: answer.sdp
  });

  // 10. Send queued ICE candidates
  for (const candidate of queuedCandidates) {
    await sendIceCandidate(sessionId, candidate);
  }
  queuedCandidates.length = 0;

  return { pc, dataChannel, sessionId };
}

async function sendIceCandidate(sessionId, candidate) {
  await fetch(`http://localhost:8000/api/v1/webrtc/offer/${sessionId}`, {
    method: "PATCH",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      candidates: [{
        candidate: candidate.candidate,
        sdpMid: candidate.sdpMid,
        sdpMLineIndex: candidate.sdpMLineIndex
      }]
    })
  });
}
```

***

## Step-by-Step Breakdown

<Steps>
  <Step title="Get ICE Servers">
    ```javascript theme={null}
    const iceResponse = await fetch("http://localhost:8000/api/v1/webrtc/ice-servers");
    const { iceServers } = await iceResponse.json();
    ```

    The server returns STUN/TURN server configuration. If TURN credentials are configured (via [`HF_TOKEN`](/scope/guides/huggingface) or Twilio), this enables connections through firewalls.
  </Step>

  <Step title="Create Peer Connection">
    ```javascript theme={null}
    const pc = new RTCPeerConnection({ iceServers });
    ```
  </Step>

  <Step title="Create Data Channel">
    ```javascript theme={null}
    const dataChannel = pc.createDataChannel("parameters", { ordered: true });
    ```

    The data channel allows bidirectional communication:

    * **Client to Server**: Send parameter updates (prompts, settings)
    * **Server to Client**: Receive notifications (stream stopped, errors)
  </Step>

  <Step title="Add Video Transceiver">
    ```javascript theme={null}
    pc.addTransceiver("video");
    ```

    For receive-only mode (no input video), we add a video transceiver instead of a track. This tells WebRTC we want to receive video.
  </Step>

  <Step title="Handle Incoming Track">
    ```javascript theme={null}
    pc.ontrack = (event) => {
      if (event.streams[0]) {
        document.getElementById("video").srcObject = event.streams[0];
      }
    };
    ```
  </Step>

  <Step title="Send Offer with Initial Parameters">
    ```javascript theme={null}
    const response = await fetch("http://localhost:8000/api/v1/webrtc/offer", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        sdp: offer.sdp,
        type: offer.type,
        initialParameters: {
          prompts: [{ text: "Your prompt here", weight: 1.0 }],
          denoising_step_list: [1000, 750, 500, 250],
          manage_cache: true
        }
      })
    });
    ```
  </Step>

  <Step title="Complete Signaling">
    ```javascript theme={null}
    const answer = await response.json();
    await pc.setRemoteDescription({ type: answer.type, sdp: answer.sdp });
    ```
  </Step>
</Steps>

***

## Update Parameters During Streaming

After connection is established:

```javascript theme={null}
function updatePrompt(newPrompt) {
  if (dataChannel.readyState === "open") {
    dataChannel.send(JSON.stringify({
      prompts: [{ text: newPrompt, weight: 1.0 }]
    }));
  }
}

// Smooth transition to new prompt
function transitionToPrompt(newPrompt, steps = 8) {
  dataChannel.send(JSON.stringify({
    transition: {
      target_prompts: [{ text: newPrompt, weight: 1.0 }],
      num_steps: steps
    }
  }));
}
```

***

## Stopping the Stream

```javascript theme={null}
function stopStream(pc, dataChannel) {
  if (dataChannel) {
    dataChannel.close();
  }
  if (pc) {
    pc.close();
  }
}
```

***

## Error Handling

```javascript theme={null}
dataChannel.onmessage = (event) => {
  const data = JSON.parse(event.data);

  if (data.type === "stream_stopped") {
    if (data.error_message) {
      showError(data.error_message);
    }
    // Optionally attempt reconnection
    setTimeout(() => {
      startReceiveStream(lastPrompt);
    }, 2000);
  }
};

pc.onconnectionstatechange = () => {
  if (pc.connectionState === "failed") {
    console.error("WebRTC connection failed");
    // Handle reconnection
  }
};
```

***

## See Also

<CardGroup>
  <Card title="Send & Receive Video" href="/scope/reference/api/send-receive">
    Bidirectional video streaming (V2V)
  </Card>

  <Card title="Send Parameters" href="/scope/reference/api/parameters">
    All available parameters
  </Card>

  <Card title="Load Pipeline" href="/scope/reference/api/load-pipeline">
    Configure pipeline before streaming
  </Card>

  <Card title="VACE" href="/scope/reference/api/vace">
    Reference image conditioning
  </Card>
</CardGroup>


# Send & Receive Video (V2V)
Source: https://docs.daydream.live/scope/reference/api/send-receive

Transform input video in real-time with bidirectional WebRTC

# Send and Receive Video

This guide shows how to set up a WebRTC connection for **video-to-video mode** - sending input video (webcam, screen, or file) and receiving generated video.

## Overview

In video-to-video mode:

* You send video frames as input (e.g., webcam, screen capture)
* The server transforms the video based on your prompts
* Generated video is streamed back in real-time

## Prerequisites

1. Server is running: `uv run daydream-scope`
2. Models are downloaded for your pipeline
3. Pipeline is loaded (see [Load Pipeline](/scope/reference/api/load-pipeline))

***

## Complete Example

```javascript theme={null}
async function startBidirectionalStream(inputStream, initialPrompt = "A painting") {
  const API_BASE = "http://localhost:8000";

  // 1. Get ICE servers
  const iceResponse = await fetch(`${API_BASE}/api/v1/webrtc/ice-servers`);
  const { iceServers } = await iceResponse.json();

  // 2. Create peer connection
  const pc = new RTCPeerConnection({ iceServers });

  // State
  let sessionId = null;
  const queuedCandidates = [];

  // 3. Create data channel
  const dataChannel = pc.createDataChannel("parameters", { ordered: true });

  dataChannel.onopen = () => {
    console.log("Data channel ready");
  };

  dataChannel.onmessage = (event) => {
    const data = JSON.parse(event.data);
    if (data.type === "stream_stopped") {
      console.log("Stream stopped:", data.error_message);
      pc.close();
    }
  };

  // 4. Add LOCAL video track (for sending to server)
  inputStream.getTracks().forEach((track) => {
    if (track.kind === "video") {
      console.log("Adding video track for sending");
      pc.addTrack(track, inputStream);
    }
  });

  // 5. Handle REMOTE video track (from server)
  pc.ontrack = (event) => {
    if (event.streams && event.streams[0]) {
      document.getElementById("outputVideo").srcObject = event.streams[0];
    }
  };

  // 6. Connection monitoring
  pc.onconnectionstatechange = () => {
    console.log("Connection state:", pc.connectionState);
  };

  // 7. ICE candidate handling
  pc.onicecandidate = async (event) => {
    if (event.candidate) {
      if (sessionId) {
        await sendIceCandidate(sessionId, event.candidate);
      } else {
        queuedCandidates.push(event.candidate);
      }
    }
  };

  // 8. Create and send offer
  const offer = await pc.createOffer();
  await pc.setLocalDescription(offer);

  const response = await fetch(`${API_BASE}/api/v1/webrtc/offer`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      sdp: pc.localDescription.sdp,
      type: pc.localDescription.type,
      initialParameters: {
        input_mode: "video",
        prompts: [{ text: initialPrompt, weight: 1.0 }],
        denoising_step_list: [700, 500]
      }
    })
  });

  const answer = await response.json();
  sessionId = answer.sessionId;

  // 9. Set remote description
  await pc.setRemoteDescription({
    type: answer.type,
    sdp: answer.sdp
  });

  // 10. Flush queued candidates
  for (const candidate of queuedCandidates) {
    await sendIceCandidate(sessionId, candidate);
  }
  queuedCandidates.length = 0;

  return { pc, dataChannel, sessionId };
}

async function sendIceCandidate(sessionId, candidate) {
  await fetch(`http://localhost:8000/api/v1/webrtc/offer/${sessionId}`, {
    method: "PATCH",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      candidates: [{
        candidate: candidate.candidate,
        sdpMid: candidate.sdpMid,
        sdpMLineIndex: candidate.sdpMLineIndex
      }]
    })
  });
}
```

***

## Step-by-Step Breakdown

<Steps>
  <Step title="Get ICE Servers">
    ```javascript theme={null}
    const iceResponse = await fetch("http://localhost:8000/api/v1/webrtc/ice-servers");
    const { iceServers } = await iceResponse.json();
    ```

    The server returns STUN/TURN server configuration. If TURN credentials are configured (via `HF_TOKEN` or Twilio), this enables connections through firewalls.
  </Step>

  <Step title="Create Peer Connection">
    ```javascript theme={null}
    const pc = new RTCPeerConnection({ iceServers });
    ```
  </Step>

  <Step title="Create Data Channel">
    ```javascript theme={null}
    const dataChannel = pc.createDataChannel("parameters", { ordered: true });
    ```

    The data channel allows bidirectional communication:

    * **Client to Server**: Send parameter updates (prompts, settings)
    * **Server to Client**: Receive notifications (stream stopped, errors)
  </Step>

  <Step title="Add Input Video Track">
    ```javascript theme={null}
    inputStream.getTracks().forEach((track) => {
      if (track.kind === "video") {
        pc.addTrack(track, inputStream);
      }
    });
    ```

    Unlike receive-only mode which uses `addTransceiver("video")`, video-to-video mode adds an actual video track from your input source. This can be:

    * **Webcam**: `navigator.mediaDevices.getUserMedia({ video: true })`
    * **Screen capture**: `navigator.mediaDevices.getDisplayMedia({ video: true })`
    * **File/canvas**: Using a `<canvas>` element with `captureStream()`
  </Step>

  <Step title="Handle Incoming Track">
    ```javascript theme={null}
    pc.ontrack = (event) => {
      if (event.streams[0]) {
        document.getElementById("outputVideo").srcObject = event.streams[0];
      }
    };
    ```
  </Step>

  <Step title="Send Offer with Initial Parameters">
    ```javascript theme={null}
    const response = await fetch("http://localhost:8000/api/v1/webrtc/offer", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        sdp: offer.sdp,
        type: offer.type,
        initialParameters: {
          input_mode: "video",
          prompts: [{ text: "Your prompt here", weight: 1.0 }],
          denoising_step_list: [700, 500]
        }
      })
    });
    ```

    Note the `input_mode: "video"` parameter which tells the server to expect input video.
  </Step>

  <Step title="Complete Signaling">
    ```javascript theme={null}
    const answer = await response.json();
    await pc.setRemoteDescription({ type: answer.type, sdp: answer.sdp });
    ```
  </Step>
</Steps>

***

## Update Parameters During Streaming

After connection is established:

```javascript theme={null}
function updatePrompt(newPrompt) {
  if (dataChannel.readyState === "open") {
    dataChannel.send(JSON.stringify({
      prompts: [{ text: newPrompt, weight: 1.0 }]
    }));
  }
}

// Smooth transition to new prompt
function transitionToPrompt(newPrompt, steps = 8) {
  dataChannel.send(JSON.stringify({
    transition: {
      target_prompts: [{ text: newPrompt, weight: 1.0 }],
      num_steps: steps
    }
  }));
}
```

***

## Stopping the Stream

```javascript theme={null}
function stopStream(pc, dataChannel) {
  if (dataChannel) {
    dataChannel.close();
  }
  if (pc) {
    pc.close();
  }
}
```

***

## Error Handling

```javascript theme={null}
dataChannel.onmessage = (event) => {
  const data = JSON.parse(event.data);

  if (data.type === "stream_stopped") {
    if (data.error_message) {
      showError(data.error_message);
    }
    // Optionally attempt reconnection
    setTimeout(() => {
      startBidirectionalStream(inputStream, lastPrompt);
    }, 2000);
  }
};

pc.onconnectionstatechange = () => {
  if (pc.connectionState === "failed") {
    console.error("WebRTC connection failed");
    // Handle reconnection
  }
};
```

***

## VACE vs Standard V2V

When VACE is enabled on the pipeline (default), input video is routed through VACE for structural guidance. When disabled, input video is encoded and used for denoising.

```javascript theme={null}
// Load with VACE disabled for traditional V2V
await loadPipeline({
  pipeline_ids: ["longlive"],
  load_params: { vace_enabled: false }
});
```

See [VACE](/scope/reference/api/vace) for more details.

***

## Performance Tips

### Match Resolution

Set input resolution to match pipeline resolution for best quality:

```javascript theme={null}
// If pipeline loaded with 512x512
const stream = await navigator.mediaDevices.getUserMedia({
  video: { width: 512, height: 512 }
});
```

### Frame Rate

Lower frame rates reduce bandwidth and processing load:

```javascript theme={null}
video: { frameRate: { ideal: 15, max: 20 } }
```

***

## See Also

<CardGroup>
  <Card title="Receive Video" href="/scope/reference/api/receive-video">
    Text-to-video mode (no input)
  </Card>

  <Card title="Send Parameters" href="/scope/reference/api/parameters">
    Update parameters during streaming
  </Card>

  <Card title="VACE" href="/scope/reference/api/vace">
    Reference image conditioning
  </Card>

  <Card title="Load Pipeline" href="/scope/reference/api/load-pipeline">
    Configure pipeline before streaming
  </Card>
</CardGroup>


# VACE
Source: https://docs.daydream.live/scope/reference/api/vace

API endpoints for VACE reference images, control videos, and visual conditioning parameters

# Using VACE

VACE (Video All-In-One Creation and Editing) enables guiding generation using reference images and control videos. This guide shows how to use VACE for style transfer, character consistency, and video transformation.

## Overview

VACE allows you to:

* Condition generation on reference images (style, character, scene)
* Use control videos to preserve structure and motion
* Control the influence strength of visual conditioning

## Prerequisites

1. Pipeline loaded with VACE enabled (default):

```javascript theme={null}
await loadPipeline({
  pipeline_ids: ["longlive"],
  load_params: { vace_enabled: true }  // This is the default
});
```

2. For reference images: Images uploaded or available in the assets directory
3. For control videos: A video input source (webcam, screen capture, or file)

***

## Uploading Reference Images

Upload images via the assets API:

```javascript theme={null}
async function uploadReferenceImage(file) {
  const arrayBuffer = await file.arrayBuffer();
  const filename = encodeURIComponent(file.name);

  const response = await fetch(
    `http://localhost:8000/api/v1/assets?filename=${filename}`,
    {
      method: "POST",
      headers: { "Content-Type": "application/octet-stream" },
      body: arrayBuffer
    }
  );

  if (!response.ok) {
    throw new Error(`Upload failed: ${response.statusText}`);
  }

  return await response.json();
}

// Usage
const fileInput = document.getElementById("imageInput");
const file = fileInput.files[0];
const assetInfo = await uploadReferenceImage(file);
console.log("Uploaded:", assetInfo.path);
// Returns: { path: "/path/to/assets/image.png", ... }
```

***

## Setting Reference Images

### Via Initial Parameters

Set reference images when starting the WebRTC connection:

```javascript theme={null}
const response = await fetch("http://localhost:8000/api/v1/webrtc/offer", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({
    sdp: offer.sdp,
    type: offer.type,
    initialParameters: {
      prompts: [{ text: "A person walking in a forest", weight: 1.0 }],
      vace_ref_images: ["/path/to/reference.png"],
      vace_context_scale: 1.0
    }
  })
});
```

### Via Data Channel

Update reference images during streaming:

```javascript theme={null}
// Set new reference image
dataChannel.send(JSON.stringify({
  vace_ref_images: ["/path/to/new_reference.png"],
  vace_context_scale: 1.0
}));

// Multiple reference images
dataChannel.send(JSON.stringify({
  vace_ref_images: [
    "/path/to/style_ref.png",
    "/path/to/character_ref.png"
  ],
  vace_context_scale: 1.2
}));

// Clear reference images
dataChannel.send(JSON.stringify({
  vace_ref_images: []
}));
```

***

## VACE Parameters

| Parameter            | Type  | Range   | Default | Description                   |
| :------------------- | :---- | :------ | :------ | :---------------------------- |
| `vace_ref_images`    | array | -       | `[]`    | List of reference image paths |
| `vace_context_scale` | float | 0.0-2.0 | 1.0     | Visual conditioning strength  |

### Context Scale

The `vace_context_scale` controls how strongly reference images influence generation:

* **0.0**: No reference influence (pure text-to-video)
* **0.5**: Subtle influence, more creative freedom
* **1.0**: Balanced influence (default)
* **1.5**: Strong influence, closer to reference
* **2.0**: Maximum influence, may reduce diversity

```javascript theme={null}
// Subtle style influence
dataChannel.send(JSON.stringify({
  vace_ref_images: ["/path/to/style.png"],
  vace_context_scale: 0.5
}));

// Strong character preservation
dataChannel.send(JSON.stringify({
  vace_ref_images: ["/path/to/character.png"],
  vace_context_scale: 1.5
}));
```

***

## Using Control Videos

When VACE is enabled, you can send a control video to guide generation while preserving the structure and motion of your input. This works the same way as sending video in regular video-to-video mode.

### Sending Control Video

Set up a WebRTC connection with video input, just as you would for [Send and Receive Video](/scope/reference/api/send-receive):

```javascript theme={null}
// Get input video (webcam, screen capture, or file)
const inputStream = await navigator.mediaDevices.getUserMedia({ video: true });

// Add video track to peer connection
inputStream.getTracks().forEach((track) => {
  if (track.kind === "video") {
    pc.addTrack(track, inputStream);
  }
});

// Send offer with video input mode
const response = await fetch("http://localhost:8000/api/v1/webrtc/offer", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({
    sdp: offer.sdp,
    type: offer.type,
    initialParameters: {
      input_mode: "video",
      prompts: [{ text: "A cyberpunk city scene", weight: 1.0 }]
    }
  })
});
```

When VACE is enabled (the default), the input video is routed through VACE for structural guidance. The generated output will follow the motion and composition of your input video while applying the style and content from your prompts.

### Combining Control Video with Reference Images

You can use both control video and reference images together for maximum control:

```javascript theme={null}
// Send control video via WebRTC track (as shown above)
// Then set reference images via data channel
dataChannel.send(JSON.stringify({
  vace_ref_images: ["/path/to/style_reference.png"],
  vace_context_scale: 1.0
}));
```

This allows you to:

* Use the control video for motion and structure
* Use reference images for style, character appearance, or scene elements

***

## Listing Available Assets

Get existing assets in the assets directory:

```javascript theme={null}
async function listAssets(type = "image") {
  const response = await fetch(`http://localhost:8000/api/v1/assets?type=${type}`);
  return await response.json();
}

const { assets } = await listAssets("image");
console.log("Available reference images:", assets);
// [{ name: "ref1", path: "/path/to/ref1.png", ... }, ...]
```

***

## See Also

<CardGroup>
  <Card title="Receive Video" href="/scope/reference/api/receive-video">
    Text-to-video mode
  </Card>

  <Card title="Send & Receive Video" href="/scope/reference/api/send-receive">
    Video-to-video mode
  </Card>

  <Card title="Send Parameters" href="/scope/reference/api/parameters">
    All available parameters
  </Card>

  <Card title="VACE Guide" href="/scope/guides/vace">
    VACE concepts and best practices
  </Card>
</CardGroup>


# Pipeline Architecture
Source: https://docs.daydream.live/scope/reference/architecture/pipelines

Technical reference for the Scope pipeline system

# Understanding pipeline architecture

Pipelines are the core abstraction for handling streaming video in Scope. A pipeline encapsulates model loading, inference logic, configuration schemas, and metadata.

***

## Pipeline Definition

### The Pipeline Base Class

All pipelines inherit from the abstract `Pipeline` class:

```python theme={null}
from abc import ABC, abstractmethod

class Pipeline(ABC):
    @classmethod
    def get_config_class(cls) -> type["BasePipelineConfig"]:
        """Return the Pydantic config class for this pipeline."""
        ...

    @abstractmethod
    def __call__(self, **kwargs) -> dict:
        """Process video frames and return results."""
        pass
```

| Method               | Purpose                                                                |
| :------------------- | :--------------------------------------------------------------------- |
| `get_config_class()` | Returns the Pydantic config class that defines parameters and metadata |
| `__call__()`         | Processes input frames and returns generated video                     |

***

## Configuration Schema

Every pipeline defines a Pydantic configuration class that inherits from `BasePipelineConfig`. This class serves as the **single source of truth** for:

1. Pipeline metadata (ID, name, description, version)
2. Feature flags (LoRA support, VACE support, quantization)
3. Parameter definitions with validation constraints
4. UI rendering hints for the frontend

### Example Configuration

```python theme={null}
from pydantic import Field
from scope.core.pipelines.base_schema import BasePipelineConfig, ModeDefaults, ui_field_config

class LongLiveConfig(BasePipelineConfig):
    # Metadata (class variables)
    pipeline_id = "longlive"
    pipeline_name = "LongLive"
    pipeline_description = "A streaming autoregressive video diffusion model..."
    estimated_vram_gb = 20.0
    supports_lora = True
    supports_vace = True

    # Parameters (instance fields with validation)
    height: int = Field(
        default=320,
        ge=1,
        description="Output height in pixels",
        json_schema_extra=ui_field_config(
            order=4, component="resolution", is_load_param=True
        ),
    )

    noise_scale: float = Field(
        default=0.7,
        ge=0.0,
        le=1.0,
        description="Amount of noise to add during video generation",
        json_schema_extra=ui_field_config(
            order=7, component="noise", modes=["video"], is_load_param=True
        ),
    )
```

***

## Pipeline Metadata

Configuration classes declare metadata as class variables:

| Metadata               | Type             | Description                                |
| :--------------------- | :--------------- | :----------------------------------------- |
| `pipeline_id`          | `str`            | Unique identifier used for registry lookup |
| `pipeline_name`        | `str`            | Human-readable display name                |
| `pipeline_description` | `str`            | Description of capabilities                |
| `pipeline_version`     | `str`            | Semantic version string                    |
| `docs_url`             | `str \| None`    | Link to pipeline documentation             |
| `estimated_vram_gb`    | `float \| None`  | Estimated VRAM requirement in GB           |
| `artifacts`            | `list[Artifact]` | Model files required by the pipeline       |

### Feature Flags

Feature flags control which UI controls are shown:

| Flag                        | Effect                          |
| :-------------------------- | :------------------------------ |
| `supports_lora`             | Enables LoRA management UI      |
| `supports_vace`             | Enables VACE reference image UI |
| `supports_cache_management` | Enables cache controls          |
| `supports_quantization`     | Enables quantization selector   |
| `supports_kv_cache_bias`    | Enables KV cache bias slider    |

***

## Artifacts

Artifacts declare model files and resources that a pipeline requires. The system downloads these automatically before the pipeline loads.

### Available Artifact Types

| Type                      | Source          | Attributes                                       |
| :------------------------ | :-------------- | :----------------------------------------------- |
| `HuggingfaceRepoArtifact` | HuggingFace Hub | `repo_id`, `files`                               |
| `GoogleDriveArtifact`     | Google Drive    | `file_id`, `files` (optional), `name` (optional) |

### Example

```python theme={null}
from scope.core.pipelines.artifacts import HuggingfaceRepoArtifact, GoogleDriveArtifact

class MyPipelineConfig(BasePipelineConfig):
    pipeline_id = "my-pipeline"

    artifacts = [
        HuggingfaceRepoArtifact(
            repo_id="depth-anything/Video-Depth-Anything-Small",
            files=["video_depth_anything_vits.pth"],
        ),
        GoogleDriveArtifact(
            file_id="1Smy6gY7BkS_RzCjPCbMEy-TsX8Ma5B0R",
            files=["flownet.pkl"],
            name="RIFE",
        ),
    ]
```

Common artifacts shared across pipelines can be reused:

```python theme={null}
from scope.core.pipelines.common_artifacts import WAN_1_3B_ARTIFACT, VACE_ARTIFACT

class MyPipelineConfig(BasePipelineConfig):
    artifacts = [WAN_1_3B_ARTIFACT, VACE_ARTIFACT]
```

***

## Input Requirements

The `prepare()` method declares input frame requirements before processing. **Pipelines that accept video input must implement it.**

| Return Value                 | Meaning                                                     |
| :--------------------------- | :---------------------------------------------------------- |
| `Requirements(input_size=N)` | Pipeline needs N input frames before `__call__()`           |
| `None`                       | Pipeline operates in text-only mode (no video input needed) |

### Example

```python theme={null}
from scope.core.pipelines.interface import Requirements

class MyPipeline(Pipeline):
    def prepare(self, **kwargs) -> Requirements:
        return Requirements(input_size=4)

    def __call__(self, **kwargs) -> dict:
        video = kwargs.get("video")  # List of 4 frames
        # ... process video ...
```

### Multi-mode Pipeline Example

Pipelines that support both text-to-video and video-to-video modes use the `prepare_for_mode()` helper from `defaults.py`:

```python theme={null}
from scope.core.pipelines.defaults import prepare_for_mode
from scope.core.pipelines.interface import Requirements

class LongLivePipeline(Pipeline):
    def prepare(self, **kwargs) -> Requirements | None:
        """Return input requirements based on current mode."""
        return prepare_for_mode(self.__class__, self.components.config, kwargs)
```

The helper returns `Requirements` when video mode is active (indicated by `video=True` in kwargs) and `None` for text mode. The `input_size` is calculated from the pipeline's configuration.

**Why implement `prepare()`:**

* Without it, the frame processor cannot know how many frames to buffer before calling `__call__()`
* Enables efficient queue management - the processor sizes queues based on requirements
* Allows multi-mode pipelines to dynamically switch between text and video input modes

***

## Mode System

Pipelines can support multiple input modes with different default parameters:

```python theme={null}
class LongLiveConfig(BasePipelineConfig):
    # Base defaults
    height: int = Field(default=320, ...)
    width: int = Field(default=576, ...)

    # Mode-specific overrides
    modes = {
        "text": ModeDefaults(default=True),
        "video": ModeDefaults(
            height=512,
            width=512,
            noise_scale=0.7,
        ),
    }
```

| Mode    | Description                                |
| :------ | :----------------------------------------- |
| `text`  | Text-to-video generation from prompts only |
| `video` | Video-to-video with input conditioning     |

The `default=True` flag marks which mode is selected initially.

***

## Preprocessors and Postprocessors

Pipelines can be declared as preprocessors or postprocessors using the `usage` class variable:

```python theme={null}
from scope.core.pipelines.base_schema import BasePipelineConfig, ModeDefaults, UsageType

class MyPreprocessorConfig(BasePipelineConfig):
    pipeline_id = "my-preprocessor"
    pipeline_name = "My Preprocessor"
    usage = [UsageType.PREPROCESSOR]

    modes = {"video": ModeDefaults(default=True)}
```

| Type                      | Purpose                                  | UI Location            |
| :------------------------ | :--------------------------------------- | :--------------------- |
| `UsageType.PREPROCESSOR`  | Process input video before main pipeline | Preprocessor dropdown  |
| `UsageType.POSTPROCESSOR` | Process output video after main pipeline | Postprocessor dropdown |
| (empty list)              | Standard pipeline                        | Main pipeline selector |

***

## Dynamic UI Rendering

### JSON Schema Generation

Pydantic models automatically generate JSON Schema via `model_json_schema()`. The backend exposes this schema through the `/pipelines` endpoint, which the frontend consumes for dynamic UI rendering.

The schema includes:

* Field types and validation constraints (`minimum`, `maximum`, `enum`)
* Default values
* Descriptions (used as tooltips)
* Custom UI metadata (via `json_schema_extra`)

### Schema-to-UI Flow

```
Backend (Python)                    Frontend (React)
─────────────────                   ─────────────────
BasePipelineConfig
       │
       ▼
model_json_schema()
       │
       ▼
GET /pipelines  ───────────────────▶  configSchema
                                           │
                                           ▼
                                    parseConfigurationFields()
                                           │
                                           ▼
                                    Field type inference
                                           │
                                    ┌──────┴──────┐
                                    ▼             ▼
                              Primitive      Complex
                              widgets        components
```

### Field Type Inference

For fields without a `component` specified, the frontend automatically renders an appropriate widget based on the JSON Schema type. The frontend (`schemaSettings.ts`) infers widget types from schema properties:

| Schema Property                           | Inferred Type | Widget            |
| :---------------------------------------- | :------------ | :---------------- |
| `type: "boolean"`                         | `toggle`      | Toggle switch     |
| `type: "string"`                          | `text`        | Text input        |
| `type: "number"` with `minimum`/`maximum` | `slider`      | Slider with input |
| `type: "number"` without bounds           | `number`      | Number input      |
| `enum` or `$ref`                          | `enum`        | Select dropdown   |

### Two-Tier Component System

The UI uses a two-tier approach: **primitive fields** render as individual widgets based on inferred type, while **complex components** group related fields into unified UI blocks.

***

## UI Metadata

The `ui_field_config()` helper attaches rendering hints to schema fields:

```python theme={null}
height: int = Field(
    default=320,
    ge=1,
    description="Output height in pixels",
    json_schema_extra=ui_field_config(
        order=4,              # Display order (lower = higher)
        component="resolution", # Complex component grouping
        is_load_param=True,   # Requires pipeline reload
        modes=["video"],      # Only show in video mode
        label="Height",       # Short label
        category="configuration", # Panel placement
    ),
)
```

| Field           | Type        | Description                                                          |
| :-------------- | :---------- | :------------------------------------------------------------------- |
| `order`         | `int`       | Sort order for display (lower values appear first)                   |
| `component`     | `str`       | Groups fields into complex widgets (e.g., "resolution", "noise")     |
| `modes`         | `list[str]` | Restrict visibility to specific input modes                          |
| `is_load_param` | `bool`      | If `True`, field is disabled during streaming                        |
| `label`         | `str`       | Short display label; description becomes tooltip                     |
| `category`      | `str`       | `"configuration"` for Settings panel, `"input"` for Input & Controls |

### Complex Components

Fields with the same `component` value are grouped and rendered together:

| Component         | Fields                            | Rendered As              |
| :---------------- | :-------------------------------- | :----------------------- |
| `resolution`      | `height`, `width`                 | Linked dimension inputs  |
| `noise`           | `noise_scale`, `noise_controller` | Slider + toggle          |
| `vace`            | `vace_context_scale`              | VACE configuration panel |
| `lora`            | `lora_merge_strategy`             | LoRA manager             |
| `denoising_steps` | `denoising_steps`                 | Multi-step slider        |

Fields with the same `component` value are grouped and rendered once.

### Mode-Aware Filtering

Fields specify which modes they appear in via `modes`:

```python theme={null}
noise_scale: float = Field(
    ...,
    json_schema_extra=ui_field_config(modes=["video"]),
)
```

The frontend filters fields based on the current input mode, hiding irrelevant controls.

***

## Load-time vs Runtime Parameters

| Type              | `is_load_param` | Editable During Streaming | Example                        |
| :---------------- | :-------------- | :------------------------ | :----------------------------- |
| Load parameter    | `True`          | No                        | Resolution, quantization, seed |
| Runtime parameter | `False`         | Yes                       | Prompt strength, noise scale   |

Load parameters are passed when the pipeline loads and require a restart to change. Runtime parameters can be adjusted while streaming.

***

## Pipeline Registry

Pipelines register with the central `PipelineRegistry` at startup:

```python theme={null}
from scope.core.pipelines.registry import PipelineRegistry

PipelineRegistry.register(config_class.pipeline_id, pipeline_class)
```

### Built-in vs Plugin Pipelines

**Built-in pipelines** are registered automatically when the registry module is imported.

**Plugin pipelines** register through the pluggy hook system:

```python theme={null}
from scope.core.plugins.hookspecs import hookimpl

@hookimpl
def register_pipelines(register):
    from .pipelines import MyCustomPipeline
    register(MyCustomPipeline)
```

### GPU-Based Filtering

Pipelines with `estimated_vram_gb` set are only registered if a compatible GPU is detected. This prevents showing pipelines that cannot run on the current hardware.

***

## See Also

<CardGroup>
  <Card title="Developing Plugins" icon="code" href="/scope/guides/plugin-development">
    Create your own custom pipelines
  </Card>

  <Card title="Plugin Architecture" icon="puzzle-piece" href="/scope/reference/architecture/plugins">
    Technical details of the plugin system
  </Card>
</CardGroup>


# Plugin Architecture
Source: https://docs.daydream.live/scope/reference/architecture/plugins

Technical reference for the Scope plugin system

# Understanding plugin system

The Scope plugin system enables third-party extensions to provide custom pipelines. This document describes the architectural design and data flows that enable plugin discovery, installation, and lifecycle management.

### Architecture Layers

```
Desktop App (Electron) → Frontend (React) → Backend (FastAPI) → Plugins
```

Each layer has distinct responsibilities, communicating through well-defined interfaces.

***

## Key Technologies

* **pluggy**: Python hook system for pipeline registration and discovery
* **uv**: Fast Python package manager for dependency resolution and installation
* **Electron IPC**: Communication bridge between desktop app and frontend

***

## System Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                        Desktop App (Electron)                    │
│  ┌─────────────┐    ┌──────────────┐    ┌───────────────────┐  │
│  │ Deep Links  │───▶│  IPC Bridge  │◀──▶│ Python Process    │  │
│  │ File Browse │    │              │    │ Manager           │  │
│  └─────────────┘    └──────────────┘    └───────────────────┘  │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                         Frontend (React)                         │
│  ┌─────────────┐    ┌──────────────┐    ┌───────────────────┐  │
│  │ Settings UI │───▶│  API Client  │───▶│ State Management  │  │
│  └─────────────┘    └──────────────┘    └───────────────────┘  │
└─────────────────────────────────────────────────────────────────┘
                              │ HTTP
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                      Backend (FastAPI)                           │
│  ┌─────────────┐    ┌──────────────┐    ┌───────────────────┐  │
│  │  REST API   │───▶│Plugin Manager│───▶│ Pipeline Registry │  │
│  └─────────────┘    └──────────────┘    └───────────────────┘  │
│                              │                                   │
│                     ┌────────┴────────┐                         │
│                     ▼                 ▼                         │
│              ┌────────────┐    ┌────────────┐                   │
│              │ Dependency │    │   Venv     │                   │
│              │ Validator  │    │ Snapshot   │                   │
│              └────────────┘    └────────────┘                   │
└─────────────────────────────────────────────────────────────────┘
```

***

## Plugin Discovery

Plugins integrate with Scope through Python's entry point mechanism:

1. Plugins declare entry points in their `pyproject.toml` under `[project.entry-points."scope"]`
2. The backend uses pluggy hooks to discover installed plugins at startup
3. Each plugin implements a `register_pipelines` hook to register its pipeline implementations
4. The Pipeline Registry maintains a mapping of pipeline IDs to their implementations

***

## Plugin Sources

| Source    | Description                                       |
| :-------- | :------------------------------------------------ |
| **PyPI**  | Standard Python packages                          |
| **Git**   | Direct repository installation                    |
| **Local** | File system paths (editable mode for development) |

***

## Installation Flow

```
User                Frontend              Backend              Desktop App
 │                    │                     │                      │
 │ Click Install      │                     │                      │
 │───────────────────▶│                     │                      │
 │                    │ POST /plugins       │                      │
 │                    │────────────────────▶│                      │
 │                    │                     │ Validate deps        │
 │                    │                     │────────┐             │
 │                    │                     │◀───────┘             │
 │                    │                     │ Capture venv         │
 │                    │                     │────────┐             │
 │                    │                     │◀───────┘             │
 │                    │                     │ Install via uv       │
 │                    │                     │────────┐             │
 │                    │                     │◀───────┘             │
 │                    │     200 OK          │                      │
 │                    │◀────────────────────│                      │
 │                    │ Request restart     │                      │
 │                    │─────────────────────────────────────────────▶
 │                    │                     │                      │ Respawn
 │                    │                     │                      │────────┐
 │                    │                     │                      │◀───────┘
 │                    │ Poll health         │                      │
 │                    │────────────────────▶│                      │
 │                    │ Refresh pipelines   │                      │
 │                    │────────────────────▶│                      │
```

**Installation steps:**

1. User initiates install (UI or deep link)
2. Frontend sends install request to backend API
3. Backend validates dependencies won't conflict with existing environment
4. Backend captures current venv state (for rollback)
5. Backend resolves and installs dependencies via uv
6. Backend updates plugin registry
7. Frontend triggers server restart
8. Server restarts with new plugin loaded
9. Frontend polls until server is healthy
10. Frontend refreshes pipeline list

**Rollback**: If installation fails at any step, the venv is restored to its captured state.

***

## Plugin Update Flow

The update flow has two phases: **detection** (happens automatically when plugins are listed) and **execution** (triggered by the user). The execution phase reuses the installation flow with an `upgrade: true` flag.

### Update Detection

```
Frontend                        Backend
 │                                │
 │ GET /plugins                   │
 │───────────────────────────────▶│
 │                                │ For each plugin:
 │                                │ _check_plugin_update()
 │                                │────────┐
 │                                │        │ Compare installed
 │                                │        │ vs latest version
 │                                │◀───────┘
 │  Plugin list                   │
 │  (with update_available flags) │
 │◀───────────────────────────────│
```

**Step by step:**

1. Frontend fetches the plugin list via `GET /plugins`
2. Backend iterates over installed plugins and calls `_check_plugin_update()` for each
3. For PyPI plugins, the installed version is compared against the latest version on PyPI
4. For Git plugins, the installed commit hash is compared against the latest commit on the remote
5. Local plugins are skipped (use Reload instead)
6. Each plugin in the response includes an `update_available` flag
7. Frontend displays an update badge on plugins where the flag is `true`

### Update Execution

```
User       Frontend              Backend              Desktop App
 │           │                     │                      │
 │ Click     │                     │                      │
 │ Update    │                     │                      │
 │──────────▶│                     │                      │
 │           │ POST /plugins       │                      │
 │           │ {upgrade: true}     │                      │
 │           │────────────────────▶│                      │
 │           │                     │ Capture venv         │
 │           │                     │────────┐             │
 │           │                     │◀───────┘             │
 │           │                     │ Compile with         │
 │           │                     │ --upgrade-package    │
 │           │                     │────────┐             │
 │           │                     │◀───────┘             │
 │           │                     │ Sync deps            │
 │           │                     │────────┐             │
 │           │                     │◀───────┘             │
 │           │     200 OK          │                      │
 │           │◀────────────────────│                      │
 │           │ Request restart     │                      │
 │           │─────────────────────────────────────────────▶
 │           │                     │                      │ Respawn
 │           │                     │                      │────────┐
 │           │                     │                      │◀───────┘
 │           │ Poll health         │                      │
 │           │────────────────────▶│                      │
 │           │ Refresh pipelines   │                      │
 │           │────────────────────▶│                      │
```

**Step by step:**

1. User clicks the Update button on a plugin
2. Frontend sends `POST /plugins` with the plugin spec and `upgrade: true`
3. Backend captures the current venv state (for rollback)
4. Backend runs `uv pip compile` with `--upgrade-package` targeting only the plugin package
5. Backend syncs the environment with the newly resolved dependencies
6. Backend updates the plugin registry
7. Frontend triggers server restart
8. Server restarts with the updated plugin loaded
9. Frontend polls until server is healthy
10. Frontend refreshes pipeline list

**Rollback**: If the update fails at any step, the venv is restored to its pre-update state, just like a failed installation.

### Source-Specific Update Behavior

| Source    | Detection Method                                            | Notes                                                   |
| :-------- | :---------------------------------------------------------- | :------------------------------------------------------ |
| **PyPI**  | Compares installed version against latest version on PyPI   | Standard version comparison                             |
| **Git**   | Compares installed commit hash against latest remote commit | Detects new commits on the default branch               |
| **Local** | Skipped                                                     | Local plugins use [Reload](#manual-reload-flow) instead |

***

## Uninstallation Flow

1. User initiates uninstall
2. Backend unloads any active pipelines from the plugin
3. Backend removes plugin from registry
4. Backend uninstalls package via uv
5. Frontend triggers server restart
6. Frontend refreshes pipeline list

***

## Manual Reload Flow

For local/editable plugins, developers can trigger a reload to pick up code changes:

<Tabs>
  <Tab title="Steps" icon="list-ol">
    1. Developer modifies code
    2. Developer clicks Reload button
    3. Frontend requests server restart
    4. Server restarts with fresh Python module imports
    5. Code changes take effect

    In standalone mode (without the desktop app), the reload flow is the same except the server performs a self-restart instead of being respawned by the desktop app (see [Standalone Mode](#standalone-mode) below).
  </Tab>

  <Tab title="Sequence Diagram" icon="diagram-project">
    ```
    Developer      Frontend              Backend              Desktop App
     │                │                     │                      │
     │ Modify code    │                     │                      │
     │────────┐       │                     │                      │
     │◀───────┘       │                     │                      │
     │ Click Reload   │                     │                      │
     │───────────────▶│                     │                      │
     │                │ Request restart     │                      │
     │                │───────────────────────────────────────────▶│
     │                │                     │                      │ Respawn server
     │                │                     │                      │────────┐
     │                │                     │                      │◀───────┘
     │                │ Poll health         │                      │
     │                │────────────────────▶│                      │
     │                │ Refresh pipelines   │                      │
     │                │────────────────────▶│                      │
    ```

    This triggers a server restart to ensure Python modules are fully reloaded.
  </Tab>
</Tabs>

***

## Deep Link Installation

External sources can facilitate plugin installation via protocol URLs:

```
daydream-scope://install-plugin?package=<spec>
```

| Source | Raw Spec                               | Encoded URL                                                                                |
| :----- | :------------------------------------- | :----------------------------------------------------------------------------------------- |
| PyPI   | `my-plugin`                            | `daydream-scope://install-plugin?package=my-plugin`                                        |
| Git    | `git+https://github.com/user/repo.git` | `daydream-scope://install-plugin?package=git%2Bhttps%3A%2F%2Fgithub.com%2Fuser%2Frepo.git` |

**Flow:**

1. External source opens the deep link URL
2. Desktop app receives URL via OS protocol handler
3. If app is starting: stores pending deep link for later processing
4. Once frontend is loaded: sends action via IPC to renderer
5. Frontend opens settings with plugin tab and pre-filled package spec
6. User confirms installation

***

## Component Responsibilities

### Backend Components

| Component                | Responsibility                                                               |
| :----------------------- | :--------------------------------------------------------------------------- |
| **Plugin Manager**       | Singleton that manages plugin lifecycle (install, uninstall, update, reload) |
| **Dependency Validator** | Pre-validates that new packages won't break existing environment             |
| **Venv Snapshot**        | Captures and restores environment state for safe rollback                    |
| **Pipeline Registry**    | Maps pipeline IDs to their implementations and source plugins                |
| **REST API**             | Exposes plugin operations to frontend                                        |

### Frontend Components

| Component               | Responsibility                              |
| :---------------------- | :------------------------------------------ |
| **Settings Dialog**     | User interface for plugin management        |
| **API Client**          | HTTP calls to backend plugin endpoints      |
| **Restart Coordinator** | Handles server restart and health polling   |
| **Pipeline Context**    | Refreshes available pipelines after changes |

### Desktop App Components

| Component                  | Responsibility                                          |
| :------------------------- | :------------------------------------------------------ |
| **Python Process Manager** | Spawns/respawns backend server, handles restart signals |
| **Deep Link Handler**      | Receives and parses protocol URLs                       |
| **IPC Bridge**             | Communicates between main process and renderer          |
| **File Browser**           | Native dialog for selecting local plugin directories    |

<Note>
  The File Browser is a desktop convenience feature. In standalone mode, users can type local paths directly into the plugin installation input field.
</Note>

***

## Server Restart Protocol

### Managed Mode (Desktop App)

When running in the desktop app, server restarts are handled automatically:

<Tabs>
  <Tab title="Summary" icon="list">
    * Backend exits with code 42 (signals intentional restart)
    * Desktop app waits for port release
    * Desktop app respawns server process
    * Frontend polls health endpoint until ready
  </Tab>

  <Tab title="Sequence Diagram" icon="diagram-project">
    ```
    Frontend              Backend              Desktop App
     │                     │                      │
     │ POST /restart       │                      │
     │────────────────────▶│                      │
     │                     │ Exit code 42         │
     │                     │─────────────────────▶│
     │                     │                      │ Wait for port
     │                     │                      │────────┐
     │                     │                      │◀───────┘
     │                     │                      │ Respawn server
     │                     │◀─────────────────────│
     │ Poll /health        │                      │
     │────────────────────▶│                      │
     │     200 OK          │                      │
     │◀────────────────────│                      │
     │ Resume ops          │                      │
     │────────┐            │                      │
     │◀───────┘            │                      │
    ```

    **Key points:**

    * Exit code 42 signals intentional restart (not a crash)
    * Brief wait ensures the port is released before respawn
    * Frontend polls health endpoint until server is ready
    * This ensures proper Python module reloading since imports are cached
  </Tab>
</Tabs>

### Standalone Mode

When running the server directly (`uv run daydream-scope`):

* **Unix/macOS**: Uses `os.execv()` to replace the current process in-place
* **Windows**: Spawns a new subprocess and exits the old one

***

## Data Storage

Plugin state is persisted in the user's data directory:

| Data          | Location                                 | Purpose                                                      |
| :------------ | :--------------------------------------- | :----------------------------------------------------------- |
| Plugin list   | `~/.daydream-scope/plugins/plugins.txt`  | Installed package specs                                      |
| Resolved deps | `~/.daydream-scope/plugins/resolved.txt` | Lock file for reproducibility; baseline for update detection |
| Venv backup   | `~/.daydream-scope/plugins/freeze.txt`   | Rollback state                                               |

***

## Error Handling

The plugin system uses defensive error handling at each stage:

| Error Type                | Detection                          | Recovery                                      |
| :------------------------ | :--------------------------------- | :-------------------------------------------- |
| **Dependency conflicts**  | Dry-run compilation before install | Installation blocked with clear error message |
| **Installation failures** | Exception during uv install        | Venv rolled back to pre-install state         |
| **Runtime errors**        | Pipeline execution failure         | Pipeline unloaded without affecting others    |
| **Network errors**        | Health check timeouts              | Frontend retries with exponential backoff     |

This multi-layer approach ensures that plugin operations cannot corrupt the base Scope installation.

***

## See Also

<CardGroup>
  <Card title="Using Plugins" icon="plug" href="/scope/guides/plugins">
    Install and manage plugins
  </Card>

  <Card title="Developing Plugins" icon="code" href="/scope/guides/plugin-development">
    Create your own custom pipelines
  </Card>

  <Card title="Pipeline Architecture" icon="sitemap" href="/scope/reference/architecture/pipelines">
    Technical details of the pipeline system
  </Card>

  <Card title="Tutorial: Build a VFX Plugin" icon="wand-magic-sparkles" href="/scope/tutorials/build-video-effects-plugin">
    Step-by-step tutorial building a complete plugin from scratch
  </Card>
</CardGroup>


# Environment Variables
Source: https://docs.daydream.live/scope/reference/environment-variables

All environment variables supported by Daydream Scope

# Scope Environment Variables

Scope reads the following environment variables at startup. Set them in your shell, `.env` file, or cloud provider's configuration panel. The `HF_TOKEN` can also be set directly in the Scope UI via **Settings > API Keys** - see [HuggingFace Auth](/scope/guides/huggingface) for details.

***

## Authentication

| Variable   | Default | Description                                                                                                                                                                                                                                        |
| :--------- | :------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `HF_TOKEN` | -       | HuggingFace access token for downloading gated models and obtaining Cloudflare TURN credentials for WebRTC. Can also be set via **Settings > API Keys** in the Scope UI. See [HuggingFace Auth](/scope/guides/huggingface) for setup instructions. |

***

## TURN Server Credentials

Scope uses TURN servers for WebRTC connections behind firewalls. By default, it obtains credentials from Cloudflare using your `HF_TOKEN`. You can override this with direct Cloudflare or Twilio credentials.

| Variable                        | Default | Description                                                                             |
| :------------------------------ | :------ | :-------------------------------------------------------------------------------------- |
| `CLOUDFLARE_TURN_KEY_ID`        | -       | Cloudflare TURN key ID for direct TURN configuration (alternative to `HF_TOKEN`)        |
| `CLOUDFLARE_TURN_KEY_API_TOKEN` | -       | Cloudflare TURN key API token for direct TURN configuration (alternative to `HF_TOKEN`) |
| `TWILIO_ACCOUNT_SID`            | -       | Twilio account SID for TURN server credentials                                          |
| `TWILIO_AUTH_TOKEN`             | -       | Twilio auth token for TURN server credentials                                           |

***

## Recording

| Variable                            | Default | Description                                                                                                                  |
| :---------------------------------- | :------ | :--------------------------------------------------------------------------------------------------------------------------- |
| `RECORDING_ENABLED`                 | `false` | Enable recording as a fallback when no recording parameter is provided via the UI. The UI recording toggle takes precedence. |
| `RECORDING_MAX_LENGTH`              | `1h`    | Maximum total recording length (sum of all segments). Supports `1h`, `30m`, `120s`, or plain seconds (e.g. `3600`).          |
| `RECORDING_STARTUP_CLEANUP_ENABLED` | `true`  | Clean up recording files from previous sessions at startup.                                                                  |

***

## Directory Overrides

By default, Scope stores data in `~/.daydream-scope/`. Use these variables to override specific directories.

| Variable                     | Default                     | Description                            |
| :--------------------------- | :-------------------------- | :------------------------------------- |
| `DAYDREAM_SCOPE_MODELS_DIR`  | `~/.daydream-scope/models`  | Directory for downloaded model weights |
| `DAYDREAM_SCOPE_LOGS_DIR`    | `~/.daydream-scope/logs`    | Directory for application logs         |
| `DAYDREAM_SCOPE_PLUGINS_DIR` | `~/.daydream-scope/plugins` | Directory for plugin data and state    |

***

## Debugging

| Variable          | Default | Description                                             |
| :---------------- | :------ | :------------------------------------------------------ |
| `PIPELINE`        | -       | Force-load a specific pipeline at startup (for testing) |
| `VERBOSE_LOGGING` | -       | Enable verbose logging for uvicorn, FastAPI, and aiortc |

***

## See Also

<CardGroup>
  <Card title="HuggingFace Auth" icon="key" href="/scope/guides/huggingface">
    Set up your HuggingFace token
  </Card>

  <Card title="System Requirements" icon="server" href="/scope/reference/system-requirements">
    Hardware and software requirements
  </Card>
</CardGroup>


# Reference
Source: https://docs.daydream.live/scope/reference/index

VAE types, environment variables, system requirements, and additional technical details

# Scope Reference

Technical reference for VAE configuration, environment variables, and system requirements. For API documentation and pipeline details, see the dedicated sections below.

***

## In This Section

<CardGroup>
  <Card title="VAE Types" icon="cube" href="/scope/reference/vae">
    Configure the Variational Autoencoder for quality/speed tradeoffs
  </Card>

  <Card title="Environment Variables" icon="gear" href="/scope/reference/environment-variables">
    All environment variables supported by Scope
  </Card>

  <Card title="System Requirements" icon="server" href="/scope/reference/system-requirements">
    GPU, RAM, and OS requirements for each pipeline
  </Card>

  <Card title="Plugin Architecture" icon="puzzle-piece" href="/scope/reference/architecture/plugins">
    Technical details of the plugin system
  </Card>
</CardGroup>

***

## See Also

<CardGroup>
  <Card title="API Reference" icon="brackets-curly" href="/scope/reference/api/index">
    Server API, WebRTC signaling, and real-time parameter updates
  </Card>

  <Card title="Pipelines" icon="layer-group" href="/scope/reference/pipelines">
    Compare pipelines - features, VRAM requirements, and use cases
  </Card>
</CardGroup>

***

## Open Source Contributions

Scope is open source, and we welcome your contributions!

<CardGroup>
  <Card title="Contribute on GitHub" icon="github" href="https://github.com/daydreamlive/scope">
    Submit code, tackle open issues, and help build the future of real-time AI video
  </Card>

  <Card title="Share in Discord" icon="discord" href="https://discord.com/invite/5sZu8xmn6U">
    Report issues, share feedback, or connect with the community in our #scope channel
  </Card>
</CardGroup>


# Overview
Source: https://docs.daydream.live/scope/reference/pipelines

Compare and choose the right autoregressive video diffusion pipeline for your use case

# Pipelines Supported in Scope

Scope supports five autoregressive video diffusion pipelines. All are built on Wan2.1 base models and use the [Self-Forcing](https://self-forcing.github.io/) training methodology, except RewardForcing which uses Rewarded Distribution Matching.

Each pipeline has its own strengths - some optimize for speed, others for quality or long-context consistency. The comparison tables below summarize the key differences to help you choose.

***

## Quick Comparison

Four pipelines use the smaller Wan2.1 1.3B model, while Krea Realtime uses the larger 14B model for higher quality output at the cost of increased VRAM requirements.

| Pipeline                                                            | Base Model  | Creator                              | Estimated VRAM\* |
| :------------------------------------------------------------------ | :---------- | :----------------------------------- | ---------------: |
| [StreamDiffusion V2](/scope/reference/pipelines/streamdiffusion-v2) | Wan2.1 1.3B | StreamDiffusion team                 |           \~20GB |
| [LongLive](/scope/reference/pipelines/longlive)                     | Wan2.1 1.3B | Nvidia, MIT, HKUST, HKU, THU         |           \~20GB |
| [Krea Realtime](/scope/reference/pipelines/krea-realtime)           | Wan2.1 14B  | Krea                                 |           \~32GB |
| [RewardForcing](/scope/reference/pipelines/reward-forcing)          | Wan2.1 1.3B | ZJU, Ant Group, SIAS-ZJU, HUST, SJTU |           \~20GB |
| [MemFlow](/scope/reference/pipelines/memflow)                       | Wan2.1 1.3B | Kling                                |           \~20GB |

\*Estimated runtime VRAM usage. A 24GB GPU (e.g. RTX 4090) is the minimum commercially available card for 1.3B pipelines. See [System Requirements](/scope/reference/system-requirements) for minimum hardware specs.

***

## Feature Support

All pipelines support both Text-to-Video (T2V) and Video-to-Video (V2V) generation modes, as well as LoRA adapters and VACE conditioning. MemFlow is unique in having a memory bank for improved long-context consistency.

| Feature              | StreamDiffusion V2 |  LongLive  | Krea Realtime | RewardForcing |   MemFlow  |
| :------------------- | :----------------: | :--------: | :-----------: | :-----------: | :--------: |
| Text-to-Video (T2V)  |         Yes        |     Yes    |      Yes      |      Yes      |     Yes    |
| Video-to-Video (V2V) |         Yes        |     Yes    |   Limited\*   |      Yes      |     Yes    |
| LoRA Support         |     1.3B LoRAs     | 1.3B LoRAs |   14B LoRAs   |   1.3B LoRAs  | 1.3B LoRAs |
| VACE Support         |         Yes        |     Yes    |      Yes      |      Yes      |     Yes    |
| Memory Bank          |         No         |     No     |       No      |       No      |     Yes    |

<Warning>
  \*Krea Realtime's regular V2V mode (latent initialization) has known quality issues. Use **VACE V2V** (visual conditioning with input video) for better results.
</Warning>

***

## Pipeline Details

<CardGroup>
  <Card title="StreamDiffusion V2" href="/scope/reference/pipelines/streamdiffusion-v2">
    Real-time streaming from the original StreamDiffusion creators
  </Card>

  <Card title="LongLive" href="/scope/reference/pipelines/longlive">
    Smooth prompt transitions and extended generation from Nvidia
  </Card>

  <Card title="Krea Realtime" href="/scope/reference/pipelines/krea-realtime">
    14B model for highest quality generation
  </Card>
</CardGroup>

<CardGroup>
  <Card title="RewardForcing" href="/scope/reference/pipelines/reward-forcing">
    Reward-matched training for improved output quality
  </Card>

  <Card title="MemFlow" href="/scope/reference/pipelines/memflow">
    Memory bank for long-context consistency
  </Card>
</CardGroup>

***

## Shared Parameters

All pipelines share these common parameters.

### Resolution

Generation is faster at smaller resolutions, resulting in smoother video. The visual quality is best at **832x480**, which is the training resolution for most pipelines. You may need a more powerful GPU to maintain high FPS at this resolution.

### Seed

The seed parameter enables reproducible generations. If you find a seed value that produces good results with a specific prompt sequence, save it to reproduce that generation later.

***

## Prompting

These techniques apply to all pipelines and significantly improve output quality.

### Subject and Background Anchors

Include a clear **subject** (who/what) and **background/setting** (where) in each prompt. For scene continuity, reference the same subject and/or setting across prompts.

```
"A 3D animated scene. A panda walks along a path towards the camera in a park on a spring day."
```

```
"A 3D animated scene. A panda halts along a path in a park on a spring day."
```

### Cinematic Long Takes

The models work better with **long cinematic takes** rather than rapid shot-by-shot transitions. Avoid quick cutscenes, rapid scene changes, and jump cuts. Instead, let scenes flow naturally with gradual transitions.

### Long, Detailed Prompts

All pipelines perform better with **detailed prompts**. A helpful technique is to expand a base prompt using an LLM (ChatGPT, Claude, etc.).

**Base prompt:**

```
"A cartoon dog jumping and then running."
```

**Expanded prompt:**

```
"A cartoon dog with big expressive eyes and floppy ears suddenly leaps into the frame, tail wagging, and then sprints joyfully toward the camera. Its oversized paws pound playfully on the ground, tongue hanging out in excitement. The animation style is colorful, smooth, and bouncy, with exaggerated motion to emphasize energy and fun. The background blurs slightly with speed lines, giving a lively, comic-style effect as if the dog is about to jump right into the viewer."
```

***

## See Also

<CardGroup>
  <Card title="VAE Types" icon="cube" href="/scope/reference/vae">
    Configure VAE for quality/speed tradeoffs
  </Card>

  <Card title="System Requirements" icon="server" href="/scope/reference/system-requirements">
    Hardware requirements for each pipeline
  </Card>

  <Card title="Pipeline Architecture" icon="sitemap" href="/scope/reference/architecture/pipelines">
    Technical details for plugin developers
  </Card>
</CardGroup>


# Krea Realtime Video
Source: https://docs.daydream.live/scope/reference/pipelines/krea-realtime

Streaming pipeline and autoregressive video diffusion model from Krea

# Krea Realtime Video Pipeline

[Krea Realtime Video](https://www.krea.ai/blog/krea-realtime-14b) is a streaming pipeline and autoregressive video diffusion model from Krea.

The model is trained using Self-Forcing on Wan2.1 14b.

### At a Glance

|                    |                            |
| :----------------- | :------------------------- |
| **Base Model**     | Wan2.1 14B                 |
| **Estimated VRAM** | \~32GB (40GB+ recommended) |
| **Training**       | Self-Forcing               |
| **LoRA Support**   | 14B LoRAs                  |
| **VACE Support**   | Yes                        |
| **T2V / V2V**      | Yes / Limited\*            |

<Warning>
  \*Regular V2V (using input video for latent initialization) has known quality issues. Use **VACE V2V** (visual conditioning with input video) for better results.
</Warning>

***

## Examples

The following examples include timeline JSON files with the prompts used so you can try them as well.

A GPU with >40GB VRAM (e.g. H100, RTX 6000 Pro) is recommended for these examples since they use a higher resolution.

### Flower Bloom

<video />

<CardGroup>
  <Card title="Daydream Project" icon="globe" href="https://app.daydream.live/creators/yondonfu/creations/flowers?utm_source=docs&utm_medium=web&utm_campaign=docs2com">
    View on Community Hub
  </Card>

  <Card title="Timeline JSON file" icon="file-code" href="/workflows/scope/pipelines/krea-realtime/timeline-krea-flower-bloom.json">
    Download timeline
  </Card>
</CardGroup>

### Abstract Shape

<video />

<CardGroup>
  <Card title="Daydream Project" icon="globe" href="https://app.daydream.live/creators/yondonfu/creations/abstract?utm_source=docs&utm_medium=web&utm_campaign=docs2com">
    View on Community Hub
  </Card>

  <Card title="Timeline JSON file" icon="file-code" href="/workflows/scope/pipelines/krea-realtime/timeline-krea-abstract-shape.json">
    Download timeline
  </Card>
</CardGroup>

A >= 32 GB VRAM GPU (eg RTX 5090) is recommended for these examples which have lower VRAM requirements due to the lower resolution.

### Flower Bloom (Low Resolution)

<video />

<CardGroup>
  <Card title="Daydream Project" icon="globe" href="https://app.daydream.live/creators/yondonfu/creations/flowers-low?utm_source=docs&utm_medium=web&utm_campaign=docs2com">
    View on Community Hub
  </Card>

  <Card title="Timeline JSON file" icon="file-code" href="/workflows/scope/pipelines/krea-realtime/timeline-krea-flower-bloom-low-res.json">
    Download timeline
  </Card>
</CardGroup>

***

## Acceleration

The pipeline uses different attention kernels to accelerate inference depending on the hardware used:

* [SageAttention 2](https://github.com/thu-ml/SageAttention) is used on all GPUs *except* for Hopper GPUs (eg H100). If you run into video quality issues (which some folks have reported while using SageAttention) you can restart the server with `DISABLE_SAGEATTENTION=1` (eg `DISABLE_SAGEATTENTION=1 uv run daydream-scope`) to fallback to Flash Attention 2.
* [Flash Attention 2](https://github.com/Dao-AILab/flash-attention) is the fallback if SageAttention 2 is disabled.
* [Flash Attention 3](https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#flashattention-3-beta-release) is used on Hopper GPUs (eg H100).

***

## Resolution

The generation will be faster for smaller resolutions resulting in smoother video. The visual quality will be better at higher resolutions (eg 832x480 and larger), but you may need a more powerful GPU in order to achieve a higher FPS.

***

## Seed

The seed parameter in the UI can be used to reproduce generations. If you like the generation for a certain seed value and sequence of prompts you can re-use that value later with those same prompts to reproduce the generation.

***

## Prompting

**Subject and Background/Setting Anchors**

The model works better if you include a subject (who/what) and background/setting (where) in each prompt. If you want continuity in the next scene then you can continue referencing the same subject and/or background/setting.

For example:

```
"A 3D animated scene. A **panda** walks along a path towards the camera in a park on a spring day."

"A 3D animated scene. A **panda** halts along a path in a park on a spring day."
```

**Cinematic Long Takes**

The model works better for scene transitions that involve long cinematic long takes and works less well with rapid shot-by-shot transitions or fast cutscenes.

**Long, Detailed Prompts**

The model works better with long, detailed prompts. A helpful technique to extend prompts is to take a base prompt and then ask a LLM chatbot (eg. ChatGPT, Claude, Gemini, etc.) to write a more detailed version.

If your base prompt is:

```
"A cartoon dog jumping and then running."
```

Then, the extended prompt could be:

```
"A cartoon dog with big expressive eyes and floppy ears suddenly leaps into the frame, tail wagging, and then sprints joyfully toward the camera. Its oversized paws pound playfully on the ground, tongue hanging out in excitement. The animation style is colorful, smooth, and bouncy, with exaggerated motion to emphasize energy and fun. The background blurs slightly with speed lines, giving a lively, comic-style effect as if the dog is about to jump right into the viewer."
```

***

## Offline Generation

A test [script](https://github.com/daydreamlive/scope/blob/main/src/scope/core/pipelines/krea_realtime_video/test.py) can be used for offline generation.

If the model weights are not downloaded yet:

```bash theme={null}
# Run from scope directory
uv run download_models --pipeline krea-realtime-video
```

Then:

```bash theme={null}
# Run from scope directory
uv run -m scope.core.pipelines.krea_realtime_video.test
```

This will create an `output.mp4` file in the `krea_realtime_video` directory.

***

## See Also

### Other Pipelines

<Card title="StreamDiffusion V2" href="/scope/reference/pipelines/streamdiffusion-v2">
  Real-time streaming from the original StreamDiffusion creators
</Card>

<Card title="LongLive" href="/scope/reference/pipelines/longlive">
  Smooth prompt transitions and extended generation from Nvidia
</Card>

<Card title="RewardForcing" href="/scope/reference/pipelines/reward-forcing">
  Reward-matched training for improved output quality
</Card>

<Card title="MemFlow" href="/scope/reference/pipelines/memflow">
  Memory bank for long-context consistency
</Card>


# LongLive
Source: https://docs.daydream.live/scope/reference/pipelines/longlive

Streaming pipeline and autoregressive video diffusion model from Nvidia

# LongLive Pipeline

[LongLive](https://nvlabs.github.io/LongLive) is a streaming pipeline and autoregressive video diffusion model from Nvidia, MIT, HKUST, HKU and THU.

The model is trained using [Self-Forcing](https://self-forcing.github.io/) on Wan2.1 1.3b with modifications to support smoother prompt switching and improved quality over longer time periods while maintaining fast generation.

### At a Glance

|                    |              |
| :----------------- | :----------- |
| **Base Model**     | Wan2.1 1.3B  |
| **Estimated VRAM** | \~20GB       |
| **Training**       | Self-Forcing |
| **LoRA Support**   | 1.3B LoRAs   |
| **VACE Support**   | Yes          |
| **T2V / V2V**      | Yes / Yes    |

***

## Examples

The following examples include timeline JSON files with the prompts used so you can try them as well.

### Panda

<video />

<CardGroup>
  <Card title="Daydream Project" icon="globe" href="https://app.daydream.live/creators/yondonfu/creations/panda?utm_source=docs&utm_medium=web&utm_campaign=docs2com">
    View on Community Hub
  </Card>

  <Card title="Timeline JSON file" icon="file-code" href="/workflows/scope/pipelines/longlive/timeline-panda.json">
    Download timeline
  </Card>
</CardGroup>

### Factory

<video />

<CardGroup>
  <Card title="Daydream Project" icon="globe" href="https://app.daydream.live/creators/yondonfu/creations/factory?utm_source=docs&utm_medium=web&utm_campaign=docs2com">
    View on Community Hub
  </Card>

  <Card title="Timeline JSON file" icon="file-code" href="/workflows/scope/pipelines/longlive/timeline-factory.json">
    Download timeline
  </Card>
</CardGroup>

***

## Resolution

The generation will be faster for smaller resolutions resulting in smoother video. The visual quality will be better at **832x480** which is the resolution that the model was trained on, but you may need a more powerful GPU in order to achieve a higher FPS (the \~20 FPS reported in the paper is on a H100).

***

## Seed

The seed parameter in the UI can be used to reproduce generations. If you like the generation for a certain seed value and sequence of prompts you can re-use that value later with those same prompts to reproduce the generation.

***

## Prompting

The [original project repo](https://github.com/NVlabs/LongLive) contains additional tips for prompting.

**Subject and Background/Setting Anchors**

The model works better if you include a subject (who/what) and background/setting (where) in each prompt. If you want continuity in the next scene then you can continue referencing the same subject and/or background/setting.

For example:

```
"A 3D animated scene. A **panda** walks along a path towards the camera in a park on a spring day."

"A 3D animated scene. A **panda** halts along a path in a park on a spring day."
```

**Cinematic Long Takes**

The model works better for scene transitions that involve long cinematic long takes and works less well with rapid shot-by-shot transitions or fast cutscenes.

**Long, Detailed Prompts**

The model works better with long, detailed prompts. A helpful technique to extend prompts is to take a base prompt and then ask a LLM chatbot (eg. ChatGPT, Claude, Gemini, etc.) to write a more detailed version.

If your base prompt is:

```
"A cartoon dog jumping and then running."
```

Then, the extended prompt could be:

```
"A cartoon dog with big expressive eyes and floppy ears suddenly leaps into the frame, tail wagging, and then sprints joyfully toward the camera. Its oversized paws pound playfully on the ground, tongue hanging out in excitement. The animation style is colorful, smooth, and bouncy, with exaggerated motion to emphasize energy and fun. The background blurs slightly with speed lines, giving a lively, comic-style effect as if the dog is about to jump right into the viewer."
```

***

## Offline Generation

A test [script](https://github.com/daydreamlive/scope/blob/main/src/scope/core/pipelines/longlive/test.py) can be used for offline generation.

If the model weights are not downloaded yet:

```bash theme={null}
# Run from scope directory
uv run download_models --pipeline longlive
```

Then:

```bash theme={null}
# Run from scope directory
uv run -m scope.core.pipelines.longlive.test
```

This will create an `output.mp4` file in the `longlive` directory.

***

## See Also

### Other Pipelines

<Card title="StreamDiffusion V2" href="/scope/reference/pipelines/streamdiffusion-v2">
  Real-time streaming from the original StreamDiffusion creators
</Card>

<Card title="Krea Realtime" href="/scope/reference/pipelines/krea-realtime">
  14B model for highest quality generation
</Card>

<Card title="RewardForcing" href="/scope/reference/pipelines/reward-forcing">
  Reward-matched training for improved output quality
</Card>

<Card title="MemFlow" href="/scope/reference/pipelines/memflow">
  Memory bank for long-context consistency
</Card>


# MemFlow
Source: https://docs.daydream.live/scope/reference/pipelines/memflow

Streaming pipeline and autoregressive video diffusion model from Kling

# MemFlow Pipeline

[MemFlow](https://sihuiji.github.io/MemFlow.github.io/) is a streaming pipeline and autoregressive video diffusion model from Kling.

The model is trained using [Self-Forcing](https://self-forcing.github.io/) on Wan2.1 1.3b based on the [LongLive](https://nvlabs.github.io/LongLive/) training and inference pipeline with the additions of a memory bank to improve long context consistency and using sparse memory activation to maintain generation efficiency.

### At a Glance

|                    |              |
| :----------------- | :----------- |
| **Base Model**     | Wan2.1 1.3B  |
| **Estimated VRAM** | \~20GB       |
| **Training**       | Self-Forcing |
| **LoRA Support**   | 1.3B LoRAs   |
| **VACE Support**   | Yes          |
| **T2V / V2V**      | Yes / Yes    |
| **Memory Bank**    | Yes          |

<Note>
  The sparse memory activation technique is discussed in the paper, but not implemented right now as mentioned in [this issue](https://github.com/KlingTeam/MemFlow/issues/1).
</Note>

***

## Examples

The following examples include timeline JSON files with the prompts used so you can try them as well.

### Beekeeper

<video />

<Card title="Timeline JSON file" icon="file-code" href="/workflows/scope/pipelines/memflow/timeline-memflow-beekeeper.json">
  Download timeline
</Card>

### Detective

<video />

<Card title="Timeline JSON file" icon="file-code" href="/workflows/scope/pipelines/memflow/timeline-memflow-detective.json">
  Download timeline
</Card>

***

## Resolution

The generation will be faster for smaller resolutions resulting in smoother video. The visual quality will be better at **832x480** which is the resolution that the model was trained on, but you may need a more powerful GPU in order to achieve a higher FPS.

***

## Seed

The seed parameter in the UI can be used to reproduce generations. If you like the generation for a certain seed value and sequence of prompts you can re-use that value later with those same prompts to reproduce the generation.

***

## Prompting

**Subject and Background/Setting Anchors**

The model works better if you include a subject (who/what) and background/setting (where) in each prompt. If you want continuity in the next scene then you can continue referencing the same subject and/or background/setting.

For example:

```
"A 3D animated scene. A **panda** walks along a path towards the camera in a park on a spring day."

"A 3D animated scene. A **panda** halts along a path in a park on a spring day."
```

**Cinematic Long Takes**

The model works better for scene transitions that involve long cinematic long takes and works less well with rapid shot-by-shot transitions or fast cutscenes.

**Long, Detailed Prompts**

The model works better with long, detailed prompts. A helpful technique to extend prompts is to take a base prompt and then ask a LLM chatbot (eg. ChatGPT, Claude, Gemini, etc.) to write a more detailed version.

If your base prompt is:

```
"A cartoon dog jumping and then running."
```

Then, the extended prompt could be:

```
"A cartoon dog with big expressive eyes and floppy ears suddenly leaps into the frame, tail wagging, and then sprints joyfully toward the camera. Its oversized paws pound playfully on the ground, tongue hanging out in excitement. The animation style is colorful, smooth, and bouncy, with exaggerated motion to emphasize energy and fun. The background blurs slightly with speed lines, giving a lively, comic-style effect as if the dog is about to jump right into the viewer."
```

***

## Offline Generation

A test [script](https://github.com/daydreamlive/scope/blob/main/src/scope/core/pipelines/memflow/test.py) can be used for offline generation.

If the model weights are not downloaded yet:

```bash theme={null}
# Run from scope directory
uv run download_models --pipeline memflow
```

Then:

```bash theme={null}
# Run from scope directory
uv run -m scope.core.pipelines.memflow.test
```

This will create an `output.mp4` file in the `memflow` directory.

***

## See Also

### Other Pipelines

<Card title="StreamDiffusion V2" href="/scope/reference/pipelines/streamdiffusion-v2">
  Real-time streaming from the original StreamDiffusion creators
</Card>

<Card title="LongLive" href="/scope/reference/pipelines/longlive">
  Smooth prompt transitions and extended generation from Nvidia
</Card>

<Card title="Krea Realtime" href="/scope/reference/pipelines/krea-realtime">
  14B model for highest quality generation
</Card>

<Card title="RewardForcing" href="/scope/reference/pipelines/reward-forcing">
  Reward-matched training for improved output quality
</Card>


# RewardForcing
Source: https://docs.daydream.live/scope/reference/pipelines/reward-forcing

Streaming pipeline and autoregressive video diffusion model trained with Rewarded Distribution Matching

# RewardForcing Pipeline

[RewardForcing](https://reward-forcing.github.io/) is a streaming pipeline and autoregressive video diffusion model from ZJU, Ant Group, SIAS-ZJU, HUST and SJTU.

The model is trained with Rewarded Distribution Matching Distillation using Wan2.1 1.3b as the base model.

### At a Glance

|                    |                                |
| :----------------- | :----------------------------- |
| **Base Model**     | Wan2.1 1.3B                    |
| **Estimated VRAM** | \~20GB                         |
| **Training**       | Rewarded Distribution Matching |
| **LoRA Support**   | 1.3B LoRAs                     |
| **VACE Support**   | Yes                            |
| **T2V / V2V**      | Yes / Yes                      |

***

## Examples

The following examples include timeline JSON files with the prompts used so you can try them as well.

### Ink Drop

<video />

<Card title="Timeline JSON file" icon="file-code" href="/workflows/scope/pipelines/reward-forcing/timeline-reward-forcing-ink.json">
  Download timeline
</Card>

### Melting Ice

<video />

<Card title="Timeline JSON file" icon="file-code" href="/workflows/scope/pipelines/reward-forcing/timeline-reward-forcing-melt.json">
  Download timeline
</Card>

***

## Resolution

The generation will be faster for smaller resolutions resulting in smoother video. The visual quality will be better at **832x480** which is the resolution that the model was trained on, but you may need a more powerful GPU in order to achieve a higher FPS.

***

## Seed

The seed parameter in the UI can be used to reproduce generations. If you like the generation for a certain seed value and sequence of prompts you can re-use that value later with those same prompts to reproduce the generation.

***

## Prompting

**Subject and Background/Setting Anchors**

The model works better if you include a subject (who/what) and background/setting (where) in each prompt. If you want continuity in the next scene then you can continue referencing the same subject and/or background/setting.

For example:

```
"A 3D animated scene. A **panda** walks along a path towards the camera in a park on a spring day."

"A 3D animated scene. A **panda** halts along a path in a park on a spring day."
```

**Cinematic Long Takes**

The model works better for scene transitions that involve long cinematic long takes and works less well with rapid shot-by-shot transitions or fast cutscenes.

**Long, Detailed Prompts**

The model works better with long, detailed prompts. A helpful technique to extend prompts is to take a base prompt and then ask a LLM chatbot (eg. ChatGPT, Claude, Gemini, etc.) to write a more detailed version.

If your base prompt is:

```
"A cartoon dog jumping and then running."
```

Then, the extended prompt could be:

```
"A cartoon dog with big expressive eyes and floppy ears suddenly leaps into the frame, tail wagging, and then sprints joyfully toward the camera. Its oversized paws pound playfully on the ground, tongue hanging out in excitement. The animation style is colorful, smooth, and bouncy, with exaggerated motion to emphasize energy and fun. The background blurs slightly with speed lines, giving a lively, comic-style effect as if the dog is about to jump right into the viewer."
```

***

## Offline Generation

A test [script](https://github.com/daydreamlive/scope/blob/main/src/scope/core/pipelines/reward_forcing/test.py) can be used for offline generation.

If the model weights are not downloaded yet:

```bash theme={null}
# Run from scope directory
uv run download_models --pipeline reward_forcing
```

Then:

```bash theme={null}
# Run from scope directory
uv run -m scope.core.pipelines.reward_forcing.test
```

This will create an `output.mp4` file in the `reward_forcing` directory.

***

## See Also

### Other Pipelines

<Card title="StreamDiffusion V2" href="/scope/reference/pipelines/streamdiffusion-v2">
  Real-time streaming from the original StreamDiffusion creators
</Card>

<Card title="LongLive" href="/scope/reference/pipelines/longlive">
  Smooth prompt transitions and extended generation from Nvidia
</Card>

<Card title="Krea Realtime" href="/scope/reference/pipelines/krea-realtime">
  14B model for highest quality generation
</Card>

<Card title="MemFlow" href="/scope/reference/pipelines/memflow">
  Memory bank for long-context consistency
</Card>


# StreamDiffusion V2
Source: https://docs.daydream.live/scope/reference/pipelines/streamdiffusion-v2

Streaming inference pipeline and autoregressive video diffusion model

# StreamDiffusion V2 Pipeline

[StreamDiffusionV2](https://streamdiffusionv2.github.io/) is a streaming inference pipeline and autoregressive video diffusion model from the creators of the original [StreamDiffusion](https://github.com/cumulo-autumn/StreamDiffusion) project.

The model is trained using [Self-Forcing](https://self-forcing.github.io/) on Wan2.1 1.3b with modifications to support streaming.

### At a Glance

|                    |              |
| :----------------- | :----------- |
| **Base Model**     | Wan2.1 1.3B  |
| **Estimated VRAM** | \~20GB       |
| **Training**       | Self-Forcing |
| **LoRA Support**   | 1.3B LoRAs   |
| **VACE Support**   | Yes          |
| **T2V / V2V**      | Yes / Yes    |

***

## Examples

The following examples include timeline JSON files with the prompts used so you can try them as well.

### Evolution

<video />

<Card title="Timeline JSON file" icon="file-code" href="/workflows/scope/pipelines/streamdiffusion-v2/timeline-evolution.json">
  Download the timeline to try this example
</Card>

### Feline

<video />

<Card title="Timeline JSON file" icon="file-code" href="/workflows/scope/pipelines/streamdiffusion-v2/timeline-feline.json">
  Download the timeline to try this example
</Card>

### Prey

<video />

<Card title="Timeline JSON file" icon="file-code" href="/workflows/scope/pipelines/streamdiffusion-v2/timeline-prey.json">
  Download the timeline to try this example
</Card>

***

## Resolution

The generation will be faster for smaller resolutions resulting in smoother video. Scope currently will use the input video's resolution as the output resolution. The visual quality will be better at **832x480** which is the resolution that the model was trained on, but you may need a more powerful GPU in order to achieve a higher FPS.

***

## Seed

The seed parameter in the UI can be used to reproduce generations. If you like the generation for a certain seed value, input video and sequence of prompts you can re-use that value later with those same input video and prompts to reproduce the generation.

***

## Prompting

The model works better with long, detailed prompts. A helpful technique to extend prompts is to take a base prompt and then ask a LLM chatbot (eg. ChatGPT, Claude, Gemini, etc.) to write a more detailed version.

If your base prompt is:

```
"A cartoon dog jumping and then running."
```

Then, the extended prompt could be:

```
"A cartoon dog with big expressive eyes and floppy ears suddenly leaps into the frame, tail wagging, and then sprints joyfully toward the camera. Its oversized paws pound playfully on the ground, tongue hanging out in excitement. The animation style is colorful, smooth, and bouncy, with exaggerated motion to emphasize energy and fun. The background blurs slightly with speed lines, giving a lively, comic-style effect as if the dog is about to jump right into the viewer."
```

***

## Offline Generation

A test [script](https://github.com/daydreamlive/scope/blob/main/src/scope/core/pipelines/streamdiffusionv2/test.py) can be used for offline generation.

If the model weights are not downloaded yet:

```bash theme={null}
# Run from scope directory
uv run download_models --pipeline streamdiffusionv2
```

Then:

```bash theme={null}
# Run from scope directory
uv run -m scope.core.pipelines.streamdiffusionv2.test
```

This will create an `output.mp4` file in the `streamdiffusionv2` directory.

***

## See Also

### Other Pipelines

<Card title="LongLive" href="/scope/reference/pipelines/longlive">
  Smooth prompt transitions and extended generation from Nvidia
</Card>

<Card title="Krea Realtime" href="/scope/reference/pipelines/krea-realtime">
  14B model for highest quality generation
</Card>

<Card title="RewardForcing" href="/scope/reference/pipelines/reward-forcing">
  Reward-matched training for improved output quality
</Card>

<Card title="MemFlow" href="/scope/reference/pipelines/memflow">
  Memory bank for long-context consistency
</Card>


# System Requirements
Source: https://docs.daydream.live/scope/reference/system-requirements

Hardware and software requirements for running Daydream Scope

# Scope System Requirements

Comprehensive specifications for running Daydream Scope locally or in the cloud. Use this reference to verify compatibility, plan hardware purchases, or troubleshoot performance issues.

***

## GPU Requirements by Pipeline

### StreamDiffusion V2 Pipeline

* **Minimum VRAM:** 24GB
* **CUDA Version:** 12.8+

StreamDiffusion V2 provides real-time video generation with streaming capabilities.

### LongLive Pipeline

* **Minimum VRAM:** 24GB
* **CUDA Version:** 12.8+

LongLive enables extended generation capabilities for longer video sequences with consistent quality.

### Krea Realtime Pipeline

* **Minimum VRAM:** 32GB (40GB recommended for higher resolutions)
* **CUDA Version:** 12.8+

Krea Realtime requires significantly more VRAM than other pipelines. You can run it on 32GB GPUs with fp8 quantization at lower resolutions, but 40GB+ is recommended for optimal performance at higher resolutions.

### RewardForcing Pipeline

* **Minimum VRAM:** 24GB
* **CUDA Version:** 12.8+

RewardForcing uses Rewarded Distribution Matching for improved output quality on the Wan2.1 1.3B model.

### MemFlow Pipeline

* **Minimum VRAM:** 24GB
* **CUDA Version:** 12.8+

MemFlow uses a memory bank mechanism for improved long-context consistency on the Wan2.1 1.3B model.

<Note>
  Better GPUs generally yield better results with higher FPS and lower latency.
</Note>

***

## Software Dependencies

### Required Software

* **CUDA Drivers:** Version 12.8 or higher
* **UV:** Python package manager (used to run the Scope server)
* **Node.js and npm:** Required for building the frontend
* **Operating System:** Linux or Windows

### Verifying CUDA Installation

To check your CUDA version and GPU status:

```bash theme={null}
nvidia-smi
```

The output should show your GPU model and a CUDA version of at least 12.8.

***

## Cloud Deployment Options

Don't have access to a local GPU? Scope can run on cloud GPU services.

### RunPod

RunPod is a cloud GPU service that supports Scope deployment. We provide a pre-configured template for easy setup.

**Minimum GPU Requirements:**

* ≥24GB VRAM for StreamDiffusion V2, LongLive, RewardForcing, and MemFlow
* ≥32GB VRAM for Krea Realtime (≥40GB recommended)
* CUDA 12.8+ support

**Recommended Instance Types:**

* NVIDIA RTX 4090 (24GB) - Good for StreamDiffusion V2 and LongLive
* NVIDIA RTX 5090 (32GB) - Recommended for Krea Realtime

For detailed RunPod setup instructions, see the [Quick Start guide](/scope/getting-started/quickstart#cloud-deployment-runpod).

***

## Need Help?

<CardGroup>
  <Card title="Quick Start Guide" icon="rocket" href="/scope/getting-started/quickstart">
    Follow our installation guide for step-by-step setup instructions
  </Card>

  <Card title="Join Discord" icon="discord" href="https://discord.com/invite/5sZu8xmn6U">
    Get help from the community in our #scope channel
  </Card>
</CardGroup>


# VAE Types
Source: https://docs.daydream.live/scope/reference/vae

Different VAE types offer tradeoffs between quality, speed, and memory usage

# Configuring VAE Types

VAE (Variational Autoencoder) is used for encoding pixels to latents and decoding latents back to pixels. Different VAE types offer tradeoffs between quality, speed, and memory usage.

The comparison and descriptions below are sourced from the lightx2v [docs](https://huggingface.co/lightx2v/Autoencoders) which contain additional technical details about each implementation.

***

## Comparison

| Type       | Quality | Speed  | Description                   |
| ---------- | ------- | ------ | ----------------------------- |
| `wan`      | Best    | Slow   | Full WanVAE (default)         |
| `lightvae` | High    | Medium | 75% pruned WanVAE             |
| `tae`      | Average | Fast   | Tiny AutoEncoder              |
| `lighttae` | High    | Fast   | TAE with WanVAE normalization |

***

## WanVAE (`wan`)

The full WanVAE is the default and provides the best quality output. It uses the complete model architecture without any pruning, resulting in the highest fidelity encoding and decoding at the cost of speed.

***

## LightVAE (`lightvae`)

LightVAE is a 75% pruned version of WanVAE. It removes 75% of the model's channels, significantly reducing computation while maintaining high quality output. This takes a middle ground between quality and speed.

***

## TAE (`tae`)

TAE (Tiny AutoEncoder) is a completely different architecture from WanVAE. It's a lightweight model specifically designed for quick encoding/decoding previews. Key architectural differences include:

* Uses MemBlock for temporal memory
* Has TPool/TGrow blocks for temporal downsampling/upsampling
* Much simpler architecture with 64 channels throughout

TAE trades quality for speed.

***

## LightTAE (`lighttae`)

LightTAE combines the fast TAE architecture with WanVAE's normalization parameters. This provides a balance of high quality and fast performance.

***

## See Also

<CardGroup>
  <Card title="Load Pipeline" icon="play" href="/scope/reference/api/load-pipeline">
    Configure VAE type as a load parameter
  </Card>

  <Card title="Pipelines Overview" icon="layer-group" href="/scope/reference/pipelines">
    Compare pipelines and their capabilities
  </Card>
</CardGroup>

<Card title="lightx2v Autoencoders" icon="external-link" href="https://huggingface.co/lightx2v/Autoencoders">
  Technical documentation for VAE implementations
</Card>


# Build a Video Effects Plugin
Source: https://docs.daydream.live/scope/tutorials/build-video-effects-plugin

Create a GPU-accelerated visual effects plugin for Scope with live UI controls

# Build a Real-Time Video Effects Plugin

In this tutorial you will create **scope-vfx** - a Scope plugin that applies GPU-accelerated visual effects to any video input. You will ship two effects (chromatic aberration and VHS/retro CRT) and set the project up so adding more effects later is as simple as dropping in a new file.

By the end you will have a working plugin installed in Scope with live UI controls, and you will understand the full plugin development workflow.

Watch the full 13-minute build walkthrough:

<iframe title="Build a Video Effects Plugin for Daydream Scope" />

<Card title="scope-vfx source code" icon="github" href="https://github.com/viborc/scope-vfx">
  The complete plugin built in this tutorial
</Card>

***

## What is a Scope plugin?

[Daydream Scope](https://github.com/daydreamlive/scope) is an open-source tool for running real-time interactive AI video pipelines. It supports models like StreamDiffusion V2, LongLive, and Krea Realtime - and its plugin system lets anyone add new pipelines without touching the core codebase.

A plugin is a Python package that registers one or more **pipelines**. A pipeline is a class that:

1. Declares a configuration schema (what parameters appear in the UI)
2. Accepts video frames and/or text prompts as input
3. Returns processed video frames as output

That is the whole contract. Scope handles discovery, installation, UI rendering, and streaming. You write the frame processing logic.

***

## Prerequisites

* Python 3.12 or newer
* [uv](https://docs.astral.sh/uv/) package manager
* Daydream Scope installed and running (desktop app or CLI)
* Basic Python and PyTorch knowledge

***

## Scaffold the project

<Steps>
  <Step title="Create the directory structure">
    Create a new directory with the following layout:

    ```
    scope-vfx/
    ├── pyproject.toml
    └── src/
        └── scope_vfx/
            ├── __init__.py
            ├── schema.py
            ├── pipeline.py
            └── effects/
                ├── __init__.py
                ├── chromatic.py
                └── vhs.py
    ```

    The plugin entry point lives in `__init__.py`, the configuration schema in `schema.py`, the pipeline logic in `pipeline.py`, and each effect gets its own file under `effects/`.
  </Step>

  <Step title="Configure pyproject.toml">
    ```toml theme={null}
    [project]
    name = "scope-vfx"
    version = "0.1.0"
    description = "GPU-accelerated visual effects pack for Daydream Scope"
    requires-python = ">=3.12"

    [project.entry-points."scope"]
    scope_vfx = "scope_vfx"

    [build-system]
    requires = ["hatchling"]
    build-backend = "hatchling.build"

    [tool.hatch.build.targets.wheel]
    packages = ["src/scope_vfx"]
    ```

    The key line is `[project.entry-points."scope"]`. This is how Scope discovers your plugin - it scans all installed packages for entry points in the `"scope"` group and loads whatever module they point to. No configuration files, no manual registration - just a standard Python entry point.

    <Note>
      There are no dependencies listed. Scope's environment already includes PyTorch, Pydantic, and everything else this plugin needs. Only add `[project.dependencies]` if your plugin uses third-party packages that Scope does not already provide.
    </Note>
  </Step>
</Steps>

***

## Register the plugin hook

Create `src/scope_vfx/__init__.py`:

```python theme={null}
from scope.core.plugins.hookspecs import hookimpl


@hookimpl
def register_pipelines(register):
    from .pipeline import VFXPipeline

    register(VFXPipeline)
```

This is the entire plugin registration. The `@hookimpl` decorator (from [pluggy](https://pluggy.readthedocs.io/)) marks this function as a hook implementation. When Scope loads your plugin, it calls `register_pipelines()` and passes a `register` callback. You call it once per pipeline class you want to make available.

<Tip>
  The lazy import (`from .pipeline import VFXPipeline` inside the function body) is intentional - it delays importing PyTorch and your heavy pipeline code until Scope actually needs it.
</Tip>

***

## Define the configuration schema

Create `src/scope_vfx/schema.py`:

```python theme={null}
from pydantic import Field

from scope.core.pipelines.base_schema import BasePipelineConfig, ModeDefaults, ui_field_config


class VFXConfig(BasePipelineConfig):
    """Configuration for the VFX Pack pipeline."""

    pipeline_id = "vfx-pack"
    pipeline_name = "VFX Pack"
    pipeline_description = (
        "GPU-accelerated visual effects: chromatic aberration, VHS/retro CRT, and more"
    )

    supports_prompts = False

    modes = {"video": ModeDefaults(default=True)}

    # --- Chromatic Aberration ---

    chromatic_enabled: bool = Field(
        default=True,
        description="Enable chromatic aberration (RGB channel displacement)",
        json_schema_extra=ui_field_config(order=1, label="Chromatic Aberration"),
    )

    chromatic_intensity: float = Field(
        default=0.3,
        ge=0.0,
        le=1.0,
        description="Strength of the RGB channel displacement (0 = none, 1 = maximum)",
        json_schema_extra=ui_field_config(order=2, label="Intensity"),
    )

    chromatic_angle: float = Field(
        default=0.0,
        ge=0.0,
        le=360.0,
        description="Direction of the channel displacement in degrees",
        json_schema_extra=ui_field_config(order=3, label="Angle"),
    )

    # --- VHS / Retro CRT ---

    vhs_enabled: bool = Field(
        default=False,
        description="Enable VHS / retro CRT effect (scan lines, noise, tracking)",
        json_schema_extra=ui_field_config(order=10, label="VHS / Retro CRT"),
    )

    scan_line_intensity: float = Field(
        default=0.3,
        ge=0.0,
        le=1.0,
        description="Darkness of the scan lines (0 = invisible, 1 = fully black)",
        json_schema_extra=ui_field_config(order=11, label="Scan Lines"),
    )

    scan_line_count: int = Field(
        default=100,
        ge=10,
        le=500,
        description="Number of scan lines across the frame height",
        json_schema_extra=ui_field_config(order=12, label="Line Count"),
    )

    vhs_noise: float = Field(
        default=0.1,
        ge=0.0,
        le=1.0,
        description="Amount of analog noise / film grain",
        json_schema_extra=ui_field_config(order=13, label="Noise"),
    )

    tracking_distortion: float = Field(
        default=0.2,
        ge=0.0,
        le=1.0,
        description="Horizontal tracking distortion (wavy displacement)",
        json_schema_extra=ui_field_config(order=14, label="Tracking"),
    )
```

### Understanding the schema

**Pipeline metadata** - `pipeline_id`, `pipeline_name`, and `pipeline_description` are class variables that tell Scope how to display your pipeline in the UI.

**`modes = {"video": ModeDefaults(default=True)}`** - This declares that the pipeline requires video input (camera feed or video file). It will not appear in text-to-video mode. For a text-only pipeline (one that generates frames from nothing), you would use `"text"` instead.

**`supports_prompts = False`** - These effects do not use text prompts, so the prompt input is hidden.

**Each field becomes a UI control.** Scope's frontend reads the JSON Schema that Pydantic generates from this class and automatically renders the right widget:

| Field type             | UI widget     |
| :--------------------- | :------------ |
| `bool`                 | Toggle switch |
| `float` with `ge`/`le` | Slider        |
| `int` with `ge`/`le`   | Slider        |
| `enum`                 | Dropdown      |

The `ui_field_config()` helper sets display order, labels, and other UI hints. The `order` values control the vertical position in the settings panel - we use 1-3 for chromatic params and 10-14 for VHS params to keep them grouped with room for future effects in between.

<Note>
  All parameters here are **runtime parameters** (the default). They are editable while the pipeline is streaming - move a slider and see the result instantly. If you need a parameter that requires a restart (like model selection), add `is_load_param=True` to its `ui_field_config()`.
</Note>

***

## Build the first effect - Chromatic Aberration

Create `src/scope_vfx/effects/chromatic.py`:

```python theme={null}
import math

import torch


def chromatic_aberration(
    frames: torch.Tensor,
    intensity: float = 0.3,
    angle: float = 0.0,
) -> torch.Tensor:
    """Displace RGB channels in opposite directions for a chromatic aberration look."""
    if intensity <= 0:
        return frames

    max_shift = int(intensity * 20)
    if max_shift == 0:
        return frames

    rad = math.radians(angle)
    dx = int(round(max_shift * math.cos(rad)))
    dy = int(round(max_shift * math.sin(rad)))

    if dx == 0 and dy == 0:
        return frames

    result = frames.clone()

    # Red channel shifts one direction
    result[..., 0] = torch.roll(frames[..., 0], shifts=(dy, dx), dims=(1, 2))
    # Blue channel shifts the opposite direction
    result[..., 2] = torch.roll(frames[..., 2], shifts=(-dy, -dx), dims=(1, 2))
    # Green channel stays centred

    return result
```

The effect takes the red and blue color channels and shifts them in opposite directions. The green channel stays put. This mimics the optical imperfection in real camera lenses where different wavelengths of light focus at slightly different positions.

`torch.roll()` does the heavy lifting - it shifts a tensor along specified dimensions, wrapping pixels that fall off one edge back onto the other. Since this operates on GPU tensors, it runs in microseconds even at high resolutions.

The `intensity` parameter maps to a 0-20 pixel displacement range, and `angle` controls the direction. At intensity 0.3 (the default), you get about 6 pixels of shift - enough to notice without being overwhelming.

***

## Build the second effect - VHS / Retro CRT

Create `src/scope_vfx/effects/vhs.py`:

```python theme={null}
import torch


def vhs_retro(
    frames: torch.Tensor,
    scan_line_intensity: float = 0.3,
    scan_line_count: int = 100,
    noise: float = 0.1,
    tracking: float = 0.2,
) -> torch.Tensor:
    """Apply a VHS / retro CRT look."""
    _T, H, W, _C = frames.shape
    result = frames.clone()

    # --- Scan lines ---
    if scan_line_intensity > 0 and scan_line_count > 0:
        rows = torch.arange(H, device=frames.device, dtype=torch.float32)
        wave = torch.sin(rows * (scan_line_count * 3.14159 / H))
        mask = 1.0 - scan_line_intensity * 0.5 * (1.0 - wave)
        result = result * mask.view(1, H, 1, 1)

    # --- Analog noise / film grain ---
    if noise > 0:
        grain = torch.randn_like(result) * (noise * 0.15)
        result = result + grain

    # --- Tracking distortion ---
    if tracking > 0:
        max_shift = tracking * 0.05
        rows_norm = torch.linspace(-1.0, 1.0, H, device=frames.device)
        offsets = max_shift * torch.sin(rows_norm * 6.2832 * 3.0)

        grid_y = torch.linspace(-1.0, 1.0, H, device=frames.device)
        grid_x = torch.linspace(-1.0, 1.0, W, device=frames.device)
        gy, gx = torch.meshgrid(grid_y, grid_x, indexing="ij")

        gx = gx + offsets.view(H, 1)

        grid = torch.stack([gx, gy], dim=-1).unsqueeze(0).expand(result.shape[0], -1, -1, -1)

        result_nchw = result.permute(0, 3, 1, 2)
        result_nchw = torch.nn.functional.grid_sample(
            result_nchw, grid, mode="bilinear", padding_mode="border", align_corners=True
        )
        result = result_nchw.permute(0, 2, 3, 1)

    return result.clamp(0, 1)
```

This effect combines three sub-effects that together create the unmistakable VHS aesthetic:

**Scan lines** use a sine wave across the frame height to create alternating dark bands, just like a CRT monitor. The `scan_line_count` parameter controls how many lines you see, and `scan_line_intensity` controls how dark they are.

**Analog noise** adds random Gaussian noise to simulate the grain you would see on a VHS tape. The multiplier is kept conservative (`noise * 0.15`) so even at maximum the image is not obliterated.

**Tracking distortion** is the most visually interesting part. It shifts each row of pixels horizontally by a different amount, following a sine curve. This creates the classic "wobbly VHS" look where the image drifts sideways. We use `torch.nn.functional.grid_sample()` instead of a per-row loop - this is the GPU-friendly way to apply spatially-varying displacements. It runs a single kernel on the GPU regardless of image resolution.

***

## Wire it all together

Create `src/scope_vfx/pipeline.py`:

```python theme={null}
from typing import TYPE_CHECKING

import torch

from scope.core.pipelines.interface import Pipeline, Requirements

from .effects import chromatic_aberration, vhs_retro
from .schema import VFXConfig

if TYPE_CHECKING:
    from scope.core.pipelines.base_schema import BasePipelineConfig


class VFXPipeline(Pipeline):
    """GPU-accelerated visual effects pipeline."""

    @classmethod
    def get_config_class(cls) -> type["BasePipelineConfig"]:
        return VFXConfig

    def __init__(self, device: torch.device | None = None, **kwargs):
        self.device = (
            device
            if device is not None
            else torch.device("cuda" if torch.cuda.is_available() else "cpu")
        )

    def prepare(self, **kwargs) -> Requirements:
        """We need exactly one input frame per call."""
        return Requirements(input_size=1)

    def __call__(self, **kwargs) -> dict:
        """Apply enabled effects to input video frames."""
        video = kwargs.get("video")
        if video is None:
            raise ValueError("VFXPipeline requires video input")

        # Stack input frames -> (T, H, W, C) and normalise to [0, 1]
        frames = torch.stack([frame.squeeze(0) for frame in video], dim=0)
        frames = frames.to(device=self.device, dtype=torch.float32) / 255.0

        # --- Effect chain ---

        if kwargs.get("chromatic_enabled", True):
            frames = chromatic_aberration(
                frames,
                intensity=kwargs.get("chromatic_intensity", 0.3),
                angle=kwargs.get("chromatic_angle", 0.0),
            )

        if kwargs.get("vhs_enabled", False):
            frames = vhs_retro(
                frames,
                scan_line_intensity=kwargs.get("scan_line_intensity", 0.3),
                scan_line_count=kwargs.get("scan_line_count", 100),
                noise=kwargs.get("vhs_noise", 0.1),
                tracking=kwargs.get("tracking_distortion", 0.2),
            )

        return {"video": frames.clamp(0, 1)}
```

### Understanding the pipeline class

**`get_config_class()`** tells Scope which schema to use for this pipeline.

**`__init__()`** receives load-time parameters. We only need the device. The `**kwargs` catch-all is important - Scope may pass additional parameters that we do not use.

**`prepare()`** tells Scope's frame processor how many input frames to buffer before calling `__call__()`. We need exactly 1 frame since our effects are per-frame (no temporal dependencies).

**`__call__()`** is where the action happens. It extracts the video frames from `kwargs` (a list of tensors, each `(1, H, W, C)` in `[0, 255]` range), stacks and normalises them to `[0, 1]`, runs each enabled effect in sequence, and returns the result in the required `[0, 1]` THWC format.

<Warning>
  Every runtime parameter must be read from `kwargs` in `__call__()`, not stored in `__init__()`. Scope passes the current slider values on every frame. If you read them in `__init__()`, you would always get the defaults.
</Warning>

Finally, create `effects/__init__.py` to re-export the effect functions for clean imports:

```python theme={null}
from .chromatic import chromatic_aberration
from .vhs import vhs_retro

__all__ = ["chromatic_aberration", "vhs_retro"]
```

***

## Install and test

<Steps>
  <Step title="Open the Plugins panel">
    In Scope, open **Settings > Plugins**.
  </Step>

  <Step title="Install the plugin">
    If you are using the desktop app, click **Browse** and select the `scope-vfx` folder. If you are running the server directly, enter the full path to the plugin directory or a Git URL:

    ```
    git+https://github.com/viborc/scope-vfx.git
    ```

    Click **Install**. Scope will install the plugin and restart the server.
  </Step>

  <Step title="Select the pipeline">
    After restart, select **VFX Pack** from the pipeline selector. Connect a camera or video source and you should see your feed with chromatic aberration applied.
  </Step>

  <Step title="Adjust the controls">
    Open the Settings panel to see your sliders. Try cranking up the intensity, changing the angle, and toggling on the VHS effect.
  </Step>
</Steps>

### Development workflow

When you are iterating on effects, the cycle is:

1. Edit the effect code
2. Click **Reload** next to the plugin in Settings
3. Scope restarts and picks up your changes

No reinstall needed. The reload triggers a full server restart which clears Python's module cache, so your latest code is always loaded.

***

## Use it as a post-processor

So far we have been running VFX Pack as a **main pipeline**, meaning it processes raw camera or video input directly. But what if you want to apply these effects on top of AI-generated video? For example, run LongLive to generate video from a prompt and then add chromatic aberration and VHS effects on top of that output.

That is what **post-processors** are for. In Scope, every pipeline sits in a chain:

```
Input -> [Preprocessor] -> [Main Pipeline] -> [Post-processor] -> Output
```

The main pipeline is usually the generative AI model. A post-processor runs after it, transforming the model's output before it reaches your screen. The pipeline code stays exactly the same. The only change is a single metadata field in the schema.

Add `UsageType` to your import and set `usage` in your config class:

```python theme={null}
from scope.core.pipelines.base_schema import BasePipelineConfig, ModeDefaults, UsageType, ui_field_config


class VFXConfig(BasePipelineConfig):
    # ... same as before ...

    usage = [UsageType.POSTPROCESSOR]
    modes = {"video": ModeDefaults(default=True)}
```

With this one line, VFX Pack moves from the main pipeline dropdown to the **post-processor slot** in the UI. You can pick any generative model as your main pipeline and VFX Pack will process its output.

Your `__call__()` method receives the exact same tensor format either way. The only difference is who produced those frames - the webcam, or the AI model.

<Note>
  As of writing, Scope's UI renders parameter sliders for the main pipeline but not yet for pre/post-processors. Your effects will still apply with whatever values are set, but you will not see the sliders when VFX Pack is in the post-processor slot. A workaround: select VFX Pack as the main pipeline first, adjust your sliders, then switch back to your generative model with VFX Pack as post-processor. The values persist.
</Note>

***

## Adding more effects

The architecture makes extending trivial. Here is how you would add a **pixelation / mosaic** effect:

<Steps>
  <Step title="Create the effect function">
    Create `src/scope_vfx/effects/pixelate.py`:

    ```python theme={null}
    import torch
    import torch.nn.functional as F


    def pixelate(frames: torch.Tensor, block_size: int = 8) -> torch.Tensor:
        """Pixelate by downscaling then upscaling with nearest-neighbour."""
        if block_size <= 1:
            return frames

        T, H, W, C = frames.shape
        small_h, small_w = max(1, H // block_size), max(1, W // block_size)

        nchw = frames.permute(0, 3, 1, 2)
        small = F.interpolate(nchw, size=(small_h, small_w), mode="area")
        big = F.interpolate(small, size=(H, W), mode="nearest")
        return big.permute(0, 2, 3, 1)
    ```
  </Step>

  <Step title="Add parameters to the schema">
    In `schema.py`, add:

    ```python theme={null}
    pixelate_enabled: bool = Field(
        default=False,
        description="Enable pixelation / mosaic effect",
        json_schema_extra=ui_field_config(order=20, label="Pixelate"),
    )

    pixelate_block_size: int = Field(
        default=8,
        ge=1,
        le=64,
        description="Size of each pixel block",
        json_schema_extra=ui_field_config(order=21, label="Block Size"),
    )
    ```
  </Step>

  <Step title="Wire it into the effect chain">
    In `pipeline.py`, add to the effect chain inside `__call__()`:

    ```python theme={null}
    from .effects import chromatic_aberration, vhs_retro, pixelate

    # ... inside __call__():
    if kwargs.get("pixelate_enabled", False):
        frames = pixelate(
            frames,
            block_size=kwargs.get("pixelate_block_size", 8),
        )
    ```
  </Step>

  <Step title="Re-export the function">
    In `effects/__init__.py`, add:

    ```python theme={null}
    from .pixelate import pixelate
    ```

    Reload the plugin in Scope and the new Pixelate section appears in the UI.
  </Step>
</Steps>

Same pattern every time: a standalone function, some schema fields, and a few lines in the effect chain.

***

## What's next

If this tutorial has inspired you, here are some effects you could add to your own VFX Pack using the exact same pattern:

* **Glitch blocks** - random rectangular displacement for a digital corruption look
* **Film grain** - more realistic than simple noise, with luminance-dependent grain
* **Vignette** - darken the edges for a cinematic frame
* **Color grading** - lift/gamma/gain per channel for full color control
* **Kaleidoscope** - radial symmetry for trippy visuals
* **Edge glow** - Sobel edge detection with additive glow

Each one follows the same pattern: a standalone function in `effects/`, some schema fields, and a few lines in the effect chain. The plugin grows but the architecture stays simple.

### AI-assisted plugin development

We have prepared a set of Claude Code skills and detailed instructions that let you scaffold an entire Scope plugin through an interactive AI-assisted workflow. A dedicated video tutorial showcasing this approach is coming soon.

<Card title="scope-vfx source code" icon="github" href="https://github.com/viborc/scope-vfx">
  Browse the complete plugin source code, including the Claude Code skill in `.claude/skills/`
</Card>

***

## See Also

<CardGroup>
  <Card title="Using Plugins" icon="plug" href="/scope/guides/plugins">
    Install, manage, update, and uninstall plugins in Scope
  </Card>

  <Card title="Developing Plugins" icon="code" href="/scope/guides/plugin-development">
    Full reference for plugin project setup, schemas, and pipeline types
  </Card>

  <Card title="Plugin Architecture" icon="sitemap" href="/scope/reference/architecture/plugins">
    Technical deep-dive into how the plugin system works under the hood
  </Card>

  <Card title="Pipeline Architecture" icon="layer-group" href="/scope/reference/architecture/pipelines">
    Configuration schemas, artifacts, UI rendering, and the pipeline lifecycle
  </Card>
</CardGroup>


# Tutorials
Source: https://docs.daydream.live/scope/tutorials/index

Step-by-step guides to learn Scope from the ground up

# Scope Tutorials

Learn how to use Daydream Scope through hands-on, step-by-step tutorials. These learning-oriented guides will help you understand the fundamentals and build confidence working with real-time AI video generation.

***

## Extend Scope with Plugins

<CardGroup>
  <Card title="Build a Video Effects Plugin" icon="wand-magic-sparkles" href="/scope/tutorials/build-video-effects-plugin">
    Create a GPU-accelerated visual effects plugin with chromatic aberration and VHS effects - includes a full video walkthrough
  </Card>

  <Card title="Vibe Code a Scope Plugin" icon="microchip-ai" href="/scope/tutorials/vibe-code-a-scope-plugin">
    Build a complete plugin using AI-assisted development with Claude Code - two approaches for vibe coding with Context7 and the Scope Plugin Skill
  </Card>
</CardGroup>

***

## More Tutorials Coming Soon

We are actively developing comprehensive tutorials covering:

* **Working with Different Pipelines** - Deep dives into StreamDiffusion V2, LongLive, and Krea Realtime
* **Timeline Editor Mastery** - Creating complex, multi-prompt generations with the timeline editor
* **Camera Integration** - Building real-time interactive experiences with camera inputs
* **Custom Model Parameters** - Understanding and optimizing generation settings
* **Creative Workflows** - Practical examples for different use cases

***

## Get Help

<CardGroup>
  <Card title="Join Discord" icon="discord" href="https://discord.com/invite/5sZu8xmn6U">
    Ask questions and learn from the community in our #scope channel
  </Card>

  <Card title="GitHub Discussions" icon="github" href="https://github.com/daydreamlive/scope/discussions">
    Share your projects and get feedback from other Scope users
  </Card>
</CardGroup>


# Vibe Code a Scope Plugin
Source: https://docs.daydream.live/scope/tutorials/vibe-code-a-scope-plugin

Build a complete Scope plugin using AI-assisted development with Claude Code

# Vibe Code a Scope Plugin

In the [Build a Video Effects Plugin](/scope/tutorials/build-video-effects-plugin) tutorial you learned how every Scope plugin is put together - the project structure, schema, pipeline class, and effect functions. Now you are going to build one without writing most of the code yourself.

This tutorial walks you through two ways to **vibe code** a complete Scope plugin using [Claude Code](https://docs.anthropic.com/en/docs/claude-code/overview). You will describe what you want in plain English and let Claude handle the scaffolding, schemas, and GPU effect code. By the end you will have a working **Film Noir** plugin installed in Scope - high-contrast black and white with vignette and grain.

Watch the full walkthrough:

<iframe title="Vibe Code a Scope Plugin" />

***

## Prerequisites

* [Claude Code](https://docs.anthropic.com/en/docs/claude-code/overview) installed and authenticated
* Daydream Scope installed and running ([desktop app or local install](/scope/getting-started/quickstart))
* Basic familiarity with the terminal
* Completed (or at least read through) the [Build a Video Effects Plugin](/scope/tutorials/build-video-effects-plugin) tutorial

You do **not** need to know Python or PyTorch. That is the whole point.

If you want to understand the plugin system in more depth before diving in, check out the [Developing Plugins](/scope/guides/plugin-development) guide and the [Plugin Architecture](/scope/reference/architecture/plugins) reference.

***

## Choose your approach

Both approaches produce the same result: a fully functional Scope plugin with live UI controls. Pick the one that fits your style.

<CardGroup>
  <Card title="Context7 MCP" icon="globe">
    **Explore and build.** Claude fetches the latest Scope docs on-the-fly via an MCP server and generates your plugin from the official patterns. Most flexible - you can go off-script and explore any part of the API.
  </Card>

  <Card title="Plugin Skill" icon="terminal">
    **Answer questions and ship.** A purpose-built Claude Code skill walks you through a structured flow - concept, specs, file generation, testing. Faster and more predictable, with the full plugin reference baked in.
  </Card>
</CardGroup>

***

## Build your plugin

<Tabs>
  <Tab title="Context7 MCP" icon="globe">
    This approach uses [Context7](https://context7.com), an MCP server that gives Claude access to up-to-date library documentation. You point Claude at the Scope docs, describe the effect you want, and it generates everything from scratch using the official plugin patterns.

    ### Why this approach?

    Context7 pulls the latest Scope documentation into Claude's context on-the-fly. This means Claude always works from the current plugin API - even if it has changed since Claude's training data was last updated. It is the most flexible approach because you are not limited to a predefined workflow. You can ask Claude to build any kind of plugin, explore the docs interactively, and iterate on the design in conversation.

    ### Setup

    <Steps>
      <Step title="Install the Context7 MCP server">
        The fastest way to add Context7 to Claude Code is with a single CLI command:

        ```bash theme={null}
        claude mcp add context7 -- npx -y @upstash/context7-mcp@latest
        ```

        For higher rate limits, grab a free API key from [context7.com/dashboard](https://context7.com/dashboard) and pass it in:

        ```bash theme={null}
        claude mcp add context7 -- npx -y @upstash/context7-mcp@latest --api-key YOUR_API_KEY
        ```

        Alternatively, you can add it manually to your Claude Code settings file (`~/.claude.json` or your project's `.claude/settings.json`):

        ```json theme={null}
        {
          "mcpServers": {
            "context7": {
              "command": "npx",
              "args": ["-y", "@upstash/context7-mcp@latest"]
            }
          }
        }
        ```

        Restart Claude Code. You should see Context7 listed when Claude starts up.

        <Note>
          Context7 is built by [Upstash](https://github.com/upstash/context7). If you run into `ERR_MODULE_NOT_FOUND` issues with `npx`, try using `bunx` instead: `claude mcp add context7 -- bunx @upstash/context7-mcp@latest`
        </Note>
      </Step>

      <Step title="Verify it works">
        Ask Claude to look up the Scope plugin docs:

        ```
        Can you use Context7 to look up how Daydream Scope plugins work?
        ```

        Claude will call `resolve-library-id` to find Scope, then `query-docs` to pull the plugin development documentation. If you see schema examples and pipeline code in the response, you are good to go.
      </Step>
    </Steps>

    ### Build the plugin

    Start a new Claude Code session in an empty directory and walk through this conversation:

    <Steps>
      <Step title="Ask Claude to research the plugin system">
        ```
        I want to build a Scope plugin that creates a film noir effect - classic black
        and white, high contrast, dark vignette around the edges, and subtle film grain.
        Can you look up how Scope plugins work and tell me what we need to build?
        ```

        Claude will query Context7 for the Scope plugin documentation and come back with the full picture: project structure, `pyproject.toml` entry points, schema patterns, pipeline class interface, and effect function conventions. It is essentially reading the same docs you read in the previous tutorial, but doing it in real time.
      </Step>

      <Step title="Generate the entire plugin">
        Once Claude has the context, tell it to build:

        ```
        Go ahead and build the complete plugin for me. Include all the files -
        pyproject.toml, schema with UI controls for each effect parameter, pipeline
        class, and the individual effect functions. Make the effects GPU-accelerated
        with PyTorch.
        ```

        Claude will generate the full project structure:

        ```
        scope-film-noir/
        ├── pyproject.toml
        └── src/
            └── scope_film_noir/
                ├── __init__.py
                ├── schema.py
                ├── pipeline.py
                └── effects/
                    ├── __init__.py
                    ├── desaturate.py
                    ├── contrast.py
                    ├── vignette.py
                    └── grain.py
        ```

        Each file will follow the exact patterns from the Scope documentation - Pydantic schema with `ui_field_config` for automatic UI controls, lazy imports in `__init__.py`, `torch.Tensor` operations for GPU-accelerated processing, and the `Pipeline` base class with `prepare()` and `__call__()` methods.
      </Step>

      <Step title="Review and iterate">
        This is the vibe coding part. Read through what Claude generated and ask for changes in plain English:

        ```
        Can you make the grain more like actual film grain - luminance-dependent so
        bright areas get less grain than shadows? Also bump the default contrast up
        a bit, it should feel dramatic out of the box.
        ```

        Claude will update the effect code and schema defaults. Keep iterating until the parameters feel right. You can also ask Claude to explain any part of the code:

        ```
        How does the vignette effect work? What does the radial gradient do?
        ```

        Since Claude has the Scope docs in context, its explanations will be grounded in how the framework actually works.
      </Step>

      <Step title="Install and test">
        Once you are happy with the code, install it in Scope:

        * **Desktop app**: Open **Settings > Plugins**, click **Browse**, and select the `scope-film-noir` folder
        * **Server**: Enter the path to the plugin directory and click **Install**

        Select **Film Noir** from the pipeline selector, connect a camera, and you should see your feed transformed into a moody black-and-white scene with dramatic shadows and subtle grain.
      </Step>
    </Steps>

    <Tip>
      You can ask Claude to query specific parts of the Scope docs mid-conversation. For example: "Can you look up how post-processors work in Scope? I want to chain this after a generative model." Claude will fetch the relevant documentation and adapt your plugin accordingly.
    </Tip>
  </Tab>

  <Tab title="Plugin Skill" icon="terminal">
    This approach uses a purpose-built [Claude Code skill](https://docs.anthropic.com/en/docs/claude-code/skills) that guides you through plugin creation with a structured, interactive flow. Instead of an open-ended conversation, the skill asks you specific questions about your plugin and then generates all the files in one go.

    ### Why this approach?

    The skill encodes the full Scope plugin reference and a complete working example (the VFX Pack from the previous tutorial) directly in its prompt. Claude does not need to look anything up - it already has everything it needs. This makes the process faster and more predictable. The structured question flow also ensures you do not forget anything (like setting up the `pyproject.toml` entry point or choosing between main pipeline and post-processor modes).

    If Context7 is the "explore and build" approach, the skill is the "answer questions and ship" approach.

    <Card title="Scope Plugin Skill source code" icon="github" href="https://github.com/viborc/scope-plugin-skill">
      Browse the skill source to see how it works under the hood - includes the full Scope plugin reference and a complete working example baked into the prompt
    </Card>

    ### Setup

    <Steps>
      <Step title="Install the skill">
        Clone the skill repository and symlink it into your Claude Code skills directory:

        ```bash theme={null}
        git clone https://github.com/viborc/scope-plugin-skill.git
        cd scope-plugin-skill
        mkdir -p ~/.claude/skills
        ln -s "$(pwd)" ~/.claude/skills/create-scope-plugin
        ```

        <Note>
          For project-specific installation, copy the skill folder into your project's `.claude/skills/` directory instead of symlinking to the global location.
        </Note>
      </Step>

      <Step title="Verify it works">
        Open Claude Code and type:

        ```
        /create-scope-plugin
        ```

        Claude should respond by asking about your plugin concept. If it does, you are set.
      </Step>
    </Steps>

    ### Build the plugin

    <Steps>
      <Step title="Invoke the skill with your idea">
        You can start with just the slash command, or give it a head start with a description:

        ```
        /create-scope-plugin a film noir black and white effect with vignette and grain
        ```

        The skill kicks off a **four-phase workflow**:

        1. **Concept Exploration** - Claude discusses your idea, suggests effects and parameters, and confirms the creative direction
        2. **Specification Gathering** - Structured questions about naming, pipeline settings, UI parameters, and technical details
        3. **File Generation** - Claude creates every file: `pyproject.toml`, schema, pipeline, effects, and `__init__.py`
        4. **Testing Support** - Installation instructions and validation steps
      </Step>

      <Step title="Answer the questions">
        The skill will walk you through questions like:

        * What should the plugin be called?
        * Which effects do you want? (desaturation, contrast, vignette, grain, etc.)
        * What parameters should be exposed as sliders? What are the defaults and ranges?
        * Should this be a main pipeline or a post-processor?
        * Any third-party dependencies beyond what Scope provides?

        You can answer in as much or as little detail as you want. For anything you skip, the skill uses sensible defaults based on the Scope plugin patterns.
      </Step>

      <Step title="Review the generated files">
        Once the questions are done, Claude generates all the files at once. You will get the complete project structure with production-ready code following every Scope convention - entry points, lazy imports, Pydantic schemas with `ui_field_config`, GPU-accelerated tensor operations, the works.

        Review the output and ask for any changes:

        ```
        Can you add a sepia tone option as a toggle? When enabled it should add a
        slight warm tint instead of pure black and white.
        ```
      </Step>

      <Step title="Install and test">
        The skill provides installation instructions at the end. Follow them to install the plugin in Scope, select it from the pipeline dropdown, and start tweaking the sliders.
      </Step>
    </Steps>
  </Tab>
</Tabs>

***

## Comparing the two approaches

|                 | **Context7 MCP**                       | **Plugin Skill**                       |
| :-------------- | :------------------------------------- | :------------------------------------- |
| **Best for**    | Exploration, unusual plugins, learning | Quick builds, standard plugin patterns |
| **Setup**       | Add MCP server to config               | Clone repo + symlink                   |
| **Workflow**    | Open-ended conversation                | Structured question flow               |
| **Docs access** | Fetches latest docs on-the-fly         | Reference baked into the skill         |
| **Flexibility** | Ask anything, go off-script            | Follows a defined 4-phase flow         |
| **Speed**       | Slightly slower (doc fetching)         | Faster (no external lookups)           |

You can also **combine them**. Start with the skill to get a working plugin quickly, then use Context7 in a follow-up session to explore advanced patterns like post-processors, custom model loading, or chaining multiple pipelines.

***

## Tips for better results

Regardless of which approach you use, these tips will help you get better plugins out of the conversation:

* **Be specific about the visual effect.** "Film noir" is good. "Film noir with crushed blacks, a radial vignette that is stronger in the corners, and grain that is heavier in the shadows" is better. The more detail you give, the closer the first generation will be to what you want.

* **Reference real-world examples.** "Make the contrast curve look like a classic S-curve from Lightroom" gives Claude a concrete target. "Make it look like the movie Sin City" works too.

* **Iterate on the parameters.** The schema defaults are just a starting point. Ask Claude to adjust them: "The vignette is too subtle at 0.3, make the default 0.6 and cap the max at 1.0."

* **Ask for explanations.** If you want to understand what the generated code is doing, just ask. Both approaches can explain the GPU operations, the tensor shapes, and how each parameter maps to a visual change.

* **Test incrementally.** Install the plugin after the first generation, see how it looks, then come back to Claude with feedback. "The grain looks too uniform - can you make it more organic?" is much easier to act on once you have seen the output on screen.

***

## What's next

You now have two AI-assisted workflows for building Scope plugins without writing code from scratch. Here are some ideas for plugins you could vibe code next:

* **Cyberpunk neon glow** - Edge detection with colorful bloom and scanlines
* **Watercolor painting** - Bilateral filtering with color quantization for a painted look
* **Thermal camera** - Map luminance to a heat color palette
* **Tilt-shift miniature** - Selective blur to make real scenes look like tiny models
* **Glitch art** - Random block displacement, color channel splitting, and data moshing

Each of these follows the same pattern. Describe the effect, let Claude generate the code, install it, and iterate. The plugin system handles the rest.

<CardGroup>
  <Card title="Build a Video Effects Plugin" icon="wand-magic-sparkles" href="/scope/tutorials/build-video-effects-plugin">
    The manual tutorial - understand every line of code in a Scope plugin
  </Card>

  <Card title="Scope Plugin Skill" icon="github" href="https://github.com/viborc/scope-plugin-skill">
    Install the Claude Code skill for guided plugin scaffolding
  </Card>

  <Card title="Developing Plugins" icon="code" href="/scope/guides/plugin-development">
    Full reference for plugin project setup, schemas, and pipeline types
  </Card>

  <Card title="Plugin Architecture" icon="sitemap" href="/scope/reference/architecture/plugins">
    Technical deep-dive into how the plugin system works under the hood
  </Card>
</CardGroup>


# Delivery and Outputs
Source: https://docs.daydream.live/streaming-basics/delivery-outputs

Learn how to deliver and output video from Daydream

# What is Delivery and Outputs?

Delivery is the mechanism that transports your livestream from the server to the viewers watching the stream.

The platform takes the ingested stream and sends it to viewers at scale
using **CDNs** (Content Delivery Networks) with servers around the world
and handles **transcoding** (creating multiple quality versions)
along with optimizing routing to get video to viewers quickly.

### Why This Matters

After you create your AI-generated stream in Daydream, there's one more important decision: how will you get it to your viewers?

Think of it like choosing how to deliver a package. You could use overnight express shipping (fast but expensive) or standard shipping (slower but handles more volume). With streaming, you're choosing between speed and scale.

## WHEP Output (Low Latency)

**WHEP** (WebRTC HTTP Egress Protocol) is designed for ultra-low latency streaming — typically under 1 second of delay.
How it works:

Uses WebRTC technology (the same technology used for video calls)
creates direct peer-to-peer-style connections between your stream and viewers.
Data travels quickly with minimal buffering.

| **Best For**                                             | **Trade-offs**                                                      |
| -------------------------------------------------------- | ------------------------------------------------------------------- |
| Live auctions or betting where every second counts       | Smaller audience capacity (harder to scale to thousands of viewers) |
| Interactive streams (gaming, live Q\&A, reactions)       | Requires more server resources per viewer                           |
| Video conferencing or webinar-style broadcasts           | Less compatible with traditional CDNs                               |
| Real-time collaboration where immediate feedback matters | May have more connection stability issues on poor networks          |

## HLS Output (CDN Distribution)

**HLS** (HTTP Live Streaming) is the industry standard for scalable, reliable delivery to large audiences.
How it works:

Breaks your stream into small video chunks (typically 2-10 seconds each)
Distributes these chunks across CDN (Content Delivery Network) servers worldwide
Viewers download and play chunks sequentially

| **Best For**                                       | **Trade-offs**                                                                   |
| -------------------------------------------------- | -------------------------------------------------------------------------------- |
| Large-scale events (concerts, conferences, sports) | Higher latency (typically 10-30 seconds delay)                                   |
| On-demand playback and DVR features                | Not suitable for real-time interaction                                           |
| Mobile apps and broad device compatibility         | The chunked nature means slight quality adjustments as network conditions change |

## Choosing the right output

| **WHEP**                                                  | **HLS**                                                               |
| --------------------------------------------------------- | --------------------------------------------------------------------- |
| Latency under 2 seconds is critical                       | You need to reach a large audience (thousands or millions of viewers) |
| You have a smaller, engaged audience (dozens to hundreds) | A few seconds of delay won't impact the experience                    |
| Interaction is a key part of the experience               | You want maximum device/platform compatibility                        |
| You're okay with higher infrastructure costs per viewer   | Reliability and scale matter more than instant feedback               |

## API Examples

### Create a Output with WHEP

WHEP output is provided when you create a stream with the Daydream API. The `output_playback_id` provided in the response is used to pull the stream from Daydream.

### How It Works

```mermaid theme={null}
graph LR
    OBS[OBS] -->|WHIP| Daydream[Daydream<br/>AI Proc]
    Daydream -->|WHEP<br/>Pulling from Daydream| Client[Your Client<br/>Browser/App]
```

<Warning>
  Make sure to keep your API Key safe and secure and not exposed to the public
</Warning>

<CodeGroup>
  ```bash Curl Example theme={null}
  # Step 1: Create a Daydream stream
  DAYDREAM_API_KEY="<YOUR_DAYDREAM_API_KEY>"
  RESPONSE=$(curl -s -X POST "https://api.daydream.live/v1/streams" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer ${DAYDREAM_API_KEY}" \
    -d '{
      "pipeline": "streamdiffusion",
      "params": {
        "model_id": "stabilityai/sd-turbo",
        "prompt": "a serene mountain landscape at sunset"
      }
    }')

  echo "=== Daydream Response ==="
  echo $RESPONSE | jq '.'

  PLAYBACK_ID=$(echo $RESPONSE | jq -r '.output_playback_id')
  echo "=== Playback ID ==="
  echo $PLAYBACK_ID

  # Debug: See the full Livepeer response
  echo "=== Livepeer Response ==="
  LIVEPEER_RESPONSE=$(curl -s "https://livepeer.studio/api/playback/${PLAYBACK_ID}")
  echo $LIVEPEER_RESPONSE | jq '.'

  # Use the URL for under the WebRTC source
  {
    "type": "live",
    "meta": {
      "live": 0,
      "source": [
        {
          "hrn": "WebRTC (H264)",
          "type": "html5/video/h264",
          "url": "https://livepeercdn.studio/webrtc/8a5eiih6kev07buv"
        },
      ]
    }
  }

  ```

  ```javascript JavaScript Example theme={null}
  // Step 1: Create a Daydream stream
  const DAYDREAM_API_KEY = "<YOUR_DAYDREAM_API_KEY>";

  async function createDaydreamStream() {
    try {
      // Create the stream
      const response = await fetch("https://api.daydream.live/v1/streams", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "Authorization": `Bearer ${DAYDREAM_API_KEY}`
        },
        body: JSON.stringify({
          pipeline: "streamdiffusion",
          params: {
            model_id: "stabilityai/sd-turbo",
            prompt: "a serene mountain landscape at sunset"
          }
        })
      });

      const data = await response.json();
      console.log("=== Daydream Response ===");
      console.log(JSON.stringify(data, null, 2));

      const playbackId = data.output_playback_id;
      console.log("=== Playback ID ===");
      console.log(playbackId);

      // Get Livepeer playback information
      console.log("=== Livepeer Response ===");
      const livepeerResponse = await fetch(
        `https://livepeer.studio/api/playback/${playbackId}`
      );
      const livepeerData = await livepeerResponse.json();
      console.log(JSON.stringify(livepeerData, null, 2));

      // Extract WebRTC URL
      const webrtcSource = livepeerData.meta?.source?.find(
        s => s.hrn === "WebRTC (H264)"
      );
      
      if (webrtcSource) {
        console.log("=== WebRTC URL ===");
        console.log(webrtcSource.url);
        return webrtcSource.url;
      }

    } catch (error) {
      console.error("Error:", error);
    }
  }

  // Run the function
  createDaydreamStream();
  ```
</CodeGroup>

### Create a Output with HLS

HLS output is provided when you create a stream with the Daydream API. The `output_playback_id` provided in the response is used to pull the stream from Daydream for scalable CDN distribution.

### How It Works

```mermaid theme={null}
graph LR
    OBS[OBS] -->|WHIP| Daydream[Daydream<br/>AI Proc]
    Daydream -->|HLS<br/>Pulling from Daydream| Client[Your Client<br/>Browser/App]
```

<Warning>
  Make sure to keep your API Key safe and secure and not exposed to the public
</Warning>

<CodeGroup>
  ```bash Curl Example theme={null}
  # Step 1: Create a Daydream stream
  DAYDREAM_API_KEY="<YOUR_DAYDREAM_API_KEY>"
  RESPONSE=$(curl -s -X POST "https://api.daydream.live/v1/streams" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer ${DAYDREAM_API_KEY}" \
    -d '{
      "pipeline": "streamdiffusion",
      "params": {
        "model_id": "stabilityai/sd-turbo",
        "prompt": "a serene mountain landscape at sunset"
      }
    }')

  echo "=== Daydream Response ==="
  echo $RESPONSE | jq '.'

  PLAYBACK_ID=$(echo $RESPONSE | jq -r '.output_playback_id')
  echo "=== Playback ID ==="
  echo $PLAYBACK_ID

  # Debug: See the full Livepeer response
  echo "=== Livepeer Response ==="
  LIVEPEER_RESPONSE=$(curl -s "https://livepeer.studio/api/playback/${PLAYBACK_ID}")
  echo $LIVEPEER_RESPONSE | jq '.'

  # Use the URL for under the HLS source
  {
    "type": "live",
    "meta": {
      "live": 0,
      "source": [
        {
          "hrn": "HLS (TS)",
          "type": "html5/application/vnd.apple.mpegurl",
          "url": "https://livepeercdn.studio/hls/8a5eiih6kev07buv/index.m3u8"
        },
      ]
    }
  }

  ```

  ```JavaScript JavaScript Example theme={null}
  // Step 1: Create a Daydream stream
  const DAYDREAM_API_KEY = "<YOUR_DAYDREAM_API_KEY>";

  async function createDaydreamStream() {
    try {
      // Create the stream
      const response = await fetch("https://api.daydream.live/v1/streams", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "Authorization": `Bearer ${DAYDREAM_API_KEY}`
        },
        body: JSON.stringify({
          pipeline: "streamdiffusion",
          params: {
            model_id: "stabilityai/sd-turbo",
            prompt: "a serene mountain landscape at sunset"
          }
        })
      });

      const data = await response.json();
      console.log("=== Daydream Response ===");
      console.log(JSON.stringify(data, null, 2));

      const playbackId = data.output_playback_id;
      console.log("=== Playback ID ===");
      console.log(playbackId);

      // Get Livepeer playback information
      console.log("=== Livepeer Response ===");
      const livepeerResponse = await fetch(
        `https://livepeer.studio/api/playback/${playbackId}`
      );
      const livepeerData = await livepeerResponse.json();
      console.log(JSON.stringify(livepeerData, null, 2));

      // Extract HLS URL
      const hlsSource = livepeerData.meta?.source?.find(
        s => s.hrn === "HLS (TS)"
      );
      
      if (hlsSource) {
        console.log("=== HLS URL ===");
        console.log(hlsSource.url);
        return hlsSource.url;
      }

    } catch (error) {
      console.error("Error:", error);
    }
  }

  // Run the function
  createDaydreamStream();
  ```
</CodeGroup>

### Create a Output with RTMP

Using Streaming Platforms to deliver your stream.

<Note>
  You are only able to change the `output_playback_id` when creating a **NEW**
  stream. You cannot change it on an existing stream.
</Note>

<Warning>
  Make sure to keep your API Key safe and secure and not exposed to the public
</Warning>

<CodeGroup>
  ```bash YouTube Example theme={null}
  curl -X POST \
    "https://api.daydream.live/v1/streams" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer <YOUR_DAYDREAM_API_KEY>" \
    -d '{
      "pipeline": "streamdiffusion",
      "output_rtmp_url": "rtmp://a.rtmp.youtube.com/live2/<YOUR_YOUTUBE_STREAM_KEY>",
      "params": {
        "model_id": "stabilityai/sdxl-turbo",
        "prompt": "a serene mountain landscape at sunset"
      }
    }'
  ```

  ```bash Twitch Example theme={null}
  curl -X POST \
    "https://api.daydream.live/v1/streams" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer <YOUR_DAYDREAM_API_KEY>" \
    -d '{
      "pipeline": "streamdiffusion",
      "output_rtmp_url": "rtmp://ingest.global-contribute.live-video.net/app/<YOUR_TWITCH_STREAM_KEY>",
      "params": {
        "model_id": "stabilityai/sdxl-turbo",
        "prompt": "a serene mountain landscape at sunset"
      }
    }'
  ```
</CodeGroup>

### How It Works

```mermaid theme={null}
graph LR
    OBS[OBS] -->|WHIP| Daydream[Daydream<br/>AI Proc]
    Daydream -->|RTMP <br/>Pushing from Daydream| RTMP[RTMP Server<br/>YouTube/etc]
```


# Ingest Guide
Source: https://docs.daydream.live/streaming-basics/ingest-guide

Learn how to send video into Daydream

# What is Ingest?

Think of ingest as the pathway that livestreams passes through to reach the internet.

## Why do we need ingest?

Your video needs to get to a platform (like Twitch, YouTube, or Facebook) somehow! The ingest server is the platform's receiving point. It accepts your stream, processes it, and then distributes it to all your viewers.

The key term you'll see:

**Ingest URL** : The specific address where you send your stream.

When you create a stream with the Daydream API, you'll receive an ingest URL in the response that looks like the following.

Example:

```json theme={null}
{
  "whip_url": "https://ai.livepeer.com/live/video-to-video/<stream_key>/whip"
}
```

## How to Ingest

When you go live, your camera and microphone capture video and audio. Your streaming software (like OBS, Streamlabs, or built-in apps) takes that content and sends it to an ingest server - basically a computer that receives your stream.

### Web Browser Method

<Warning>
  Don't forget to replace `<API_KEY>` with your own.
</Warning>

<Steps>
  <Step title="Create a stream">
    Create a stream using the Daydream API.

    <CodeGroup>
      ```bash curl theme={null}
      # Available models:
      # - stabilityai/sd-turbo: Fast generation with SD-turbo (default)
      # - stabilityai/sdxl-turbo: High-quality with SDXL-turbo
      # - Lykon/dreamshaper-8: Stable Diffusion 1.5 for more control
      # - prompthero/openjourney-v4: SD1.5 with artistic style

      curl -X POST \
        "https://api.daydream.live/v1/streams" \
        -H "Content-Type: application/json" \
        -H "Authorization: Bearer <API_KEY>" \
        -d '{
          "pipeline": "streamdiffusion",
          "params": {
            "model_id": "stabilityai/sd-turbo"
          }
        }'
      ```

      ```javascript JavaScript theme={null}
      // Available models:
      // - stabilityai/sd-turbo: Fast generation with SD-turbo (default)
      // - stabilityai/sdxl-turbo: High-quality with SDXL-turbo
      // - Lykon/dreamshaper-8: Stable Diffusion 1.5 for more control
      // - prompthero/openjourney-v4: SD1.5 with artistic style

      const response = await fetch('https://api.daydream.live/v1/streams', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': 'Bearer <API_KEY>'
        },
        body: JSON.stringify({
          pipeline: 'streamdiffusion',
          params: {
            model_id: 'stabilityai/sd-turbo'
          }
        })
      });

      const data = await response.json();
      console.log(data);
      ```
    </CodeGroup>
  </Step>

  <Step title="Implement the Webcam">
    <Warning>
      Don't forget to replace `<STREAM_KEY>` with your own.
    </Warning>

    <CodeGroup>
      ```JavaScript JS Snippet theme={null}
      <script>
         // 👇 PASTE THE JS SNIPPET HERE
         const WHIP_URL = 'https://ai.livepeer.com/live/video-to-video/<STREAM_KEY>/whip';
         
         let localStream = null;
         let peerConnection = null;
         
         async function startDaydreamStream() {
             localStream = await navigator.mediaDevices.getUserMedia({
                 video: { width: 1280, height: 720 },
                 audio: false
             });
             
             peerConnection = new RTCPeerConnection({
                 iceServers: [{ urls: 'stun:stun.l.google.com:19302' }]
             });
             
             localStream.getTracks().forEach(track => {
                 peerConnection.addTrack(track, localStream);
             });
             
             const offer = await peerConnection.createOffer();
             await peerConnection.setLocalDescription(offer);
             
             const response = await fetch(WHIP_URL, {
                 method: 'POST',
                 headers: { 'Content-Type': 'application/sdp' },
                 body: peerConnection.localDescription.sdp
             });
             
             const answer = await response.text();
             await peerConnection.setRemoteDescription({
                 type: 'answer',
                 sdp: answer
             });
             
             return localStream;
         }
         
         // 👇 CALL IT FROM YOUR BUTTON
         async function start() {
             const stream = await startDaydreamStream();
             document.getElementById('video').srcObject = stream;
             alert('Streaming to Daydream!');
         }
      ```

      ```html Full Example theme={null}
      <!DOCTYPE html>
      <html>
      <head>
      <title>My Daydream App</title>
      </head>
      <body>
      <button onclick="start()">Start</button>
      <video id="video" autoplay muted></video>

      <script>
         // 👇 PASTE THE JS SNIPPET HERE
         const WHIP_URL = 'https://ai.livepeer.com/live/video-to-video/<STREAM_KEY>/whip';
         
         let localStream = null;
         let peerConnection = null;
         
         async function startDaydreamStream() {
             localStream = await navigator.mediaDevices.getUserMedia({
                 video: { width: 1280, height: 720 },
                 audio: false
             });
             
             peerConnection = new RTCPeerConnection({
                 iceServers: [{ urls: 'stun:stun.l.google.com:19302' }]
             });
             
             localStream.getTracks().forEach(track => {
                 peerConnection.addTrack(track, localStream);
             });
             
             const offer = await peerConnection.createOffer();
             await peerConnection.setLocalDescription(offer);
             
             const response = await fetch(WHIP_URL, {
                 method: 'POST',
                 headers: { 'Content-Type': 'application/sdp' },
                 body: peerConnection.localDescription.sdp
             });
             
             const answer = await response.text();
             await peerConnection.setRemoteDescription({
                 type: 'answer',
                 sdp: answer
             });
             
             return localStream;
         }
         
         // 👇 CALL IT FROM YOUR BUTTON
         async function start() {
             const stream = await startDaydreamStream();
             document.getElementById('video').srcObject = stream;
             alert('Streaming to Daydream!');
         }
      </script>
      </body>
      </html>
      ```
    </CodeGroup>
  </Step>
</Steps>

#### **Workflow Diagram**

```mermaid theme={null}
graph LR
    A[Broadcaster<br/>Webcam] -->|INGEST<br/>WebRTC WHIP<br/>Sends Live Video to Daydream| B[Streaming Platform<br/>Web Browser]
    B -->|DELIVERY<br/>WebRTC WHEP<br/>Distributes via CDN| C[PLAYBACK<br/>Daydream playback output]
    
    style A fill:#4A90E2,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#F5A623,stroke:#C17D11,stroke-width:2px,color:#000
    style C fill:#7ED321,stroke:#5FA319,stroke-width:2px,color:#000
```

### OBS Ingest Method

<Warning>
  Don't forget to replace `<API_KEY>` with your own.
</Warning>

<Steps>
  <Step title="Create a stream">
    Create a stream using the Daydream API.

    <CodeGroup>
      ```bash curl  theme={null}
      curl --request POST \
      --url https://api.daydream.live/v1/streams \
      --header 'Content-Type: application/json' \
      --header 'Authorization: Bearer <API KEY>' \
      --data '{
        "pipeline": "streamdiffusion",
        "params": {
          "model_id": "stabilityai/sdxl-turbo",
          "prompt": "a serene mountain landscape at sunset"
        }
      }'

      ```
    </CodeGroup>
  </Step>

  <Step title="Get the Ingest URL">
    In the response from the Create Stream API call, find the `whip_url` field.

    ```json theme={null}
    {
      "whip_url": "https://ai.livepeer.com/live/video-to-video/<STREAM_KEY>/whip"
    }
    ```
  </Step>

  <Step title="Copy the Ingest URL">
    Copy the `whip_url` to use in the OBS settings.
    `https://ai.livepeer.com/live/video-to-video/<STREAM_KEY>/whip`
  </Step>

  <Step title="Set the OBS Settings">
    * Open OBS and go to Settings
      <img alt="Settings" />
    * Click on the `Stream` tab
      <img alt="Stream" />
    * Set the OBS settings to the following:

    <Warning>
      Don't forget to replace `<STREAM_KEY>` with your own.
    </Warning>

    * Service: `WHIP`
    * Server: `https://ai.livepeer.com/live/video-to-video/<STREAM_KEY>/whip`
    * Bearer Token: `EMPTY`
    * Save the settings.
      <img alt="Settings" />
  </Step>

  <Step title="Start Streaming">
    Under the `Controls` section, select `Start Streaming` to start the stream.

    <Note>
      Make sure you have a video capture device selected as the source and is turned on.
    </Note>
  </Step>
</Steps>

#### **Workflow Diagram**

```mermaid theme={null}
graph LR
    A[Broadcaster<br/>OBS] -->|INGEST<br/>WebRTC WHIP<br/>Sends Live Video to Daydream| B[Streaming Platform<br/>Web Browser]
    B -->|DELIVERY<br/>WebRTC WHEP<br/>Distributes via CDN| C[PLAYBACK<br/>Daydream playback output]
    
    style A fill:#4A90E2,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#F5A623,stroke:#C17D11,stroke-width:2px,color:#000
    style C fill:#7ED321,stroke:#5FA319,stroke-width:2px,color:#000
```


# Live Video Basics
Source: https://docs.daydream.live/streaming-basics/live-video

Learn how video livestreaming works

# How Live Video Works

Live video is captured by a camera, instantly converted to digital format, split into multiple quality versions for different devices and internet speeds, then distributed through servers so viewers can watch in real-time with the quality that matches their connection.

## What is live video?

Live video is like watching something happen in real-time through your screen. It's the digital equivalent of being present at an event as it unfolds, seeing and hearing everything the moment it actually occurs.

### Ingest  vs  Delivery  vs  Playback

#### **Ingest**

Ingest is the process of getting video **FROM** the broadcaster **TO** the streaming platform. Think of it as the "upload" stage.

The broadcaster sends their live video stream to the platform's servers
using protocols like **RTMP**, **SRT**, or **WebRTC (WHIP)**
This is typically just **ONE** stream going from broadcaster to platform.

* Example: A streamer using OBS to send their video to Twitch

#### **Delivery**

Delivery is how the platform distributes the video **TO** all the viewers. This is the "distribution" stage.

The platform takes the ingested stream and sends it to potentially millions of viewers
using **CDNs** (Content Delivery Networks) with servers around the world
and handles **transcoding** (creating multiple quality versions)
along with optimizing routing to get video to viewers quickly

* Example: Twitch's servers sending your stream to 10,000 viewers worldwide

#### **Playback**

Playback is what happens on the viewer's device. This is the "watching" stage.

The viewer's device receives the video stream
The video player decodes and displays it
Automatically adjusts quality based on internet speed

* Example: You watching a stream on your phone or computer

#### **Workflow Diagram**

```mermaid theme={null}
graph LR
        A[Broadcaster<br/>Camera/Phone] -->|INGEST<br/>Sends 1 stream| B[Streaming Platform<br/>Twitch/YouTube]
    B -->|DELIVERY<br/>Distributes via CDN| C[PLAYBACK<br/>Viewer 1<br/>Phone]
    B -->|DELIVERY<br/>Distributes via CDN| D[PLAYBACK<br/>Viewer 2<br/>Laptop]
    
    style A fill:#4A90E2,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#F5A623,stroke:#C17D11,stroke-width:2px,color:#000
    style C fill:#7ED321,stroke:#5FA319,stroke-width:2px,color:#000
    style D fill:#7ED321,stroke:#5FA319,stroke-width:2px,color:#000
```

## When to use WebRTC (WHIP) vs  RTMP

Think of both WHIP and RTMP as different ways to send your video stream to the internet, like choosing between two different delivery trucks for your package.

#### **RTMP**

RTMP (Real-Time Messaging Protocol) is the older, more established option. It's been around since the mid-2000s and is like the reliable postal service everyone knows. Most streaming software like OBS Studio supports it out of the box, and almost every streaming platform (YouTube, Twitch, Facebook) accepts it. The main advantage is compatibility—it just works almost everywhere. However, it typically adds about 3-10 seconds of delay (latency) between when something happens in real life and when viewers see it.

#### **WHIP**

WHIP (WebRTC HTTP Ingestion Protocol) is the newer technology, built on WebRTC. Think of it as the express delivery option. Its superpower is ultra-low latency with delivery times of less than a second, sometimes just a few hundred milliseconds. This makes it perfect for real-time interactions like live auctions, video calls, remote collaboration, or gaming streams where you want to respond to chat instantly.

## What WebRTC (WHEP) is and why it matters

#### **WHEP**

WHEP is the standardized protocol for consuming live video streams with minimal latency in WebRTC-based architectures.
To understand the streaming workflow, consider the following components:

* **WHIP** (Ingestion): Standardized method for broadcasters to **PUSH** WebRTC streams to servers using HTTP-based signaling
* **WHEP** (Egress): Standardized method for viewers to **PULL** WebRTC streams from servers, enabling playback

#### **Workflow Diagram**

```mermaid theme={null}
graph LR
    A[🎥 Streamer<br/>Camera/Screen] -->|WHIP<br/>WebRTC Ingestion<br/>Push Video| B[☁️ Media Server<br/>WebRTC Server]
    
    B -->|WHEP<br/>WebRTC Egress<br/>Pull Video| C[👤 Viewer 1<br/>Browser/App]
    
    style A fill:#4A90E2,stroke:#2E5C8A,stroke-width:2px,color:#fff
    style B fill:#F5A623,stroke:#C17D11,stroke-width:2px,color:#000
    style C fill:#7ED321,stroke:#5FA319,stroke-width:2px,color:#000
    
    classDef whipLink stroke:#0288d1,stroke-width:3px
    classDef whepLink stroke:#388e3c,stroke-width:2px
```

#### **WHEP: Technical Benefits and Use Cases**

**Latency Reduction**

Traditional streaming protocols (HLS, DASH) typically introduce 10-30 seconds of latency, creating synchronization issues where viewers receive notifications about live events before seeing them on screen. WHEP reduces this to sub-second levels (under 1 second), effectively eliminating the temporal disconnect between live events and viewer experience.

**Enabling Bidirectional Interaction**

Reduced latency transforms streaming from one-way broadcast into an interactive communication channel, enabling:

* **Educational Content:** Instructors can respond to student questions with minimal delay, creating near-synchronous learning environments that approximate in-person instruction
* **Live Entertainment:** Artists can engage with audience feedback in real-time during concerts or performances, rather than responding to outdated comments
* **Interactive Broadcasting:** Supports time-sensitive use cases such as live Q\&A sessions, polls, or collaborative decision-making

## Latency vs scale trade-offs

Streaming systems face an inherent trade-off where low-latency protocols (WebRTC/WHIP/WHEP) requires significantly more server resources per viewer than high-scale protocols (HLS/DASH via CDN).

#### **WebRTC (Low Latency)**

Maintains persistent connections requiring dedicated server resources per viewer.

* Typical capacity: Hundreds to low thousands per server node

* Cost: 5-10x more per viewer than CDN delivery

#### **CDN-Based Protocols (High Scale)**

* Leverages edge caching and stateless HTTP delivery
* Single origin serves millions through distributed cache hierarchy
* Significantly lower per-viewer infrastructure cost

#### **Choosing the Right Approach**

**Low Latency (WebRTC)**

* Real-time interaction is essential (auctions, betting, gaming)

* Predictable, moderate audience size (\< 10,000 concurrent viewers)

**High Scale (CDN)**

* Large, unpredictable audiences (> 100,000 concurrent)
* 10-30 second delay is acceptable


#  Playback Options & Players
Source: https://docs.daydream.live/streaming-basics/playback

Learn how to playback your stream

# What is Playback?

When you stream video, you need two things: a way to send video (ingest) and a way for viewers to watch it (playback). This guide focuses on the playback side, how viewers watch your stream.

## The Playback Paths

There are different playback paths you can use to watch your stream:

### Direct WebRTC/WHEP (Ultra-Low Latency)

Best use case is for small audiences (under 1,000 viewers), interactive streams, real-time applications.

| Pros                                                             | Cons                                                          |
| ---------------------------------------------------------------- | ------------------------------------------------------------- |
| Nearly instant playback (sub-second latency)                     | Expensive at scale (each viewer needs a dedicated connection) |
| Great for two-way interaction (gaming, auctions, sports betting) | Limited to \~1,000 concurrent viewers per stream              |
|                                                                  | Not all browsers/devices support it well                      |

#### Workflow Diagram

```mermaid theme={null}
graph LR
    A[Your Camera] --> B[Server]
    B --> C[WebRTC/WHEP]
    C --> D[Viewers<br/> 0.5-2 seconds delay]
    style D fill:#5E565A
```

### HLS/DASH via CDN (Scalable)

Best use case is for large audiences, reliable playback, broad device support

| Pros                               | Cons                                     |
| ---------------------------------- | ---------------------------------------- |
| Scales to millions of viewers      | Higher latency (typically 10-30 seconds) |
| Works on virtually all devices     | Not suitable for real-time interaction   |
| Cost-effective for large audiences |                                          |
| CDN handles traffic spikes         |                                          |

#### Workflow Diagram

```mermaid theme={null}
graph LR
    A[Your Camera] --> B[Server]
    B --> C[Transcoding]
    C --> D[HLS/DASH]
    D --> E[CDN]
    E --> F[Viewers<br/> 3-30 seconds delay]
    style F fill:#5E565A
```

### Livepeer Studio

Livepeer Studio supports both WebRTC and HLS protocols within a single workflow. Best use case for
development and testing. Livepeer Studio can also be used on a smaller scale for production

## Choosing the right playback path

| Criteria          | HLS Playback                                                        | WHEP Playback                                                               | Livepeer Studio                                                   |
| ----------------- | ------------------------------------------------------------------- | --------------------------------------------------------------------------- | ----------------------------------------------------------------- |
| **What It Is**    | HTTP Live Streaming - segment-based protocol                        | WebRTC HTTP Egress Protocol - real-time WebRTC streaming                    | Managed video infrastructure platform                             |
| **Latency**       | 6-30+ seconds                                                       | \< 1 second (sub-second)                                                    | Both available (HLS + WebRTC)                                     |
| **Compatibility** | Near-universal (all browsers, devices, smart TVs)                   | Modern browsers with WebRTC support                                         | Both HLS and WebRTC endpoints provided                            |
| **Best For**      | Maximum device compatibility, large-scale broadcasting, VOD content | Ultra-low latency, real-time interaction, two-way communication             | Quick development, MVP/prototyping, flexible infrastructure       |
| **Scale**         | Thousands to millions of viewers                                    | Hundreds to low thousands of concurrent viewers                             | Tens to low hundreds of viewers                                   |
| **Use Cases**     | Live sports broadcasts, webinars, concerts, on-demand libraries     | Live trading platforms, multiplayer gaming, interactive Q\&A, live auctions | Testing ideas, proof of concepts, projects needing both protocols |
| **Choose When**   | Latency of 10-30 seconds is acceptable and reach matters most       | Real-time interaction is critical and you have modern audience              | You need to ship fast without managing infrastructure             |

## Example Players for Playback

### Eyevinn

As the SDP Offer/Answer exchange is WebRTC media server specific this WebRTC player is designed to be extended with Media Server specific adapters. You can either use one of the included media server adapters or build your own custom adapter.

Visit [Eyevinn Player](https://github.com/Eyevinn/webrtc-player)

### OvenPlayer

OvenPlayer is a JavaScript-based Player that can play Low Latency HLS (LLHLS) and WebRTC streams optimized for OvenMediaEngine. It provides various APIs, so you can build and operate your media service more easily.

Visit [OvenPlayer](https://github.com/AirenSoft/OvenPlayer)

### HLS.js

HLS.js is a JavaScript library that allows you to play HLS streams in the browser. It is a port of the native HLS.js library for Node.js.

Visit [HLS.js](https://github.com/video-dev/hls.js)

### DASH.js

DASH.js is a JavaScript library that allows you to play DASH streams in the browser. It is a port of the native DASH.js library for Node.js.

Visit [DASH.js](https://github.com/Dash-Industry-Forum/dash.js)

### Example Code

<CodeGroup>
  ```html Eyevinn theme={null}
  <!DOCTYPE html>
  <html>
  <head>
      <title>WebRTC Livestream</title>
  </head>
  <body>
      <video id="player" controls autoplay muted></video>
      
      <script type="module">
          import { WebRTCPlayer } from 'https://cdn.jsdelivr.net/npm/@eyevinn/webrtc-player@2/dist/webrtc-player.js';
          
          const player = new WebRTCPlayer({
              video: document.getElementById('player'),
              type: 'whep',
              statsTypeFilter: '^candidate-*|^inbound-rtp'
          });
          
          // Your WHEP playback URL from your own server
          player.load('<YOUR_WHEP_URL>');
      </script>
  </body>
  </html>
  ```

  ```html OvenPlayer theme={null}
  <!DOCTYPE html>
  <html>
  <head>
      <title>OvenPlayer Hybrid Stream</title>
      <script src="https://cdn.jsdelivr.net/npm/ovenplayer/dist/ovenplayer.js"></script>
  </head>
  <body>
      <div id="player"></div>
      
      <script>
          const player = OvenPlayer.create('player', {
              sources: [
                  {
                      // Try WebRTC first (low latency)
                      type: 'webrtc',
                      file: 'wss://streaming-server.com/webrtc/stream-abc123'
                  },
                  {
                      // Fallback to HLS if WebRTC fails
                      type: 'hls',
                      file: 'https://streaming-server.com/webrtc/stream-abc123/index.m3u8'
                  }
              ],
              autoStart: true,
              controls: true
          });
      </script>
  </body>
  </html>
  ```

  ```html HTML5 Video Player theme={null}
  <!DOCTYPE html>
  <html>
  <head>
      <title>HLS Livestream</title>
      <script src="https://cdn.jsdelivr.net/npm/hls.js@latest"></script>
  </head>
  <body>
      <video id="player" controls autoplay muted width="800"></video>
      
      <script>
          const video = document.getElementById('player');
          const hlsUrl = 'https://streaming-server.com/webrtc/stream-abc123/index.m3u8';
          
          // Check if browser supports HLS natively (Safari)
          if (video.canPlayType('application/vnd.apple.mpegurl')) {
              video.src = hlsUrl;
          }
          // Use HLS.js for other browsers (Chrome, Firefox)
          else if (Hls.isSupported()) {
              const hls = new Hls();
              hls.loadSource(hlsUrl);
              hls.attachMedia(video);
              
              hls.on(Hls.Events.MANIFEST_PARSED, function() {
                  video.play();
              });
          }
      </script>
  </body>
  </html>
  ```

  ```html DASH.js theme={null}
  <!doctype html>
  <html>
  <head>
      <title>dash.js Rocks</title>
      <style>
          video {
              width: 640px;
              height: 360px;
          }
      </style>
  </head>
  <body>
  <div>
      <video id="videoPlayer" controls></video>
  </div>
  <script src="https://cdn.dashjs.org/latest/modern/umd/dash.all.min.js"></script>
  <script>
      (function () {
          var url = "https://dash.akamaized.net/envivio/EnvivioDash3/manifest.mpd";
          var player = dashjs.MediaPlayer().create();
          player.initialize(document.querySelector("#videoPlayer"), url, true);
      })();
  </script>
  </body>
  </html>
  ```
</CodeGroup>
